#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Enhanced Data Collector for Continuous Learning.

ì§€ì†ì  í•™ìŠµì„ ìœ„í•œ í–¥ìƒëœ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
ì›ë³¸ ì´ë¯¸ì§€, ì„ë² ë”©, ë©”íƒ€ë°ì´í„°ë¥¼ ëª¨ë‘ ì²´ê³„ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import cv2
import json
import uuid
import shutil
import logging
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.append(str(project_root))

from common.logging import setup_logging
from domains.face_recognition.core.services.face_detection_service import FaceDetectionService
from domains.face_recognition.core.services.face_recognition_service import FaceRecognitionService

logger = logging.getLogger(__name__)


@dataclass
class FaceMetadata:
    """ì–¼êµ´ ë©”íƒ€ë°ì´í„°"""
    face_id: str
    person_name: str
    person_id: str
    
    # ì´ë¯¸ì§€ ì •ë³´
    original_image_path: str
    face_image_path: str
    image_width: int
    image_height: int
    face_width: int
    face_height: int
    
    # í’ˆì§ˆ ì •ë³´
    detection_confidence: float
    face_quality_score: float
    blur_score: float
    brightness_score: float
    contrast_score: float
    
    # ìœ„ì¹˜ ì •ë³´
    bbox: List[int]  # [x, y, w, h]
    landmarks: Optional[List[List[float]]]
    head_pose: Optional[Dict[str, float]]  # yaw, pitch, roll
    
    # ìˆ˜ì§‘ ì •ë³´
    collection_method: str  # "camera", "upload", "video", "batch"
    collection_timestamp: str
    camera_id: Optional[int]
    video_source: Optional[str]
    frame_number: Optional[int]
    
    # í™˜ê²½ ì •ë³´
    lighting_condition: str  # "good", "poor", "backlight"
    image_quality: str  # "excellent", "good", "fair", "poor"
    occlusion_level: str  # "none", "partial", "heavy"
    
    # ì„ë² ë”© ì •ë³´
    embedding_model: str
    embedding_version: str
    embedding_vector: List[float]


class EnhancedDataCollector:
    """í–¥ìƒëœ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.detection_service = FaceDetectionService()
        self.recognition_service = FaceRecognitionService()
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ êµ¬ì¡°
        self.base_dir = Path("datasets/face_recognition")
        self.setup_directory_structure()
        
        # ë°ì´í„° í’ˆì§ˆ ê¸°ì¤€
        self.quality_thresholds = {
            'min_face_size': 80,
            'max_blur_threshold': 0.3,
            'min_brightness': 50,
            'max_brightness': 200,
            'min_contrast': 0.3
        }
        
        # ìˆ˜ì§‘ í†µê³„
        self.collection_stats = {
            'total_collected': 0,
            'high_quality': 0,
            'medium_quality': 0,
            'low_quality': 0,
            'rejected': 0
        }
    
    def setup_directory_structure(self):
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •"""
        directories = [
            self.base_dir / "raw" / "original_images",      # ì›ë³¸ ì´ë¯¸ì§€
            self.base_dir / "raw" / "face_crops",           # ì–¼êµ´ í¬ë¡­ ì´ë¯¸ì§€
            self.base_dir / "raw" / "metadata",             # ë©”íƒ€ë°ì´í„°
            self.base_dir / "processed" / "aligned",        # ì •ë ¬ëœ ì–¼êµ´
            self.base_dir / "processed" / "normalized",     # ì •ê·œí™”ëœ ì–¼êµ´
            self.base_dir / "augmented" / "rotated",        # íšŒì „ ì¦ê°•
            self.base_dir / "augmented" / "brightness",     # ë°ê¸° ì¦ê°•
            self.base_dir / "augmented" / "contrast",       # ëŒ€ë¹„ ì¦ê°•
            self.base_dir / "splits" / "train",             # í›ˆë ¨ìš©
            self.base_dir / "splits" / "validation",        # ê²€ì¦ìš©
            self.base_dir / "splits" / "test",              # í…ŒìŠ¤íŠ¸ìš©
            self.base_dir / "annotations" / "bounding_boxes", # ë°”ìš´ë”© ë°•ìŠ¤
            self.base_dir / "annotations" / "landmarks",    # ëœë“œë§ˆí¬
            self.base_dir / "annotations" / "labels",       # ë¼ë²¨
            self.base_dir / "quality_analysis",             # í’ˆì§ˆ ë¶„ì„
            self.base_dir / "failed_cases",                 # ì‹¤íŒ¨ ì¼€ì´ìŠ¤
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì • ì™„ë£Œ: {self.base_dir}")
    
    def collect_from_camera(self, camera_id: int, person_name: str, 
                           target_count: int = 50) -> List[FaceMetadata]:
        """ì¹´ë©”ë¼ì—ì„œ ì²´ê³„ì  ë°ì´í„° ìˆ˜ì§‘"""
        logger.info(f"ì¹´ë©”ë¼ {camera_id}ì—ì„œ {person_name}ì˜ ì–¼êµ´ {target_count}ê°œ ìˆ˜ì§‘ ì‹œì‘")
        
        cap = cv2.VideoCapture(camera_id)
        if not cap.isOpened():
            raise RuntimeError(f"ì¹´ë©”ë¼ {camera_id} ì—´ê¸° ì‹¤íŒ¨")
        
        collected_faces = []
        frame_count = 0
        
        print(f"\nğŸ“· {person_name}ì˜ ì–¼êµ´ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤")
        print("   - ë‹¤ì–‘í•œ ê°ë„ë¡œ ì–¼êµ´ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤")
        print("   - 's' í‚¤: í˜„ì¬ í”„ë ˆì„ ì €ì¥")
        print("   - 'q' í‚¤: ì¢…ë£Œ")
        
        while len(collected_faces) < target_count:
            ret, frame = cap.read()
            if not ret:
                continue
            
            frame_count += 1
            
            # ê°„ë‹¨í•œ ì–¼êµ´ ê²€ì¶œ (OpenCV ê¸°ë³¸)
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            faces = face_cascade.detectMultiScale(gray, 1.1, 4)
            
            # ì–¼êµ´ í‘œì‹œ
            for (x, y, w, h) in faces:
                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
            
            # ì§„í–‰ë¥  í‘œì‹œ
            progress_text = f"Collected: {len(collected_faces)}/{target_count}"
            cv2.putText(frame, progress_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            cv2.imshow('Face Collection', frame)
            
            key = cv2.waitKey(1) & 0xFF
            if key == ord('s') and len(faces) > 0:
                # ê°€ì¥ í° ì–¼êµ´ ì €ì¥
                largest_face = max(faces, key=lambda face: face[2] * face[3])
                x, y, w, h = largest_face
                
                # ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = self._create_simple_metadata(
                    frame, largest_face, person_name,
                    collection_method="camera",
                    camera_id=camera_id,
                    frame_number=frame_count
                )
                
                # ì´ë¯¸ì§€ ì €ì¥
                if self._save_face_data(frame, largest_face, metadata):
                    collected_faces.append(metadata)
                    print(f"   âœ… ìˆ˜ì§‘: {len(collected_faces)}/{target_count}")
            
            elif key == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
        
        logger.info(f"ì¹´ë©”ë¼ ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_faces)}ê°œ ì–¼êµ´")
        self._update_collection_stats(collected_faces)
        
        return collected_faces
    
    def collect_from_video(self, video_path: str, person_name: str,
                          frame_skip: int = 30) -> List[FaceMetadata]:
        """ë™ì˜ìƒì—ì„œ ì²´ê³„ì  ë°ì´í„° ìˆ˜ì§‘"""
        logger.info(f"ë™ì˜ìƒ {video_path}ì—ì„œ {person_name}ì˜ ì–¼êµ´ ìˆ˜ì§‘ ì‹œì‘")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise RuntimeError(f"ë™ì˜ìƒ {video_path} ì—´ê¸° ì‹¤íŒ¨")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        collected_faces = []
        frame_number = 0
        processed_frames = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_number += 1
            
            # í”„ë ˆì„ ê±´ë„ˆë›°ê¸°ë¡œ ë‹¤ì–‘ì„± í™•ë³´
            if frame_number % frame_skip != 0:
                continue
            
            processed_frames += 1
            
            # ì–¼êµ´ ê²€ì¶œ
            detection_result = self.detection_service.detect_faces(frame)
            
            for face in detection_result.faces:
                # í’ˆì§ˆ ê²€ì‚¬
                quality_info = self._assess_face_quality(frame, face)
                
                if quality_info['overall_quality'] in ['excellent', 'good', 'fair']:
                    # ë©”íƒ€ë°ì´í„° ìƒì„±
                    metadata = self._create_face_metadata(
                        frame, face, person_name,
                        collection_method="video",
                        video_source=video_path,
                        frame_number=frame_number,
                        quality_info=quality_info
                    )
                    
                    # ì´ë¯¸ì§€ ì €ì¥
                    if self._save_face_data(frame, face, metadata):
                        collected_faces.append(metadata)
            
            # ì§„í–‰ë¥  í‘œì‹œ
            progress = processed_frames / (total_frames // frame_skip) * 100
            if processed_frames % 10 == 0:
                print(f"ì§„í–‰ë¥ : {progress:.1f}% - ìˆ˜ì§‘ëœ ì–¼êµ´: {len(collected_faces)}")
        
        cap.release()
        
        logger.info(f"ë™ì˜ìƒ ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_faces)}ê°œ ì–¼êµ´")
        self._update_collection_stats(collected_faces)
        
        return collected_faces
    
    def _assess_face_quality(self, image: np.ndarray, face) -> Dict[str, Any]:
        """ì–¼êµ´ í’ˆì§ˆ í‰ê°€"""
        x, y, w, h = face.bbox
        face_crop = image[y:y+h, x:x+w]
        
        # í¬ê¸° ê²€ì‚¬
        size_score = min(w, h) / self.quality_thresholds['min_face_size']
        
        # ë¸”ëŸ¬ ê²€ì‚¬ (Laplacian variance)
        gray_face = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)
        blur_score = cv2.Laplacian(gray_face, cv2.CV_64F).var()
        blur_normalized = min(blur_score / 1000, 1.0)
        
        # ë°ê¸° ê²€ì‚¬
        brightness = np.mean(gray_face)
        brightness_score = 1.0 - abs(brightness - 127.5) / 127.5
        
        # ëŒ€ë¹„ ê²€ì‚¬
        contrast = np.std(gray_face) / 127.5
        contrast_score = min(contrast, 1.0)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = (
            size_score * 0.3 +
            blur_normalized * 0.3 +
            brightness_score * 0.2 +
            contrast_score * 0.2
        )
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if quality_score >= 0.8:
            overall_quality = "excellent"
        elif quality_score >= 0.6:
            overall_quality = "good"
        elif quality_score >= 0.4:
            overall_quality = "fair"
        else:
            overall_quality = "poor"
        
        return {
            'size_score': size_score,
            'blur_score': blur_normalized,
            'brightness_score': brightness_score,
            'contrast_score': contrast_score,
            'quality_score': quality_score,
            'overall_quality': overall_quality,
            'brightness': brightness,
            'contrast': contrast * 127.5
        }
    
    def _create_face_metadata(self, image: np.ndarray, face, person_name: str,
                             collection_method: str, **kwargs) -> FaceMetadata:
        """ì–¼êµ´ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        face_id = str(uuid.uuid4())
        person_id = str(uuid.uuid4())  # ì‹¤ì œë¡œëŠ” ê¸°ì¡´ person_id ì‚¬ìš©
        
        x, y, w, h = face.bbox
        quality_info = kwargs.get('quality_info', {})
        
        # ì„ë² ë”© ì¶”ì¶œ
        face_crop = image[y:y+h, x:x+w]
        embedding = self.recognition_service.extract_embedding(face_crop)
        
        metadata = FaceMetadata(
            face_id=face_id,
            person_name=person_name,
            person_id=person_id,
            
            # ì´ë¯¸ì§€ ì •ë³´
            original_image_path="",  # ì €ì¥ í›„ ì„¤ì •
            face_image_path="",      # ì €ì¥ í›„ ì„¤ì •
            image_width=image.shape[1],
            image_height=image.shape[0],
            face_width=w,
            face_height=h,
            
            # í’ˆì§ˆ ì •ë³´
            detection_confidence=face.confidence,
            face_quality_score=quality_info.get('quality_score', 0.0),
            blur_score=quality_info.get('blur_score', 0.0),
            brightness_score=quality_info.get('brightness_score', 0.0),
            contrast_score=quality_info.get('contrast_score', 0.0),
            
            # ìœ„ì¹˜ ì •ë³´
            bbox=[x, y, w, h],
            landmarks=None,
            head_pose=None,
            
            # ìˆ˜ì§‘ ì •ë³´
            collection_method=collection_method,
            collection_timestamp=datetime.now().isoformat(),
            camera_id=kwargs.get('camera_id'),
            video_source=kwargs.get('video_source'),
            frame_number=kwargs.get('frame_number'),
            
            # í™˜ê²½ ì •ë³´
            lighting_condition=self._assess_lighting(quality_info),
            image_quality=quality_info.get('overall_quality', 'unknown'),
            occlusion_level="none",  # ì¶”í›„ êµ¬í˜„
            
            # ì„ë² ë”© ì •ë³´
            embedding_model="arcface",
            embedding_version="1.0",
            embedding_vector=embedding.vector.tolist()
        )
        
        return metadata
    
    def _save_face_data(self, image: np.ndarray, face, metadata: FaceMetadata) -> bool:
        """ì–¼êµ´ ë°ì´í„° ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
            original_filename = f"{metadata.person_name}_{timestamp}_original_{metadata.face_id[:8]}.jpg"
            original_path = self.base_dir / "raw" / "original_images" / original_filename
            cv2.imwrite(str(original_path), image)
            
            # ì–¼êµ´ í¬ë¡­ ì €ì¥
            x, y, w, h = metadata.bbox
            face_crop = image[y:y+h, x:x+w]
            face_filename = f"{metadata.person_name}_{timestamp}_face_{metadata.face_id[:8]}.jpg"
            face_path = self.base_dir / "raw" / "face_crops" / face_filename
            cv2.imwrite(str(face_path), face_crop)
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata.original_image_path = str(original_path)
            metadata.face_image_path = str(face_path)
            
            # ë©”íƒ€ë°ì´í„° JSON ì €ì¥
            metadata_filename = f"{metadata.person_name}_{timestamp}_metadata_{metadata.face_id[:8]}.json"
            metadata_path = self.base_dir / "raw" / "metadata" / metadata_filename
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(asdict(metadata), f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ì–¼êµ´ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {face_filename}")
            return True
            
        except Exception as e:
            logger.error(f"ì–¼êµ´ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _assess_lighting(self, quality_info: Dict) -> str:
        """ì¡°ëª… ìƒíƒœ í‰ê°€"""
        brightness = quality_info.get('brightness', 127.5)
        
        if brightness < 80:
            return "poor"
        elif brightness > 180:
            return "backlight"
        else:
            return "good"
    
    def _draw_collection_preview(self, frame: np.ndarray, faces: List, 
                               collected: int, target: int):
        """ìˆ˜ì§‘ ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ"""
        for face in faces:
            x, y, w, h = face.bbox
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
        
        # ì§„í–‰ë¥  í‘œì‹œ
        progress_text = f"Collected: {collected}/{target}"
        cv2.putText(frame, progress_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    
    def _update_collection_stats(self, collected_faces: List[FaceMetadata]):
        """ìˆ˜ì§‘ í†µê³„ ì—…ë°ì´íŠ¸"""
        for face in collected_faces:
            self.collection_stats['total_collected'] += 1
            
            if face.image_quality == 'excellent':
                self.collection_stats['high_quality'] += 1
            elif face.image_quality == 'good':
                self.collection_stats['medium_quality'] += 1
            elif face.image_quality == 'fair':
                self.collection_stats['low_quality'] += 1
            else:
                self.collection_stats['rejected'] += 1
    
    def export_training_dataset(self, train_ratio: float = 0.7, 
                               val_ratio: float = 0.2) -> Dict[str, str]:
        """í›ˆë ¨ìš© ë°ì´í„°ì…‹ export"""
        metadata_dir = self.base_dir / "raw" / "metadata"
        
        # ëª¨ë“  ë©”íƒ€ë°ì´í„° ë¡œë“œ
        all_metadata = []
        for metadata_file in metadata_dir.glob("*.json"):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                all_metadata.append(metadata)
        
        # ì¸ë¬¼ë³„ ê·¸ë£¹í™”
        person_groups = {}
        for metadata in all_metadata:
            person_name = metadata['person_name']
            if person_name not in person_groups:
                person_groups[person_name] = []
            person_groups[person_name].append(metadata)
        
        # ë¶„í•  ìˆ˜í–‰
        splits = {'train': [], 'validation': [], 'test': []}
        
        for person_name, faces in person_groups.items():
            n_faces = len(faces)
            n_train = int(n_faces * train_ratio)
            n_val = int(n_faces * val_ratio)
            
            # ëœë¤ ì…”í”Œ í›„ ë¶„í• 
            import random
            random.shuffle(faces)
            
            splits['train'].extend(faces[:n_train])
            splits['validation'].extend(faces[n_train:n_train + n_val])
            splits['test'].extend(faces[n_train + n_val:])
        
        # ë¶„í•  ê²°ê³¼ ì €ì¥
        split_info = {}
        for split_name, split_data in splits.items():
            split_file = self.base_dir / "splits" / f"{split_name}.json"
            with open(split_file, 'w', encoding='utf-8') as f:
                json.dump(split_data, f, ensure_ascii=False, indent=2)
            
            split_info[split_name] = str(split_file)
            logger.info(f"{split_name.capitalize()} split: {len(split_data)} samples")
        
        return split_info
    
    def get_collection_summary(self) -> Dict[str, Any]:
        """ìˆ˜ì§‘ ìš”ì•½ ì •ë³´"""
        # ì‹¤ì œ ì €ì¥ëœ íŒŒì¼ ìˆ˜ ê³„ì‚°
        metadata_dir = self.base_dir / "raw" / "metadata"
        stored_count = len(list(metadata_dir.glob("*.json"))) if metadata_dir.exists() else 0
        
        return {
            'stored_files': stored_count,
            'data_locations': {
                'original_images': str(self.base_dir / "raw" / "original_images"),
                'face_crops': str(self.base_dir / "raw" / "face_crops"),
                'metadata': str(self.base_dir / "raw" / "metadata")
            },
            'directory_structure': str(self.base_dir)
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    setup_logging()
    
    collector = EnhancedDataCollector()
    
    print("ğŸ¯ í–¥ìƒëœ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ")
    print("=" * 50)
    print("1. ì¹´ë©”ë¼ì—ì„œ ìˆ˜ì§‘")
    print("2. ë™ì˜ìƒì—ì„œ ìˆ˜ì§‘")  
    print("3. ìˆ˜ì§‘ ìš”ì•½ ë³´ê¸°")
    
    choice = input("ì„ íƒí•˜ì„¸ìš” (1-3): ")
    
    if choice == '1':
        person_name = input("ì¸ë¬¼ ì´ë¦„: ")
        target_count = int(input("ëª©í‘œ ìˆ˜ì§‘ ê°œìˆ˜ (ê¸°ë³¸ 50): ") or "50")
        
        faces = collector.collect_from_camera(0, person_name, target_count)
        print(f"âœ… {len(faces)}ê°œ ì–¼êµ´ ìˆ˜ì§‘ ì™„ë£Œ")
        
    elif choice == '2':
        video_path = input("ë™ì˜ìƒ ê²½ë¡œ: ")
        person_name = input("ì¸ë¬¼ ì´ë¦„: ")
        
        faces = collector.collect_from_video(video_path, person_name)
        print(f"âœ… {len(faces)}ê°œ ì–¼êµ´ ìˆ˜ì§‘ ì™„ë£Œ")
        
    elif choice == '3':
        summary = collector.get_collection_summary()
        print(f"ì €ì¥ëœ íŒŒì¼ ìˆ˜: {summary['stored_files']}")
        print(f"ë°ì´í„° ì €ì¥ì†Œ: {summary['directory_structure']}")


if __name__ == "__main__":
    main() 