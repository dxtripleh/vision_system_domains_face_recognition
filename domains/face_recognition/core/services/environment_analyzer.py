#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
í™˜ê²½ ë¶„ì„ ì„œë¹„ìŠ¤

í•˜ë“œì›¨ì–´ í™˜ê²½ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ê³¼ ì„¤ì •ì„ ì„ íƒí•©ë‹ˆë‹¤.
"""

import os
import sys
import logging
import platform
import psutil
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent.parent
sys.path.append(str(project_root))

from common.logging import setup_logging

class EnvironmentAnalyzer:
    """í™˜ê²½ ë¶„ì„ê¸°"""
    
    def __init__(self):
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # í•˜ë“œì›¨ì–´ ì •ë³´
        self.hardware_info = {}
        
        # ëª¨ë¸ ì„ íƒ ê¸°ì¤€
        self.model_selection_criteria = {
            'detection': {
                'high_performance': {
                    'model': 'retinaface',
                    'config': {
                        'confidence_threshold': 0.7,
                        'nms_threshold': 0.4,
                        'device': 'cuda'
                    }
                },
                'balanced': {
                    'model': 'opencv_dnn',
                    'config': {
                        'confidence_threshold': 0.6,
                        'nms_threshold': 0.4,
                        'device': 'cpu'
                    }
                },
                'lightweight': {
                    'model': 'opencv_cascade',
                    'config': {
                        'scale_factor': 1.1,
                        'min_neighbors': 5,
                        'min_size': (30, 30),
                        'device': 'cpu'
                    }
                }
            },
            'recognition': {
                'high_performance': {
                    'model': 'arcface',
                    'config': {
                        'embedding_size': 512,
                        'device': 'cuda'
                    }
                },
                'balanced': {
                    'model': 'facenet',
                    'config': {
                        'embedding_size': 128,
                        'device': 'cpu'
                    }
                },
                'lightweight': {
                    'model': 'simple_cnn',
                    'config': {
                        'embedding_size': 64,
                        'device': 'cpu'
                    }
                }
            }
        }
        
        self.logger.info("í™˜ê²½ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def analyze_environment(self) -> Dict:
        """ì „ì²´ í™˜ê²½ ë¶„ì„"""
        self.logger.info("í™˜ê²½ ë¶„ì„ ì‹œì‘")
        
        # í•˜ë“œì›¨ì–´ ì •ë³´ ìˆ˜ì§‘
        self.hardware_info = {
            'system': self._get_system_info(),
            'cpu': self._get_cpu_info(),
            'memory': self._get_memory_info(),
            'gpu': self._get_gpu_info(),
            'storage': self._get_storage_info()
        }
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
        performance_tier = self._determine_performance_tier()
        
        # ìµœì  ëª¨ë¸ ì„ íƒ
        optimal_models = self._select_optimal_models(performance_tier)
        
        # ê¶Œì¥ ì„¤ì • ìƒì„±
        recommended_config = self._generate_recommended_config(performance_tier)
        
        analysis_result = {
            'hardware_info': self.hardware_info,
            'performance_tier': performance_tier,
            'optimal_models': optimal_models,
            'recommended_config': recommended_config,
            'analysis_timestamp': self._get_timestamp()
        }
        
        self.logger.info(f"í™˜ê²½ ë¶„ì„ ì™„ë£Œ - ì„±ëŠ¥ ë“±ê¸‰: {performance_tier}")
        return analysis_result
    
    def _get_system_info(self) -> Dict:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        return {
            'platform': platform.system(),
            'platform_version': platform.version(),
            'architecture': platform.architecture()[0],
            'processor': platform.processor(),
            'python_version': platform.python_version()
        }
    
    def _get_cpu_info(self) -> Dict:
        """CPU ì •ë³´ ìˆ˜ì§‘"""
        cpu_info = {
            'cores_physical': psutil.cpu_count(logical=False),
            'cores_logical': psutil.cpu_count(logical=True),
            'frequency_max': 0,
            'frequency_current': 0
        }
        
        try:
            cpu_freq = psutil.cpu_freq()
            if cpu_freq:
                cpu_info['frequency_max'] = cpu_freq.max
                cpu_info['frequency_current'] = cpu_freq.current
        except:
            pass
        
        return cpu_info
    
    def _get_memory_info(self) -> Dict:
        """ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘"""
        memory = psutil.virtual_memory()
        
        return {
            'total_gb': round(memory.total / (1024**3), 2),
            'available_gb': round(memory.available / (1024**3), 2),
            'used_percent': memory.percent
        }
    
    def _get_gpu_info(self) -> Dict:
        """GPU ì •ë³´ ìˆ˜ì§‘"""
        gpu_info = {
            'available': False,
            'cuda_available': False,
            'devices': []
        }
        
        # CUDA í™•ì¸
        try:
            import torch
            if torch.cuda.is_available():
                gpu_info['cuda_available'] = True
                gpu_info['available'] = True
                
                for i in range(torch.cuda.device_count()):
                    device_props = torch.cuda.get_device_properties(i)
                    gpu_info['devices'].append({
                        'id': i,
                        'name': device_props.name,
                        'memory_total_gb': round(device_props.total_memory / (1024**3), 2),
                        'compute_capability': f"{device_props.major}.{device_props.minor}"
                    })
        except ImportError:
            self.logger.warning("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - GPU ì •ë³´ ìˆ˜ì§‘ ì œí•œ")
        
        # OpenCL í™•ì¸ (ì¶”ê°€ì ìœ¼ë¡œ)
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                gpu_info['nvidia_smi_available'] = True
        except:
            gpu_info['nvidia_smi_available'] = False
        
        return gpu_info
    
    def _get_storage_info(self) -> Dict:
        """ì €ì¥ì†Œ ì •ë³´ ìˆ˜ì§‘"""
        try:
            # Windowsì—ì„œëŠ” ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
            current_drive = os.getcwd()[:3]  # C:\ í˜•íƒœ
            disk_usage = psutil.disk_usage(current_drive)
            
            return {
                'total_gb': round(disk_usage.total / (1024**3), 2),
                'free_gb': round(disk_usage.free / (1024**3), 2),
                'used_percent': round((disk_usage.used / disk_usage.total) * 100, 2)
            }
        except Exception as e:
            self.logger.warning(f"ì €ì¥ì†Œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return {
                'total_gb': 0,
                'free_gb': 0,
                'used_percent': 0
            }
    
    def _determine_performance_tier(self) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •"""
        score = 0
        
        # GPU ì ìˆ˜ (ê°€ì¥ ì¤‘ìš”)
        if self.hardware_info['gpu']['cuda_available']:
            if self.hardware_info['gpu']['devices']:
                gpu_memory = self.hardware_info['gpu']['devices'][0]['memory_total_gb']
                if gpu_memory >= 8:
                    score += 40  # ê³ ì„±ëŠ¥ GPU
                elif gpu_memory >= 4:
                    score += 25  # ì¤‘ê¸‰ GPU
                else:
                    score += 15  # ì €ê¸‰ GPU
        
        # CPU ì ìˆ˜
        cpu_cores = self.hardware_info['cpu']['cores_logical']
        if cpu_cores >= 8:
            score += 20
        elif cpu_cores >= 4:
            score += 15
        else:
            score += 10
        
        # ë©”ëª¨ë¦¬ ì ìˆ˜
        memory_gb = self.hardware_info['memory']['total_gb']
        if memory_gb >= 16:
            score += 20
        elif memory_gb >= 8:
            score += 15
        else:
            score += 10
        
        # CPU ì£¼íŒŒìˆ˜ ì ìˆ˜
        cpu_freq = self.hardware_info['cpu']['frequency_max']
        if cpu_freq >= 3000:  # 3GHz ì´ìƒ
            score += 10
        elif cpu_freq >= 2000:  # 2GHz ì´ìƒ
            score += 5
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
        if score >= 70:
            return 'high_performance'
        elif score >= 40:
            return 'balanced'
        else:
            return 'lightweight'
    
    def _select_optimal_models(self, performance_tier: str) -> Dict:
        """ìµœì  ëª¨ë¸ ì„ íƒ"""
        return {
            'detection': self.model_selection_criteria['detection'][performance_tier],
            'recognition': self.model_selection_criteria['recognition'][performance_tier]
        }
    
    def _generate_recommended_config(self, performance_tier: str) -> Dict:
        """ê¶Œì¥ ì„¤ì • ìƒì„±"""
        base_config = {
            'lightweight': {
                'batch_size': 1,
                'num_workers': 2,
                'image_size': (224, 224),
                'max_concurrent_streams': 1
            },
            'balanced': {
                'batch_size': 2,
                'num_workers': 4,
                'image_size': (320, 320),
                'max_concurrent_streams': 2
            },
            'high_performance': {
                'batch_size': 4,
                'num_workers': 8,
                'image_size': (640, 640),
                'max_concurrent_streams': 4
            }
        }
        
        config = base_config[performance_tier].copy()
        
        # í•˜ë“œì›¨ì–´ë³„ ì„¸ë¶€ ì¡°ì •
        if performance_tier == 'high_performance' and self.hardware_info['gpu']['cuda_available']:
            config['use_mixed_precision'] = True
            config['tensorrt_optimization'] = True
        
        return config
    
    def _get_timestamp(self) -> str:
        """íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def get_optimal_detection_engine(self) -> str:
        """ìµœì  ê²€ì¶œ ì—”ì§„ ë°˜í™˜"""
        analysis = self.analyze_environment()
        return analysis['optimal_models']['detection']['model']
    
    def get_optimal_recognition_engine(self) -> str:
        """ìµœì  ì¸ì‹ ì—”ì§„ ë°˜í™˜"""
        analysis = self.analyze_environment()
        return analysis['optimal_models']['recognition']['model']
    
    def get_detection_config(self) -> Dict:
        """ê²€ì¶œ ì„¤ì • ë°˜í™˜"""
        analysis = self.analyze_environment()
        return analysis['optimal_models']['detection']['config']
    
    def get_recognition_config(self) -> Dict:
        """ì¸ì‹ ì„¤ì • ë°˜í™˜"""
        analysis = self.analyze_environment()
        return analysis['optimal_models']['recognition']['config']
    
    def print_analysis_report(self):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        analysis = self.analyze_environment()
        
        print("\n" + "="*60)
        print("ğŸ” í™˜ê²½ ë¶„ì„ ë³´ê³ ì„œ")
        print("="*60)
        
        # ì‹œìŠ¤í…œ ì •ë³´
        system = analysis['hardware_info']['system']
        print(f"\nğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   í”Œë«í¼: {system['platform']} {system['architecture']}")
        print(f"   í”„ë¡œì„¸ì„œ: {system['processor']}")
        
        # CPU ì •ë³´
        cpu = analysis['hardware_info']['cpu']
        print(f"\nğŸ”§ CPU ì •ë³´:")
        print(f"   ì½”ì–´: {cpu['cores_physical']}ë¬¼ë¦¬ / {cpu['cores_logical']}ë…¼ë¦¬")
        if cpu['frequency_max'] > 0:
            print(f"   ì£¼íŒŒìˆ˜: {cpu['frequency_max']:.0f}MHz (ìµœëŒ€)")
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory = analysis['hardware_info']['memory']
        print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ì •ë³´:")
        print(f"   ì´ ìš©ëŸ‰: {memory['total_gb']:.1f}GB")
        print(f"   ì‚¬ìš© ê°€ëŠ¥: {memory['available_gb']:.1f}GB")
        
        # GPU ì •ë³´
        gpu = analysis['hardware_info']['gpu']
        print(f"\nğŸ® GPU ì •ë³´:")
        if gpu['cuda_available']:
            print(f"   CUDA ì‚¬ìš© ê°€ëŠ¥: âœ…")
            for device in gpu['devices']:
                print(f"   - {device['name']} ({device['memory_total_gb']:.1f}GB)")
        else:
            print(f"   CUDA ì‚¬ìš© ê°€ëŠ¥: âŒ")
        
        # ì„±ëŠ¥ ë“±ê¸‰
        tier = analysis['performance_tier']
        tier_names = {
            'high_performance': 'ê³ ì„±ëŠ¥ ğŸš€',
            'balanced': 'ê· í˜•í˜• âš–ï¸',
            'lightweight': 'ê²½ëŸ‰í˜• ğŸª¶'
        }
        print(f"\nğŸ“Š ì„±ëŠ¥ ë“±ê¸‰: {tier_names.get(tier, tier)}")
        
        # ê¶Œì¥ ëª¨ë¸
        models = analysis['optimal_models']
        print(f"\nğŸ¯ ê¶Œì¥ ëª¨ë¸:")
        print(f"   ì–¼êµ´ ê²€ì¶œ: {models['detection']['model']}")
        print(f"   ì–¼êµ´ ì¸ì‹: {models['recognition']['model']}")
        
        # ê¶Œì¥ ì„¤ì •
        config = analysis['recommended_config']
        print(f"\nâš™ï¸  ê¶Œì¥ ì„¤ì •:")
        print(f"   ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
        print(f"   ì´ë¯¸ì§€ í¬ê¸°: {config['image_size']}")
        print(f"   ë™ì‹œ ìŠ¤íŠ¸ë¦¼: {config['max_concurrent_streams']}")
        
        print("="*60)
        
        return analysis

def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    analyzer = EnvironmentAnalyzer()
    analyzer.print_analysis_report()

if __name__ == "__main__":
    main() 