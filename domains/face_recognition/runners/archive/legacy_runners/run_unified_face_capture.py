#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
í†µí•© ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ (ì‚¬ìš©ì ì œì•ˆ êµ¬ì¡°)

ğŸ“‹ í†µí•© ë°ì´í„° íë¦„:
   ì¹´ë©”ë¼/ë™ì˜ìƒ â†’ ì–¼êµ´ ê²€ì¶œ â†’ ì‚¬ìš©ì ì„ íƒ â†’ data/temp/face_staging/
                                                        â†“
                                                    ğŸ¯ ë¶„ê¸° ì„ íƒ:
                                                    1ï¸âƒ£ ì¦‰ì‹œ ë“±ë¡ (ì„ë² ë”© â†’ storage)
                                                    2ï¸âƒ£ í›ˆë ¨ìš© ìˆ˜ì§‘ (í’ˆì§ˆí‰ê°€ â†’ datasets)
"""

import cv2
import numpy as np
import time
import sys
import os
import uuid
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent.parent
sys.path.append(str(project_root))

from domains.face_recognition.core.services.face_detection_service import FaceDetectionService
from domains.face_recognition.core.services.face_recognition_service import FaceRecognitionService
from domains.face_recognition.core.services.face_grouping_service import FaceGroupingService
from domains.face_recognition.core.entities.person import Person
from domains.face_recognition.core.entities.face import Face
from domains.face_recognition.core.value_objects.bounding_box import BoundingBox
from domains.face_recognition.core.value_objects.confidence_score import ConfidenceScore
from domains.face_recognition.infrastructure.storage.file_storage import FileStorage
from shared.vision_core.quality.face_quality_assessor import CustomFaceQualityAssessor
from common.logging import setup_logging
from domains.face_recognition.infrastructure.detection_engines.opencv_detection_engine import OpenCVDetectionEngine

logger = logging.getLogger(__name__)

class UnifiedFaceCaptureSystem:
    """í†µí•© ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ"""
    
    def __init__(self, camera_id: int = 0):
        """ì´ˆê¸°í™”"""
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # ì§ì ‘ OpenCV ê²€ì¶œ ì—”ì§„ ì´ˆê¸°í™” (ê°„ë‹¨í•œ ì„¤ì •)
        detection_config = {
            'confidence_threshold': 0.5,
            'scale_factor': 1.1,
            'min_neighbors': 5,
            'min_size': (30, 30)
        }
        self.detector = OpenCVDetectionEngine(detection_config)
        
        # ì¹´ë©”ë¼ ì„¤ì •
        self.camera_id = camera_id
        self.cap = None
        
        # ê²½ë¡œ ì„¤ì •
        self.face_staging_dir = project_root / 'data' / 'temp' / 'face_staging'
        self.captured_frames_dir = project_root / 'data' / 'temp' / 'captured_frames'
        self.output_dir = project_root / 'data' / 'output'
        
        # í´ë” ìƒì„±
        for directory in [self.face_staging_dir, self.captured_frames_dir, self.output_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë“œ ì„¤ì •
        self.is_auto_mode = False  # ê¸°ë³¸ê°’: ìˆ˜ë™ ëª¨ë“œ
        
        # UI ìƒíƒœ
        self.show_info = True
        self.confidence_threshold = detection_config.get('confidence_threshold', 0.5)
        self.min_confidence = 0.1
        self.max_confidence = 0.9
        self.confidence_step = 0.05
        
        # ë…¹í™” ì„¤ì •
        self.is_recording = False
        self.video_writer = None
        self.recording_start_time = None
        
        # ì¼ì‹œì •ì§€ ì„¤ì •
        self.is_paused = False
        self.paused_frame = None
        
        # ìº¡ì²˜ëœ ì–¼êµ´ ì •ë³´
        self.captured_faces = []
        
        # ìë™ ëª¨ë“œ ì„¤ì •
        self.auto_capture_buffer = []
        self.auto_capture_interval = 2.0  # 2ì´ˆë§ˆë‹¤ ìë™ ìº¡ì²˜
        self.last_auto_capture = 0
        self.similarity_threshold = 0.6
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.fps_counter = 0
        self.fps_timer = time.time()
        self.current_fps = 0
        
        # í™˜ê²½ ì •ë³´ ì €ì¥ (ê°„ë‹¨í•œ ê¸°ë³¸ê°’)
        self.detection_model = "opencv_cascade"
        self.performance_tier = "balanced"
        
        self.logger.info("í†µí•© ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
    def start_capture(self):
        """ìº¡ì²˜ ì‹œì‘"""
        print("ğŸš€ í†µí•© ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ ì‹œì‘")
        print("="*60)
        
        # í™˜ê²½ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        self._print_environment_info()
        
        # ì¹´ë©”ë¼ ì´ˆê¸°í™”
        if not self._initialize_camera():
            return False
        
        print("\nğŸ“¹ ì¹´ë©”ë¼ ì‹œì‘ë¨. í‚¤ë³´ë“œ ëª…ë ¹ì–´:")
        self._print_help()
        
        try:
            # ë©”ì¸ ìº¡ì²˜ ë£¨í”„
            self._capture_loop()
            
        except KeyboardInterrupt:
            print("\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ìº¡ì²˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            self._cleanup()
        
        return True
    
    def _print_environment_info(self):
        """í™˜ê²½ ì •ë³´ ì¶œë ¥"""
        tier_names = {
            'high_performance': 'ê³ ì„±ëŠ¥ ğŸš€',
            'balanced': 'ê· í˜•í˜• âš–ï¸',
            'lightweight': 'ê²½ëŸ‰í˜• ğŸª¶'
        }
        
        print(f"ğŸ” í™˜ê²½ ë¶„ì„ ê²°ê³¼:")
        print(f"   ì„±ëŠ¥ ë“±ê¸‰: {tier_names.get(self.performance_tier, self.performance_tier)}")
        print(f"   ì„ íƒëœ ê²€ì¶œ ëª¨ë¸: {self.detection_model}")
        print(f"   ì‹ ë¢°ë„ ì„ê³„ê°’: {self.confidence_threshold:.2f}")
        print(f"   ëª¨ë“œ: {'ğŸ¤– ìë™' if self.is_auto_mode else 'ğŸ‘¤ ìˆ˜ë™'}")
    
    def _initialize_camera(self) -> bool:
        """ì¹´ë©”ë¼ ì´ˆê¸°í™”"""
        try:
            self.cap = cv2.VideoCapture(self.camera_id)
            if not self.cap.isOpened():
                print(f"âŒ ì¹´ë©”ë¼ {self.camera_id} ì—°ê²° ì‹¤íŒ¨")
                return False
            
            # ì¹´ë©”ë¼ ì„¤ì •
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            
            # í…ŒìŠ¤íŠ¸ í”„ë ˆì„ ì½ê¸°
            ret, frame = self.cap.read()
            if not ret:
                print("âŒ ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            print(f"âœ… ì¹´ë©”ë¼ {self.camera_id} ì—°ê²° ì„±ê³µ ({frame.shape[1]}x{frame.shape[0]})")
            return True
            
        except Exception as e:
            self.logger.error(f"ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'cap') and self.cap:
            self.cap.release()
        
        if hasattr(self, 'video_writer') and self.video_writer:
            self.video_writer.release()
        
        cv2.destroyAllWindows()
        
        print(f"\nğŸ“Š ì„¸ì…˜ ìš”ì•½:")
        print(f"   ëª¨ë“œ: {'ìë™' if self.is_auto_mode else 'ìˆ˜ë™'}")
        print(f"   ìº¡ì²˜ëœ ì–¼êµ´: {len(getattr(self, 'captured_faces', []))}ê°œ")
        print("âœ… ì‹œìŠ¤í…œ ì¢…ë£Œ")
    
    def _print_help(self):
        """ë„ì›€ë§ ì¶œë ¥"""
        print("ğŸ“‹ ì‚¬ìš©ë²•:")
        print("  'c' â†’ ìº¡ì²˜ ëª¨ë“œ ì§„ì…")
        print("  '1' â†’ ì¦‰ì‹œ ë“±ë¡ ëª¨ë“œ")  
        print("  '2' â†’ í›ˆë ¨ìš© ìˆ˜ì§‘ ëª¨ë“œ")
        print("  's' â†’ í˜„ì¬ í”„ë ˆì„ ì €ì¥")
        print("  'q' â†’ ì¢…ë£Œ")
        print("=" * 50)
    
    def _capture_loop(self):
        """ìº¡ì²˜ ë£¨í”„"""
        while True:
            ret, frame = self.cap.read()
            if not ret:
                self.logger.warning("í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                continue
            
            # ğŸ” ì–¼êµ´ ê²€ì¶œ (ê³µí†µ)
            detections = self.detector.detect(frame)
            
            # ğŸ¨ í™”ë©´ í‘œì‹œ
            display_frame = self._draw_detections(frame.copy(), detections)
            self._draw_ui_info(display_frame)
            
            cv2.imshow('í†µí•© ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ', display_frame)
            
            # âŒ¨ï¸ í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                break
            elif key == ord('c'):
                self._enter_capture_mode(detections, frame)
            elif key == ord('1'):
                self._set_immediate_mode()
            elif key == ord('2'):
                self._set_training_mode()
            elif key == ord('s'):
                self._save_current_frame(frame, detections)
            elif key >= ord('1') and key <= ord('9') and self.is_auto_mode:
                self._select_face_by_number(key - ord('1'), detections, frame)
            
    def _enter_capture_mode(self, detections: List, frame: np.ndarray):
        """ğŸ® ìº¡ì²˜ ëª¨ë“œ ì§„ì…"""
        if len(detections) == 0:
            print("âŒ ê²€ì¶œëœ ì–¼êµ´ì´ ì—†ìŠµë‹ˆë‹¤!")
            return
            
        self.is_auto_mode = True
        print(f"\nğŸ¯ ìº¡ì²˜ ëª¨ë“œ ì§„ì…! {len(detections)}ê°œ ì–¼êµ´ ê²€ì¶œë¨")
        
        # ì¸ë¬¼ ì´ë¦„ ì…ë ¥
        self.selected_person_name = input("ğŸ‘¤ ì¸ë¬¼ ì´ë¦„ ì…ë ¥: ").strip()
        if not self.selected_person_name:
            print("âŒ ì´ë¦„ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.is_auto_mode = False
            return
        
        print(f"ğŸ“‹ '{self.selected_person_name}'ìœ¼ë¡œ ì„¤ì •ë¨")
        print("ğŸ”¢ 1-9ë²ˆ í‚¤ë¡œ ì›í•˜ëŠ” ì–¼êµ´ì„ ì„ íƒí•˜ì„¸ìš”")
    
    def _set_immediate_mode(self):
        """1ï¸âƒ£ ì¦‰ì‹œ ë“±ë¡ ëª¨ë“œ ì„¤ì •"""
        self.is_auto_mode = False
        print("âœ… ì¦‰ì‹œ ë“±ë¡ ëª¨ë“œ í™œì„±í™” (ì„ë² ë”© â†’ data/storage)")
    
    def _set_training_mode(self):
        """2ï¸âƒ£ í›ˆë ¨ìš© ìˆ˜ì§‘ ëª¨ë“œ ì„¤ì •"""
        self.is_auto_mode = False
        print("âœ… í›ˆë ¨ìš© ìˆ˜ì§‘ ëª¨ë“œ í™œì„±í™” (í’ˆì§ˆí‰ê°€ â†’ datasets/raw)")
    
    def _save_current_frame(self, frame: np.ndarray, detections: List):
        """ğŸ“¸ í˜„ì¬ í”„ë ˆì„ ì„ì‹œ ì €ì¥"""
        timestamp = int(time.time())
        temp_path = self.face_staging_dir / f"temp_frame_{timestamp}.jpg"
        cv2.imwrite(str(temp_path), frame)
        
        # ê²€ì¶œëœ ì–¼êµ´ ì •ë³´ë„ ì €ì¥
        faces_info = []
        for i, detection in enumerate(detections):
            faces_info.append({
                'face_index': i,
                'bbox': detection.bbox.to_list(),
                'confidence': detection.confidence.value
            })
        
        info_path = self.face_staging_dir / f"temp_frame_{timestamp}.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': timestamp,
                'frame_path': str(temp_path),
                'faces': faces_info
            }, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ í”„ë ˆì„ ì„ì‹œ ì €ì¥: {temp_path.name}")
    
    def _select_face_by_number(self, face_index: int, detections: List, frame: np.ndarray):
        """ğŸ”¢ ë²ˆí˜¸ë¡œ ì–¼êµ´ ì„ íƒ ë° ì²˜ë¦¬"""
        if face_index >= len(detections):
            print(f"âŒ ì–¼êµ´ ë²ˆí˜¸ {face_index + 1}ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨")
            return
        
        selected_face = detections[face_index]
        print(f"âœ… ì–¼êµ´ #{face_index + 1} ì„ íƒë¨")
        
        # ğŸ“ 1ë‹¨ê³„: ê³µí†µ ì„ì‹œ ì €ì¥
        temp_path = self._save_to_temp(frame, selected_face, face_index)
        
        if not self.is_auto_mode:
            print("âš ï¸ ì²˜ë¦¬ ëª¨ë“œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. '1' ë˜ëŠ” '2' í‚¤ë¡œ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”.")
            return
        
        # ğŸ¯ 2ë‹¨ê³„: ì„ íƒëœ ëª¨ë“œë¡œ ë¶„ê¸° ì²˜ë¦¬
        if self.is_auto_mode == 'immediate':
            self._process_for_immediate_registration(temp_path, selected_face, frame)
        elif self.is_auto_mode == 'training':
            self._process_for_training_collection(temp_path, selected_face, frame)
        
        self.is_auto_mode = False
    
    def _save_to_temp(self, frame: np.ndarray, face_detection, face_index: int) -> Path:
        """ğŸ“ ê³µí†µ: data/temp/face_staging ì— ì €ì¥"""
        timestamp = int(time.time())
        
        # ì „ì²´ í”„ë ˆì„ ì €ì¥
        frame_filename = f"{self.selected_person_name}_{timestamp}_frame_{face_index}.jpg"
        frame_path = self.face_staging_dir / frame_filename
        cv2.imwrite(str(frame_path), frame)
        
        # ì–¼êµ´ í¬ë¡­ ì €ì¥  
        x, y, w, h = face_detection.bbox.to_list()
        face_crop = frame[y:y+h, x:x+w]
        face_filename = f"{self.selected_person_name}_{timestamp}_face_{face_index}.jpg"
        face_path = self.face_staging_dir / face_filename
        cv2.imwrite(str(face_path), face_crop)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'person_name': self.selected_person_name,
            'timestamp': timestamp,
            'face_index': face_index,
            'bbox': face_detection.bbox.to_list(),
            'confidence': face_detection.confidence.value,
            'frame_path': str(frame_path),
            'face_path': str(face_path)
        }
        
        metadata_filename = f"{self.selected_person_name}_{timestamp}_meta_{face_index}.json"
        metadata_path = self.face_staging_dir / metadata_filename
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ê³µí†µ ì„ì‹œ ì €ì¥ ì™„ë£Œ: {face_filename}")
        return face_path
    
    def _process_for_immediate_registration(self, face_path: Path, face_detection, frame: np.ndarray):
        """
        1ï¸âƒ£ ì¦‰ì‹œ ë“±ë¡ ì²˜ë¦¬: ì„ë² ë”© ì¶”ì¶œ â†’ data/storage ì €ì¥
        """
        print("ğŸš€ 1ï¸âƒ£ ì¦‰ì‹œ ë“±ë¡ ì²˜ë¦¬ ì‹œì‘...")
        
        try:
            # ì–¼êµ´ ì´ë¯¸ì§€ ë¡œë“œ
            face_image = cv2.imread(str(face_path))
            if face_image is None:
                self.logger.error("ì–¼êµ´ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
                return
            
            # ì„ë² ë”© ì¶”ì¶œ
            embedding = self.recognition_service.extract_embedding(face_image)
            
            # Person ìƒì„± (ê¸°ì¡´ì— ìˆìœ¼ë©´ ë¡œë“œ)
            person = self._get_or_create_person(self.selected_person_name)
            
            # Face ì—”í‹°í‹° ìƒì„±
            face = Face(
                face_id=str(uuid.uuid4()),
                person_id=person.person_id,
                embedding=embedding,
                bounding_box=face_detection.bbox,
                confidence=face_detection.confidence,
                created_at=datetime.now()
            )
            
            # data/storageì— ì €ì¥
            person_saved = self.storage.save_person(person)
            face_saved = self.storage.save_face(face)
            
            if person_saved and face_saved:
                # ğŸ”— ìë™ ê·¸ë£¹í•‘ ì²˜ë¦¬
                group_id = self.grouping_service.process_face(face)
                
                # ê·¸ë£¹ì— ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì´ë¦„ ìë™ ì ìš©
                if self.selected_person_name:
                    self.grouping_service.set_group_name(group_id, self.selected_person_name)
                    print(f"ğŸ”— ê·¸ë£¹í•‘ ì™„ë£Œ: '{self.selected_person_name}' ê·¸ë£¹ (ID: {group_id[:8]})")
                
                print(f"âœ… ì¦‰ì‹œ ë“±ë¡ ì™„ë£Œ: {self.selected_person_name}")
                print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: data/storage/")
                print(f"   ğŸ†” Person ID: {person.person_id}")
                print(f"   ğŸ†” Face ID: {face.face_id}")
                print(f"   ğŸ”— Group ID: {group_id[:8]}")
                print(f"   ğŸ¯ ì¦‰ì‹œ ì‹¤ì‹œê°„ ì¸ì‹ ê°€ëŠ¥!")
            else:
                print("âŒ ì €ì¥ ì‹¤íŒ¨")
                
        except Exception as e:
            self.logger.error(f"ì¦‰ì‹œ ë“±ë¡ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ ì¦‰ì‹œ ë“±ë¡ ì‹¤íŒ¨: {str(e)}")
    
    def _process_for_training_collection(self, face_path: Path, face_detection, frame: np.ndarray):
        """
        2ï¸âƒ£ í›ˆë ¨ìš© ìˆ˜ì§‘ ì²˜ë¦¬: í’ˆì§ˆí‰ê°€ â†’ datasets/raw ì €ì¥
        """
        print("ğŸš€ 2ï¸âƒ£ í›ˆë ¨ìš© ìˆ˜ì§‘ ì²˜ë¦¬ ì‹œì‘...")
        
        try:
            # ì–¼êµ´ í’ˆì§ˆ í‰ê°€
            face_bbox = face_detection.bbox.to_list()
            quality_result = self.quality_assessor.assess_face_quality(frame, face_bbox)
            
            print(f"ğŸ“Š í’ˆì§ˆ í‰ê°€ ê²°ê³¼:")
            print(f"   ì „ì²´ ì ìˆ˜: {quality_result['quality_score']:.3f}")
            print(f"   í’ˆì§ˆ ë“±ê¸‰: {quality_result['overall_quality']}")
            print(f"   í¬ê¸° ì ìˆ˜: {quality_result.get('size_score', 0):.3f}")
            print(f"   ì„ ëª…ë„ ì ìˆ˜: {quality_result.get('blur_score', 0):.3f}")
            
            # í’ˆì§ˆ ê¸°ì¤€ í†µê³¼ í™•ì¸
            if quality_result['overall_quality'] == 'poor':
                print("âŒ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ë¡œ í›ˆë ¨ìš© ìˆ˜ì§‘ì—ì„œ ì œì™¸ë¨")
                return
            
            # datasets/raw ì— ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥ (full frame)
            original_dir = self.training_dir / "original_images"
            original_dir.mkdir(exist_ok=True)
            original_filename = f"{self.selected_person_name}_{timestamp}_original.jpg"
            original_path = original_dir / original_filename
            cv2.imwrite(str(original_path), frame)
            
            # ì–¼êµ´ í¬ë¡­ ì €ì¥
            face_crop_dir = self.training_dir / "face_crops"
            face_crop_dir.mkdir(exist_ok=True)
            face_crop_filename = f"{self.selected_person_name}_{timestamp}_face.jpg"
            face_crop_path = face_crop_dir / face_crop_filename
            
            # ì„ì‹œ ì €ì¥ëœ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ë³µì‚¬
            import shutil
            shutil.copy2(str(face_path), str(face_crop_path))
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_dir = self.training_dir / "metadata"
            metadata_dir.mkdir(exist_ok=True)
            metadata_filename = f"{self.selected_person_name}_{timestamp}_metadata.json"
            metadata_path = metadata_dir / metadata_filename
            
            training_metadata = {
                'face_id': str(uuid.uuid4()),
                'person_name': self.selected_person_name,
                'collection_timestamp': datetime.now().isoformat(),
                'original_image_path': str(original_path),
                'face_image_path': str(face_crop_path),
                'bbox': face_bbox,
                'detection_confidence': face_detection.confidence.value,
                'quality_assessment': quality_result,
                'collection_method': 'unified_capture_system'
            }
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(training_metadata, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… í›ˆë ¨ìš© ìˆ˜ì§‘ ì™„ë£Œ: {self.selected_person_name}")
            print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: datasets/face_recognition/raw/")
            print(f"   ğŸ“Š í’ˆì§ˆ ë“±ê¸‰: {quality_result['overall_quality']}")
            print(f"   ğŸ¯ í–¥í›„ ëª¨ë¸ í›ˆë ¨ì— í™œìš© ê°€ëŠ¥!")
            
        except Exception as e:
            self.logger.error(f"í›ˆë ¨ìš© ìˆ˜ì§‘ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ í›ˆë ¨ìš© ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
    
    def _get_or_create_person(self, person_name: str) -> Person:
        """ê¸°ì¡´ ì¸ë¬¼ ë¡œë“œ ë˜ëŠ” ìƒˆ ì¸ë¬¼ ìƒì„±"""
        try:
            # ê¸°ì¡´ ì¸ë¬¼ ê²€ìƒ‰
            existing_persons = self.storage.load_all_persons()
            for person in existing_persons:
                if person.name == person_name:
                    return person
            
            # ìƒˆ ì¸ë¬¼ ìƒì„±
            return Person(
                person_id=str(uuid.uuid4()),
                name=person_name,
                created_at=datetime.now()
            )
            
        except Exception as e:
            self.logger.warning(f"ì¸ë¬¼ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±: {str(e)}")
            return Person(
                person_id=str(uuid.uuid4()),
                name=person_name,
                created_at=datetime.now()
            )
    
    def _draw_detections(self, frame: np.ndarray, detections: List) -> np.ndarray:
        """ğŸ¨ ê²€ì¶œ ê²°ê³¼ í‘œì‹œ"""
        for i, detection in enumerate(detections):
            bbox = detection.bbox.to_list()
            x, y, w, h = bbox
            
            # ì–¼êµ´ ë°•ìŠ¤
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
            
            # ë²ˆí˜¸ í‘œì‹œ
            cv2.putText(frame, f'{i+1}', (x, y-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
            
            # ì‹ ë¢°ë„ í‘œì‹œ
            conf_text = f'{detection.confidence.value:.2f}'
            cv2.putText(frame, conf_text, (x, y+h+20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        return frame
    
    def _draw_ui_info(self, frame: np.ndarray):
        """ğŸ¨ UI ì •ë³´ í‘œì‹œ"""
        info_lines = [
            f"Mode: {'ğŸ¤– ìë™' if self.is_auto_mode else 'ğŸ‘¤ ìˆ˜ë™'}",
            f"Capture: {'ON' if self.is_auto_mode else 'OFF'}",
            f"Person: {self.selected_person_name or 'None'}"
        ]
        
        for i, line in enumerate(info_lines):
            y = 30 + i * 25
            cv2.putText(frame, line, (10, y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    setup_logging()
    
    try:
        system = UnifiedFaceCaptureSystem()
        system.start_capture()
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print(f"âŒ ì˜¤ë¥˜: {str(e)}")

if __name__ == "__main__":
    main() 