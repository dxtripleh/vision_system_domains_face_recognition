#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
í†µí•© AI ì–¼êµ´ ê·¸ë£¹í•‘ ì²˜ë¦¬ê¸° (ì „ì²˜ë¦¬ ë° í¬ë¡œìŠ¤ì²´í¬ ê°•í™”)

detected_faces í´ë”ì˜ ì–¼êµ´ë“¤ì„ AIë¡œ ë¶„ì„í•˜ì—¬ ê°™ì€ ì‚¬ëŒë¼ë¦¬ ê·¸ë£¹í•‘í•˜ê³ 
staging/grouped í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.

íŠ¹ì§•:
- ì–¼êµ´ ì •ë ¬ ë° ì „ì²˜ë¦¬ ê°•í™”
- ë‹¤ì¤‘ ëª¨ë¸ í¬ë¡œìŠ¤ì²´í¬ ì‹œìŠ¤í…œ
- í’ˆì§ˆ ê¸°ë°˜ í•„í„°ë§
- ìµœì‹  ì–¼êµ´ ì¸ì‹ ëª¨ë¸ í™œìš©

ì‚¬ìš©ë²•:
    python run_unified_ai_grouping_processor.py [--source from_captured|from_uploads|auto_collected|all]
"""

import os
import sys
import argparse
import logging
import json
import shutil
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Set
import numpy as np
from datetime import datetime
from collections import defaultdict
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# OpenCV import (í‘œì¤€ ë°©ì‹)
try:
    import cv2
except ImportError:
    print("âŒ OpenCVê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install opencv-python")
    sys.exit(1)

# í•„ìˆ˜ ëª¨ë“ˆ import (ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ)
try:
    import mediapipe as mp
    MP_AVAILABLE = True
except ImportError:
    print("âš ï¸ MediaPipeê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install mediapipe")
    MP_AVAILABLE = False

try:
    import dlib
    DLIB_AVAILABLE = True
except ImportError:
    print("âš ï¸ dlibì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install dlib")
    DLIB_AVAILABLE = False

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    print("âš ï¸ onnxruntimeì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install onnxruntime")
    ONNX_AVAILABLE = False

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent.parent
sys.path.append(str(project_root))

try:
    from common.logging import setup_logging, get_logger
    from common.config import load_config
except ImportError as e:
    print(f"âŒ ê³µí†µ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    print("í”„ë¡œì íŠ¸ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

class FacePreprocessor:
    """ì–¼êµ´ ì „ì²˜ë¦¬ ë° ì •ë ¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        self.face_mesh = None
        self.face_detector = None
        self.shape_predictor = None
        self.mp_available = MP_AVAILABLE
        self.dlib_available = DLIB_AVAILABLE
        self._initialize_face_processing()
    
    def _initialize_face_processing(self):
        """ì–¼êµ´ ì²˜ë¦¬ ëª¨ë¸ ì´ˆê¸°í™”"""
        # MediaPipe Face Mesh (ì–¼êµ´ ëœë“œë§ˆí¬)
        if self.mp_available:
            try:
                self.mp_face_mesh = mp.solutions.face_mesh
                self.face_mesh = self.mp_face_mesh.FaceMesh(
                    static_image_mode=True,
                    max_num_faces=1,
                    refine_landmarks=True,
                    min_detection_confidence=0.5
                )
                self.logger.info("MediaPipe Face Mesh ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.face_mesh = None
                self.mp_available = False
        
        # Dlib ì–¼êµ´ ëœë“œë§ˆí¬ (ëŒ€ì•ˆ)
        if self.dlib_available:
            try:
                predictor_path = project_root / 'models' / 'weights' / 'shape_predictor_68_face_landmarks.dat'
                if predictor_path.exists():
                    self.face_detector = dlib.get_frontal_face_detector()
                    self.shape_predictor = dlib.shape_predictor(str(predictor_path))
                    self.logger.info("Dlib ì–¼êµ´ ëœë“œë§ˆí¬ ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    self.logger.warning(f"Dlib ëœë“œë§ˆí¬ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {predictor_path}")
                    self.dlib_available = False
            except Exception as e:
                self.logger.warning(f"Dlib ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.dlib_available = False
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì²˜ë¦¬ê¸° í™•ì¸
        if not self.mp_available and not self.dlib_available:
            self.logger.warning("ì–¼êµ´ ì •ë ¬ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤.")
    
    def preprocess_face(self, face_image: np.ndarray) -> Optional[np.ndarray]:
        """ì–¼êµ´ ì „ì²˜ë¦¬ ë° ì •ë ¬"""
        if face_image is None or face_image.size == 0:
            self.logger.warning("ìœ íš¨í•˜ì§€ ì•Šì€ ì–¼êµ´ ì´ë¯¸ì§€")
            return None
        
        try:
            # 1. ì–¼êµ´ ì •ë ¬
            aligned_face = self._align_face(face_image)
            if aligned_face is None:
                self.logger.debug("ì–¼êµ´ ì •ë ¬ ì‹¤íŒ¨, ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©")
                aligned_face = face_image
            
            # 2. í’ˆì§ˆ í–¥ìƒ
            enhanced_face = self._enhance_quality(aligned_face)
            
            # 3. ì •ê·œí™”
            normalized_face = self._normalize_face(enhanced_face)
            
            return normalized_face
            
        except Exception as e:
            self.logger.error(f"ì–¼êµ´ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì •ê·œí™”ë§Œ ìˆ˜í–‰
            try:
                return self._normalize_face(face_image)
            except:
                return face_image  # ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ ì›ë³¸ ë°˜í™˜
    
    def _align_face(self, face_image: np.ndarray) -> Optional[np.ndarray]:
        """ì–¼êµ´ ì •ë ¬ (ëœë“œë§ˆí¬ ê¸°ë°˜)"""
        if self.mp_available and self.face_mesh is not None:
            return self._align_with_mediapipe(face_image)
        elif self.dlib_available and self.shape_predictor is not None:
            return self._align_with_dlib(face_image)
        else:
            self.logger.debug("ëœë“œë§ˆí¬ ëª¨ë¸ ì—†ìŒ. ì •ë ¬ ìƒëµ")
            return face_image
    
    def _align_with_mediapipe(self, face_image: np.ndarray) -> Optional[np.ndarray]:
        """MediaPipeë¥¼ ì´ìš©í•œ ì–¼êµ´ ì •ë ¬"""
        try:
            # BGR to RGB
            rgb_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)
            
            # ëœë“œë§ˆí¬ ê²€ì¶œ
            results = self.face_mesh.process(rgb_image)
            
            if not results.multi_face_landmarks:
                self.logger.debug("MediaPipe: ì–¼êµ´ ëœë“œë§ˆí¬ ê²€ì¶œ ì‹¤íŒ¨")
                return None
            
            landmarks = results.multi_face_landmarks[0]
            h, w = face_image.shape[:2]
            
            # ëˆˆ ëœë“œë§ˆí¬ ì¶”ì¶œ (MediaPipe ì¸ë±ìŠ¤)
            left_eye = landmarks.landmark[33]   # ì™¼ìª½ ëˆˆ ì¤‘ì‹¬
            right_eye = landmarks.landmark[263] # ì˜¤ë¥¸ìª½ ëˆˆ ì¤‘ì‹¬
            
            # í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
            left_eye_px = (int(left_eye.x * w), int(left_eye.y * h))
            right_eye_px = (int(right_eye.x * w), int(right_eye.y * h))
            
            # ì •ë ¬ ê°ë„ ê³„ì‚°
            angle = np.degrees(np.arctan2(
                right_eye_px[1] - left_eye_px[1],
                right_eye_px[0] - left_eye_px[0]
            ))
            
            # íšŒì „ ì¤‘ì‹¬ì 
            center = ((left_eye_px[0] + right_eye_px[0]) // 2,
                     (left_eye_px[1] + right_eye_px[1]) // 2)
            
            # íšŒì „ í–‰ë ¬
            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
            
            # íšŒì „ ì ìš©
            aligned = cv2.warpAffine(face_image, rotation_matrix, (w, h))
            
            return aligned
            
        except Exception as e:
            self.logger.error(f"MediaPipe ì •ë ¬ ì˜¤ë¥˜: {e}")
            return None
    
    def _align_with_dlib(self, face_image: np.ndarray) -> Optional[np.ndarray]:
        """Dlibì„ ì´ìš©í•œ ì–¼êµ´ ì •ë ¬"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
            
            # ì–¼êµ´ ê²€ì¶œ
            faces = self.face_detector(gray)
            if not faces:
                self.logger.debug("Dlib: ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨")
                return None
            
            # ëœë“œë§ˆí¬ ì¶”ì¶œ
            shape = self.shape_predictor(gray, faces[0])
            
            # ëˆˆ ëœë“œë§ˆí¬ (Dlib ì¸ë±ìŠ¤)
            left_eye = (shape.part(36).x, shape.part(36).y)   # ì™¼ìª½ ëˆˆ ì‹œì‘
            right_eye = (shape.part(45).x, shape.part(45).y)  # ì˜¤ë¥¸ìª½ ëˆˆ ì‹œì‘
            
            # ì •ë ¬ ê°ë„ ê³„ì‚°
            angle = np.degrees(np.arctan2(
                right_eye[1] - left_eye[1],
                right_eye[0] - left_eye[0]
            ))
            
            # íšŒì „ ì¤‘ì‹¬ì 
            center = ((left_eye[0] + right_eye[0]) // 2,
                     (left_eye[1] + right_eye[1]) // 2)
            
            # íšŒì „ í–‰ë ¬
            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
            
            # íšŒì „ ì ìš©
            h, w = face_image.shape[:2]
            aligned = cv2.warpAffine(face_image, rotation_matrix, (w, h))
            
            return aligned
            
        except Exception as e:
            self.logger.error(f"Dlib ì •ë ¬ ì˜¤ë¥˜: {e}")
            return None
    
    def _enhance_quality(self, face_image: np.ndarray) -> np.ndarray:
        """ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ"""
        try:
            # ì…ë ¥ ê²€ì¦
            if face_image is None or face_image.size == 0:
                return face_image
            
            # 1. íˆìŠ¤í† ê·¸ë¨ í‰í™œí™” (CLAHE)
            lab = cv2.cvtColor(face_image, cv2.COLOR_BGR2LAB)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            lab[:, :, 0] = clahe.apply(lab[:, :, 0])
            enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
            
            # 2. ë…¸ì´ì¦ˆ ì œê±° (ê°„ë‹¨í•œ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ëŒ€ì²´)
            denoised = cv2.GaussianBlur(enhanced, (3, 3), 0)
            
            # 3. ì„ ëª…ë„ í–¥ìƒ
            kernel = np.array([[-1, -1, -1],
                             [-1,  9, -1],
                             [-1, -1, -1]])
            sharpened = cv2.filter2D(denoised, -1, kernel)
            
            return sharpened
            
        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ í–¥ìƒ ì˜¤ë¥˜: {e}")
            return face_image
    
    def _normalize_face(self, face_image: np.ndarray) -> np.ndarray:
        """ì–¼êµ´ ì •ê·œí™”"""
        try:
            # í¬ê¸° ì •ê·œí™” (ArcFace í‘œì¤€ í¬ê¸°)
            normalized = cv2.resize(face_image, (112, 112))
            
            # ë°ê¸° ì •ê·œí™”
            normalized = cv2.convertScaleAbs(normalized, alpha=1.1, beta=5)
            
            return normalized
            
        except Exception as e:
            self.logger.error(f"ì •ê·œí™” ì˜¤ë¥˜: {e}")
            return face_image

class CrossCheckRecognizer:
    """ë‹¤ì¤‘ ëª¨ë¸ í¬ë¡œìŠ¤ì²´í¬ ì¸ì‹ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        self.models = {}
        self.preprocessor = FacePreprocessor()
        self.model_config = {'name': 'CrossCheck (Multi-Model)'}  # ê¸°ë³¸ ì„¤ì • ì¶”ê°€
        self._initialize_models()
    
    def _initialize_models(self):
        """ë‹¤ì¤‘ ëª¨ë¸ ì´ˆê¸°í™”"""
        models_dir = project_root / 'models' / 'weights'
        
        # ONNX ëª¨ë¸ë“¤ (ONNX Runtimeì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ)
        if ONNX_AVAILABLE:
            # 1. ArcFace ëª¨ë¸
            arcface_path = models_dir / 'arcface_glint360k_20250628.onnx'
            if arcface_path.exists():
                try:
                    self.models['arcface'] = {
                        'session': ort.InferenceSession(str(arcface_path)),
                        'name': 'ArcFace',
                        'weight': 1.0,
                        'threshold': 0.6
                    }
                    self.logger.info("ArcFace ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                except Exception as e:
                    self.logger.error(f"ArcFace ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                self.logger.debug(f"ArcFace ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {arcface_path}")
            
            # 2. InsightFace ëª¨ë¸ (ëŒ€ì•ˆ)
            insightface_path = models_dir / 'insightface_glint360k_20250628.onnx'
            if insightface_path.exists():
                try:
                    self.models['insightface'] = {
                        'session': ort.InferenceSession(str(insightface_path)),
                        'name': 'InsightFace',
                        'weight': 0.8,
                        'threshold': 0.65
                    }
                    self.logger.info("InsightFace ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                except Exception as e:
                    self.logger.error(f"InsightFace ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                self.logger.debug(f"InsightFace ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {insightface_path}")
        else:
            self.logger.warning("ONNX Runtime ì—†ìŒ. ONNX ëª¨ë¸ë“¤ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # 3. OpenFace ëª¨ë¸ (OpenCV DNN)
        openface_path = models_dir / 'openface_nn4.small2.v1.t7'
        if openface_path.exists():
            try:
                self.models['openface'] = {
                    'net': cv2.dnn.readNetFromTorch(str(openface_path)),
                    'name': 'OpenFace',
                    'weight': 0.6,
                    'threshold': 0.7
                }
                self.logger.info("OpenFace ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"OpenFace ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            self.logger.debug(f"OpenFace ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {openface_path}")
        
        # 4. ORB íŠ¹ì§• (í•­ìƒ ì‚¬ìš© ê°€ëŠ¥í•œ í´ë°±)
        try:
            self.models['orb'] = {
                'detector': cv2.ORB_create(nfeatures=500),
                'matcher': cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True),
                'name': 'ORB',
                'weight': 0.3,
                'threshold': 0.8
            }
            self.logger.info("ORB íŠ¹ì§• ê²€ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"ORB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ëª¨ë¸ ë¡œë“œ ê²°ê³¼ í™•ì¸
        if not self.models:
            self.logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            # ìµœì†Œí•œì˜ ORBë¼ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¬ì‹œë„
            try:
                self.models['orb'] = {
                    'detector': cv2.ORB_create(nfeatures=200),  # ë” ì ì€ íŠ¹ì§•ì 
                    'matcher': cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True),
                    'name': 'ORB-Minimal',
                    'weight': 1.0,
                    'threshold': 0.9
                }
                self.logger.warning("ìµœì†Œí•œì˜ ORB ëª¨ë¸ë¡œ í´ë°±")
            except Exception as e:
                self.logger.critical(f"ORB í´ë°±ë§ˆì € ì‹¤íŒ¨: {e}")
                raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ ì–¼êµ´ ì¸ì‹ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        self.logger.info(f"ì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ìµœì¢… ëª¨ë¸ëª… ì„¤ì •
        if len(self.models) >= 2:
            model_names = [model['name'] for model in self.models.values()]
            self.model_config['name'] = f"CrossCheck ({' + '.join(model_names)})"
        elif len(self.models) == 1:
            single_model_name = list(self.models.values())[0]['name']
            self.model_config['name'] = f"Single Model ({single_model_name})"
        else:
            self.model_config['name'] = "No Model Available"
    
    def extract_features(self, face_image: np.ndarray) -> Dict[str, np.ndarray]:
        """ë‹¤ì¤‘ ëª¨ë¸ë¡œ íŠ¹ì§• ì¶”ì¶œ"""
        # ì „ì²˜ë¦¬
        preprocessed = self.preprocessor.preprocess_face(face_image)
        if preprocessed is None:
            return {}
        
        features = {}
        
        # ê° ëª¨ë¸ë¡œ íŠ¹ì§• ì¶”ì¶œ
        for model_name, model_info in self.models.items():
            try:
                if model_name == 'arcface':
                    features[model_name] = self._extract_arcface_features(preprocessed, model_info)
                elif model_name == 'insightface':
                    features[model_name] = self._extract_insightface_features(preprocessed, model_info)
                elif model_name == 'openface':
                    features[model_name] = self._extract_openface_features(preprocessed, model_info)
                elif model_name == 'orb':
                    features[model_name] = self._extract_orb_features(preprocessed, model_info)
                    
            except Exception as e:
                self.logger.error(f"{model_name} íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return features
    
    def _extract_arcface_features(self, face_image: np.ndarray, model_info: Dict) -> np.ndarray:
        """ArcFace íŠ¹ì§• ì¶”ì¶œ"""
        session = model_info['session']
        input_name = session.get_inputs()[0].name
        
        # ì „ì²˜ë¦¬
        rgb_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)
        normalized = (rgb_image.astype(np.float32) - 127.5) / 128.0
        input_tensor = np.transpose(normalized, (2, 0, 1))
        input_tensor = np.expand_dims(input_tensor, axis=0)
        
        # ì¶”ë¡ 
        outputs = session.run(None, {input_name: input_tensor})
        embedding = outputs[0][0]
        
        # L2 ì •ê·œí™”
        embedding = embedding / np.linalg.norm(embedding)
        return embedding
    
    def _extract_insightface_features(self, face_image: np.ndarray, model_info: Dict) -> np.ndarray:
        """InsightFace íŠ¹ì§• ì¶”ì¶œ"""
        session = model_info['session']
        input_name = session.get_inputs()[0].name
        
        # ì „ì²˜ë¦¬ (InsightFace í‘œì¤€)
        rgb_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)
        normalized = rgb_image.astype(np.float32) / 255.0
        input_tensor = np.transpose(normalized, (2, 0, 1))
        input_tensor = np.expand_dims(input_tensor, axis=0)
        
        # ì¶”ë¡ 
        outputs = session.run(None, {input_name: input_tensor})
        embedding = outputs[0][0]
        
        # L2 ì •ê·œí™”
        embedding = embedding / np.linalg.norm(embedding)
        return embedding
    
    def _extract_openface_features(self, face_image: np.ndarray, model_info: Dict) -> np.ndarray:
        """OpenFace íŠ¹ì§• ì¶”ì¶œ"""
        net = model_info['net']
        
        # ì „ì²˜ë¦¬
        blob = cv2.dnn.blobFromImage(face_image, 1.0, (96, 96), (0, 0, 0), swapRB=True, crop=False)
        
        # ì¶”ë¡ 
        net.setInput(blob)
        embedding = net.forward()
        
        return embedding.flatten()
    
    def _extract_orb_features(self, face_image: np.ndarray, model_info: Dict) -> np.ndarray:
        """ORB íŠ¹ì§• ì¶”ì¶œ"""
        try:
            detector = model_info['detector']
            
            # ì…ë ¥ ê²€ì¦
            if face_image is None or face_image.size == 0:
                return np.zeros(32)  # ORB descriptorëŠ” 32ì°¨ì›
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(face_image.shape) == 3:
                gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
            else:
                gray = face_image
            
            # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ ë° ì¡°ì •
            if gray.shape[0] < 50 or gray.shape[1] < 50:
                gray = cv2.resize(gray, (112, 112))
            
            # íŠ¹ì§•ì  ê²€ì¶œ
            keypoints, descriptors = detector.detectAndCompute(gray, None)
            
            if descriptors is None or len(descriptors) == 0:
                self.logger.debug("ORB: íŠ¹ì§•ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return np.zeros(32)  # ORB descriptor í¬ê¸°
            
            # íŠ¹ì§• ë²¡í„° ì •ê·œí™”
            if len(descriptors) > 0:
                # í‰ê·  íŠ¹ì§• ë²¡í„° (ëª¨ë“  descriptorì˜ í‰ê· )
                feature_vector = np.mean(descriptors, axis=0).astype(np.float32)
                
                # ì •ê·œí™”
                norm = np.linalg.norm(feature_vector)
                if norm > 0:
                    feature_vector = feature_vector / norm
                
                return feature_vector
            else:
                return np.zeros(32)
                
        except Exception as e:
            self.logger.error(f"ORB íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return np.zeros(32)
    
    def calculate_similarity(self, features1: Dict[str, np.ndarray], 
                           features2: Dict[str, np.ndarray]) -> Dict[str, float]:
        """ë‹¤ì¤‘ ëª¨ë¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        similarities = {}
        
        for model_name in features1.keys():
            if model_name in features2:
                try:
                    if model_name in ['arcface', 'insightface']:
                        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                        similarity = self._cosine_similarity(
                            features1[model_name], features2[model_name]
                        )
                    elif model_name == 'openface':
                        # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„
                        similarity = self._euclidean_similarity(
                            features1[model_name], features2[model_name]
                        )
                    elif model_name == 'orb':
                        # í•´ë° ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„
                        similarity = self._hamming_similarity(
                            features1[model_name], features2[model_name]
                        )
                    else:
                        similarity = 0.0
                    
                    similarities[model_name] = similarity
                    
                except Exception as e:
                    self.logger.error(f"{model_name} ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
                    similarities[model_name] = 0.0
        
        return similarities
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _euclidean_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„"""
        distance = np.linalg.norm(vec1 - vec2)
        max_distance = np.sqrt(len(vec1) * 255 * 255)
        return max(0.0, 1.0 - (distance / max_distance))
    
    def _hamming_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """í•´ë° ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„"""
        distance = np.linalg.norm(vec1 - vec2)
        max_distance = np.sqrt(len(vec1) * 255 * 255)
        return max(0.0, 1.0 - (distance / max_distance))
    
    def get_consensus_similarity(self, similarities: Dict[str, float], debug_info: Dict = None) -> float:
        """ë‹¤ì¤‘ ëª¨ë¸ í•©ì˜ ìœ ì‚¬ë„"""
        if not similarities:
            return 0.0
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_weight = 0.0
        weighted_sum = 0.0
        
        for model_name, similarity in similarities.items():
            weight = self.models[model_name]['weight']
            weighted_sum += similarity * weight
            total_weight += weight
        
        if total_weight == 0:
            return 0.0
        
        consensus = weighted_sum / total_weight
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ëª¨ë¸ ê°„ ì¼ê´€ì„±)
        if len(similarities) > 1:
            values = list(similarities.values())
            std_dev = np.std(values)
            confidence = max(0.0, 1.0 - std_dev)  # í‘œì¤€í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ
            original_consensus = consensus
            consensus *= confidence
            
            # ë””ë²„ê¹… ì •ë³´ ë¡œê¹…
            if debug_info:
                face1 = debug_info.get('face1', 'unknown')
                face2 = debug_info.get('face2', 'unknown')
                self.logger.info(f"ğŸ” ìœ ì‚¬ë„ ë¶„ì„: {face1} vs {face2}")
                for model_name, similarity in similarities.items():
                    if model_name in self.models:
                        weight = self.models[model_name]['weight']
                        self.logger.info(f"  - {model_name}: {similarity:.3f} (ê°€ì¤‘ì¹˜: {weight})")
                self.logger.info(f"  ğŸ“Š ê°€ì¤‘í‰ê· : {original_consensus:.3f}")
                self.logger.info(f"  ğŸ“Š í‘œì¤€í¸ì°¨: {std_dev:.3f}, ì‹ ë¢°ë„: {confidence:.3f}")
                self.logger.info(f"  ğŸ“Š ìµœì¢… ì»¨ì„¼ì„œìŠ¤: {consensus:.3f}")
        
        return consensus

class EnvironmentAnalyzer:
    """í™˜ê²½ ë¶„ì„ì„ í†µí•œ ìµœì  ëª¨ë¸ ì„ íƒ"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
    def analyze_environment(self) -> Dict:
        """í™˜ê²½ ë¶„ì„ ë° ìµœì  ëª¨ë¸ ì¶”ì²œ"""
        env_info = {
            'gpu_available': self._check_gpu(),
            'cpu_cores': os.cpu_count(),
            'available_models': self._scan_available_models(),
            'recommended_recognition_model': None
        }
        
        # ìµœì  ì¸ì‹ ëª¨ë¸ ì„ íƒ
        env_info['recommended_recognition_model'] = self._select_optimal_recognition_model(env_info)
        
        return env_info
    
    def _check_gpu(self) -> bool:
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            try:
                # OpenCV DNN GPU ì§€ì› í™•ì¸
                return cv2.cuda.getCudaEnabledDeviceCount() > 0
            except:
                return False
    
    def _scan_available_models(self) -> List[Dict]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ì‹ ëª¨ë¸ ìŠ¤ìº”"""
        models_dir = project_root / 'models' / 'weights'
        available_models = []
        
        # ArcFace ëª¨ë¸ë“¤ ìŠ¤ìº”
        for model_file in models_dir.glob('*arcface*.onnx'):
            available_models.append({
                'name': f'ArcFace ({model_file.stem})',
                'type': 'arcface',
                'path': str(model_file),
                'speed': 'medium',
                'accuracy': 'high',
                'gpu_required': False
            })
        
        # OpenFace ëª¨ë¸ë“¤ ìŠ¤ìº”
        for model_file in models_dir.glob('*openface*.onnx'):
            available_models.append({
                'name': f'OpenFace ({model_file.stem})',
                'type': 'openface',
                'path': str(model_file),
                'speed': 'fast',
                'accuracy': 'medium',
                'gpu_required': False
            })
        
        # ê¸°ë³¸ OpenCV íŠ¹ì§• ì¶”ì¶œê¸° (í•­ìƒ ì‚¬ìš© ê°€ëŠ¥)
        available_models.append({
            'name': 'OpenCV ORB Features',
            'type': 'orb',
            'path': 'builtin',
            'speed': 'fast',
            'accuracy': 'low',
            'gpu_required': False
        })
        
        return available_models
    
    def _select_optimal_recognition_model(self, env_info: Dict) -> Dict:
        """ìµœì  ì¸ì‹ ëª¨ë¸ ì„ íƒ"""
        available_models = env_info['available_models']
        gpu_available = env_info['gpu_available']
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ê³ ì„±ëŠ¥ ëª¨ë¸ ìš°ì„ 
        if gpu_available:
            for model in available_models:
                if model['accuracy'] == 'high':
                    return model
        
        # CPUë§Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì¤‘ê°„ ì„±ëŠ¥ ëª¨ë¸ ìš°ì„ 
        for model in available_models:
            if not model['gpu_required'] and model['accuracy'] in ['high', 'medium']:
                return model
        
        # ê¸°ë³¸ê°’ (ORB)
        return available_models[-1] if available_models else None

class FaceGrouper:
    """ì–¼êµ´ ê·¸ë£¹í•‘ ì—”ì§„ (í¬ë¡œìŠ¤ì²´í¬ ì‹œìŠ¤í…œ ì ìš©)"""
    
    def __init__(self, recognizer: CrossCheckRecognizer):
        self.recognizer = recognizer
        self.logger = get_logger(__name__)
        
        # ê·¸ë£¹í•‘ ì„¤ì • (ê·¹ë„ë¡œ ì—„ê²©í•œ ì„ê³„ê°’ìœ¼ë¡œ ì¡°ì •)
        self.similarity_threshold = 0.90  # ë§¤ìš° ë†’ì€ ì„ê³„ê°’ìœ¼ë¡œ ì˜ëª»ëœ ê·¸ë£¹í•‘ ë°©ì§€
        self.min_group_size = 2
        self.consensus_threshold = 0.85  # ì»¨ì„¼ì„œìŠ¤ ì„ê³„ê°’ë„ ë§¤ìš° ë†’ê²Œ ì¡°ì •
        self.debug_mode = True  # ë””ë²„ê¹… ëª¨ë“œ í™œì„±í™”
        self.max_group_size = 20  # ìµœëŒ€ ê·¸ë£¹ í¬ê¸° ì œí•œ ì¶”ê°€
        
        # ì¶”ê°€ ì•ˆì „ì¥ì¹˜
        self.strict_mode = True  # ì—„ê²© ëª¨ë“œ í™œì„±í™”
        self.require_multiple_models = True  # ë‹¤ì¤‘ ëª¨ë¸ í•©ì˜ í•„ìˆ˜
    
    def group_faces(self, face_data_list: List[Dict]) -> List[List[Dict]]:
        """ì–¼êµ´ë“¤ì„ ê·¸ë£¹í•‘ (í¬ë¡œìŠ¤ì²´í¬ ì ìš©)"""
        if not face_data_list:
            return []
        
        self.logger.info(f"{len(face_data_list)}ê°œ ì–¼êµ´ì„ í¬ë¡œìŠ¤ì²´í¬ ê·¸ë£¹í•‘ ì¤‘...")
        
        # íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        unique_faces = []
        seen_paths = set()
        
        for face_data in face_data_list:
            face_path = face_data['face_path']
            if face_path not in seen_paths:
                unique_faces.append(face_data)
                seen_paths.add(face_path)
            else:
                self.logger.warning(f"ì¤‘ë³µ íŒŒì¼ ì œê±°: {face_path}")
        
        self.logger.info(f"ì¤‘ë³µ ì œê±° í›„ ìœ íš¨í•œ ì–¼êµ´: {len(unique_faces)}ê°œ")
        
        # ë‹¤ì¤‘ ëª¨ë¸ íŠ¹ì§• ì¶”ì¶œ
        valid_faces = []
        for i, face_data in enumerate(unique_faces):
            try:
                face_image = cv2.imread(face_data['face_path'])
                if face_image is not None:
                    # í¬ë¡œìŠ¤ì²´í¬ íŠ¹ì§• ì¶”ì¶œ
                    features = self.recognizer.extract_features(face_image)
                    if features:
                        face_data['features'] = features
                        valid_faces.append(face_data)
                        
                        # ì§„í–‰ ìƒí™© ë¡œê¹…
                        if i % 5 == 0:
                            model_count = len(features)
                            self.logger.info(f"íŠ¹ì§• ì¶”ì¶œ ì§„í–‰: {i+1}/{len(unique_faces)} (ëª¨ë¸ {model_count}ê°œ)")
                    else:
                        self.logger.warning(f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {face_data['face_path']}")
                else:
                    self.logger.warning(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {face_data['face_path']}")
            except Exception as e:
                self.logger.error(f"ì–¼êµ´ ì²˜ë¦¬ ì˜¤ë¥˜ {face_data['face_path']}: {e}")
        
        self.logger.info(f"ìœ íš¨í•œ ì–¼êµ´: {len(valid_faces)}/{len(unique_faces)}")
        
        if not valid_faces:
            return []
        
        # í¬ë¡œìŠ¤ì²´í¬ ê·¸ë£¹í•‘ ìˆ˜í–‰
        groups = self._perform_crosscheck_clustering(valid_faces)
        
        # í•„í„°ë§ (í¬ê¸° ë° ì•ˆì „ì¥ì¹˜)
        filtered_groups = []
        for group in groups:
            # ìµœì†Œ ê·¸ë£¹ í¬ê¸° í™•ì¸
            if len(group) < self.min_group_size:
                self.logger.debug(f"ê·¸ë£¹ í¬ê¸° ë¶€ì¡±ìœ¼ë¡œ ì œê±°: {len(group)}ê°œ")
                continue
            
            # ìµœëŒ€ ê·¸ë£¹ í¬ê¸° ì œí•œ (ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ëŒ€í˜• ê·¸ë£¹ ë°©ì§€)
            if len(group) > self.max_group_size:
                self.logger.warning(f"âš ï¸ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ëŒ€í˜• ê·¸ë£¹ ê°ì§€: {len(group)}ê°œ - ë¶„í•  í•„ìš”")
                # ëŒ€í˜• ê·¸ë£¹ì„ ë” ì‘ì€ ê·¸ë£¹ë“¤ë¡œ ë¶„í• 
                sub_groups = self._split_large_group(group)
                filtered_groups.extend(sub_groups)
            else:
                # ì—„ê²© ëª¨ë“œì—ì„œ ê·¸ë£¹ ì¼ê´€ì„± ì¬ê²€ì¦
                if self.strict_mode and self.validate_group_consistency(group):
                    filtered_groups.append(group)
                elif not self.strict_mode:
                    filtered_groups.append(group)
                else:
                    self.logger.warning(f"ê·¸ë£¹ ì¼ê´€ì„± ê²€ì¦ ì‹¤íŒ¨ë¡œ ì œê±°: {len(group)}ê°œ")
        
        self.logger.info(f"í¬ë¡œìŠ¤ì²´í¬ ê·¸ë£¹í•‘ ì™„ë£Œ: {len(filtered_groups)}ê°œ ê·¸ë£¹ ìƒì„±")
        
        return filtered_groups
    
    def _perform_crosscheck_clustering(self, face_data_list: List[Dict]) -> List[List[Dict]]:
        """í¬ë¡œìŠ¤ì²´í¬ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        N = len(face_data_list)
        
        # 1. ëª¨ë“  ì–¼êµ´ ìŒì˜ í¬ë¡œìŠ¤ì²´í¬ ìœ ì‚¬ë„ ê³„ì‚°
        self.logger.info("í¬ë¡œìŠ¤ì²´í¬ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
        similarity_matrix = np.zeros((N, N))
        
        for i in range(N):
            for j in range(i+1, N):
                try:
                    # ë‹¤ì¤‘ ëª¨ë¸ ìœ ì‚¬ë„ ê³„ì‚°
                    similarities = self.recognizer.calculate_similarity(
                        face_data_list[i]['features'], 
                        face_data_list[j]['features']
                    )
                    
                    # ë””ë²„ê¹… ì •ë³´ ì¤€ë¹„
                    debug_info = {
                        'face1': face_data_list[i]['filename'],
                        'face2': face_data_list[j]['filename']
                    }
                    
                    # í•©ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ë””ë²„ê¹… ì •ë³´ í¬í•¨)
                    consensus_similarity = self.recognizer.get_consensus_similarity(similarities, debug_info if self.debug_mode else None)
                    
                    # ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì£¼ì˜ ë¡œê¹…
                    if consensus_similarity >= self.similarity_threshold:
                        self.logger.warning(f"âš ï¸ ì„ê³„ê°’ ì´ˆê³¼: {face_data_list[i]['filename']} vs {face_data_list[j]['filename']} = {consensus_similarity:.3f}")
                    
                    similarity_matrix[i, j] = similarity_matrix[j, i] = consensus_similarity
                    
                except Exception as e:
                    self.logger.error(f"í¬ë¡œìŠ¤ì²´í¬ ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
                    similarity_matrix[i, j] = similarity_matrix[j, i] = 0.0
        
        # 2. ì—°ê²°ì„± ê¸°ë°˜ ê·¸ë£¹í•‘ (í¬ë¡œìŠ¤ì²´í¬ ì„ê³„ê°’ ì ìš©)
        adjacency_matrix = similarity_matrix >= self.similarity_threshold
        
        # 3. ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸(ê·¸ë£¹) ì°¾ê¸° (DFS)
        visited = [False] * N
        groups = []
        
        for i in range(N):
            if not visited[i]:
                group = []
                self._dfs_clustering(i, N, adjacency_matrix, visited, group, face_data_list)
                
                if len(group) >= self.min_group_size:
                    groups.append(group)
        
        return groups
    
    def _dfs_clustering(self, node: int, N: int, adjacency_matrix: np.ndarray, 
                       visited: List[bool], group: List[Dict], face_data_list: List[Dict]):
        """DFSë¥¼ ì´ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§"""
        visited[node] = True
        group.append(face_data_list[node])
        
        for neighbor in range(N):
            if not visited[neighbor] and adjacency_matrix[node, neighbor]:
                self._dfs_clustering(neighbor, N, adjacency_matrix, visited, group, face_data_list)
    
    def validate_group_consistency(self, group: List[Dict]) -> bool:
        """ê·¸ë£¹ ë‚´ ì¼ê´€ì„± ê²€ì¦ (ë§¤ìš° ì—„ê²©í•œ ê²€ì¦)"""
        if len(group) < 2:
            return True
        
        failed_pairs = 0
        total_pairs = 0
        
        # ê·¸ë£¹ ë‚´ ëª¨ë“  ìŒì˜ ìœ ì‚¬ë„ í™•ì¸
        for i in range(len(group)):
            for j in range(i+1, len(group)):
                total_pairs += 1
                similarities = self.recognizer.calculate_similarity(
                    group[i]['features'], group[j]['features']
                )
                
                debug_info = {
                    'face1': group[i]['filename'],
                    'face2': group[j]['filename']
                }
                
                consensus = self.recognizer.get_consensus_similarity(similarities, debug_info if self.debug_mode else None)
                
                if consensus < self.consensus_threshold:
                    failed_pairs += 1
                    self.logger.warning(f"âš ï¸ ì¼ê´€ì„± ë¶€ì¡±: {group[i]['filename']} vs {group[j]['filename']} = {consensus:.3f} < {self.consensus_threshold}")
        
        # 80% ì´ìƒì˜ ìŒì´ ì„ê³„ê°’ì„ ë§Œì¡±í•´ì•¼ í•¨
        consistency_rate = (total_pairs - failed_pairs) / total_pairs if total_pairs > 0 else 0
        min_consistency_rate = 0.8
        
        is_consistent = consistency_rate >= min_consistency_rate
        
        if not is_consistent:
            self.logger.warning(f"ê·¸ë£¹ ì¼ê´€ì„± ê²€ì¦ ì‹¤íŒ¨: {consistency_rate:.2%} < {min_consistency_rate:.0%}")
        
        return is_consistent
    
    def _split_large_group(self, large_group: List[Dict]) -> List[List[Dict]]:
        """ëŒ€í˜• ê·¸ë£¹ì„ ë” ì‘ì€ ê·¸ë£¹ë“¤ë¡œ ë¶„í• """
        self.logger.info(f"ëŒ€í˜• ê·¸ë£¹ ë¶„í•  ì‹œì‘: {len(large_group)}ê°œ ì–¼êµ´")
        
        # ë” ë†’ì€ ì„ê³„ê°’ìœ¼ë¡œ ì¬ê·¸ë£¹í•‘
        original_threshold = self.similarity_threshold
        self.similarity_threshold = min(0.95, self.similarity_threshold + 0.05)  # ì„ê³„ê°’ ìƒí–¥
        
        try:
            # ëŒ€í˜• ê·¸ë£¹ì„ ë‹¤ì‹œ í´ëŸ¬ìŠ¤í„°ë§
            sub_groups = self._perform_crosscheck_clustering(large_group)
            
            # ê° ì„œë¸Œê·¸ë£¹ì´ ìµœëŒ€ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì¶”ê°€ ë¶„í• 
            final_groups = []
            for sub_group in sub_groups:
                if len(sub_group) <= self.max_group_size:
                    final_groups.append(sub_group)
                else:
                    # ì—¬ì „íˆ í¬ë‹¤ë©´ ê°•ì œë¡œ ë¶„í• 
                    chunks = [sub_group[i:i + self.max_group_size] 
                             for i in range(0, len(sub_group), self.max_group_size)]
                    final_groups.extend(chunks)
            
            self.logger.info(f"ëŒ€í˜• ê·¸ë£¹ ë¶„í•  ì™„ë£Œ: {len(final_groups)}ê°œ ì„œë¸Œê·¸ë£¹ ìƒì„±")
            return final_groups
            
        finally:
            # ì›ë˜ ì„ê³„ê°’ ë³µì›
            self.similarity_threshold = original_threshold

class UnifiedAIGroupingProcessor:
    """í†µí•© AI ì–¼êµ´ ê·¸ë£¹í•‘ ì²˜ë¦¬ê¸° (í¬ë¡œìŠ¤ì²´í¬ ì‹œìŠ¤í…œ)"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = get_logger(__name__)
        self.dry_run = config.get('dry_run', False)
        self.verbose = config.get('verbose', False)
        
        # í™˜ê²½ ë¶„ì„
        self.env_analyzer = EnvironmentAnalyzer()
        env_info = self.env_analyzer.analyze_environment()
        
        # í¬ë¡œìŠ¤ì²´í¬ ì¸ì‹ê¸° ì´ˆê¸°í™”
        self.recognizer = CrossCheckRecognizer()
        
        # ê·¸ë£¹í•‘ ì—”ì§„ ì´ˆê¸°í™”
        self.grouper = FaceGrouper(self.recognizer)
        
        # ì‚¬ìš©ì ì„¤ì • ì ìš©
        if 'similarity_threshold' in config:
            self.grouper.similarity_threshold = config['similarity_threshold']
            self.logger.info(f"ìœ ì‚¬ë„ ì„ê³„ê°’ ì¡°ì •: {config['similarity_threshold']}")
        
        if 'debug' in config:
            self.grouper.debug_mode = config['debug']
        
        # ê²½ë¡œ ì„¤ì •
        self.project_root = project_root
        self.detected_faces_dir = self.project_root / 'data' / 'domains' / 'face_recognition' / 'detected_faces'
        self.staging_dir = self.project_root / 'data' / 'domains' / 'face_recognition' / 'staging'
        self.grouped_dir = self.staging_dir / 'grouped'
        
        # ê·¸ë£¹í•‘ ë””ë ‰í† ë¦¬ ìƒì„± (dry-runì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
        if not self.dry_run:
            self.grouped_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.logger.info("DRY RUN ëª¨ë“œ: ë””ë ‰í† ë¦¬ ìƒì„± ê±´ë„ˆëœ€")
        
        # ê·¸ë£¹í•‘ í†µê³„
        self.stats = {
            'total_faces': 0,
            'processed_faces': 0,
            'groups_created': 0,
            'largest_group_size': 0,
            'errors': 0
        }
        
        self.logger.info("í¬ë¡œìŠ¤ì²´í¬ AI ê·¸ë£¹í•‘ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        if self.dry_run:
            self.logger.info("ğŸ§ª DRY RUN ëª¨ë“œ í™œì„±í™” - ì‹¤ì œ íŒŒì¼ ì´ë™ ì—†ìŒ")
        self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(self.recognizer.models)}ê°œ")
        for model_name, model_info in self.recognizer.models.items():
            self.logger.info(f"  - {model_info['name']} (ê°€ì¤‘ì¹˜: {model_info['weight']})")

    def process_grouping(self, source: str = 'all'):
        """ê·¸ë£¹í•‘ ì²˜ë¦¬ ì‹¤í–‰"""
        self.logger.info(f"AI ê·¸ë£¹í•‘ ì²˜ë¦¬ ì‹œì‘ - ì†ŒìŠ¤: {source}")
        self.logger.info(f"ì‚¬ìš© ì¸ì‹ ëª¨ë¸: {self.recognizer.model_config['name']}")
        
        # ì–¼êµ´ ë°ì´í„° ìˆ˜ì§‘
        face_data_list = self._collect_face_data(source)
        
        if not face_data_list:
            self.logger.warning("ì²˜ë¦¬í•  ì–¼êµ´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        self.stats['total_faces'] = len(face_data_list)
        
        # ê·¸ë£¹í•‘ ìˆ˜í–‰
        groups = self.grouper.group_faces(face_data_list)
        
        if not groups:
            self.logger.warning("ìƒì„±ëœ ê·¸ë£¹ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ê·¸ë£¹ ì €ì¥
        self._save_groups(groups)
        
        self._print_summary()
    
    def _collect_face_data(self, source: str) -> List[Dict]:
        """ì–¼êµ´ ë°ì´í„° ìˆ˜ì§‘"""
        face_data_list = []
        
        # ì†ŒìŠ¤ë³„ í´ë” ê²°ì •
        source_folders = []
        if source in ['from_captured', 'all']:
            source_folders.append(('from_captured', self.detected_faces_dir / 'from_captured'))
        if source in ['from_uploads', 'all']:
            source_folders.append(('from_uploads', self.detected_faces_dir / 'from_uploads'))
        if source in ['from_manual', 'all']:
            source_folders.append(('from_manual', self.detected_faces_dir / 'from_manual'))
        if source in ['auto_collected', 'all']:
            source_folders.append(('auto_collected', self.detected_faces_dir / 'auto_collected'))
        
        # ì´ë¯¸ ê·¸ë£¹í•‘ëœ ì–¼êµ´ë“¤ í™•ì¸
        grouped_faces = self._get_grouped_faces()
        
        # ê° ì†ŒìŠ¤ í´ë”ì—ì„œ ì–¼êµ´ ë°ì´í„° ìˆ˜ì§‘
        for source_name, source_path in source_folders:
            self.logger.info(f"ìˆ˜ì§‘ ì¤‘: {source_path}")
            
            # ì´ë¯¸ì§€ íŒŒì¼ë“¤ë§Œ ì°¾ê¸° (JSON íŒŒì¼ ì œì™¸)
            image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}
            image_files = []
            for ext in image_extensions:
                image_files.extend(source_path.glob(f'*{ext}'))
                image_files.extend(source_path.glob(f'*{ext.upper()}'))
            
            # JSON íŒŒì¼ ì œì™¸ (ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì²˜ë¦¬)
            image_files = [f for f in image_files if not f.name.endswith('.json')]
            
            self.logger.info(f"{source_name}ì—ì„œ {len(image_files)}ê°œ ì–¼êµ´ ì´ë¯¸ì§€ ë°œê²¬")
            
            # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€: ì´ë¯¸ ê·¸ë£¹í•‘ëœ ì–¼êµ´ ì œì™¸
            new_faces = []
            skipped_faces = []
            
            for image_path in image_files:
                if self._is_already_grouped(image_path, grouped_faces):
                    skipped_faces.append(image_path.name)
                else:
                    new_faces.append(image_path)
            
            if skipped_faces:
                self.logger.info(f"ì´ë¯¸ ê·¸ë£¹í•‘ëœ ì–¼êµ´ {len(skipped_faces)}ê°œ ê±´ë„ˆëœ€: {', '.join(skipped_faces[:5])}{'...' if len(skipped_faces) > 5 else ''}")
            
            for image_path in new_faces:
                # ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸
                metadata_path = image_path.with_suffix('.json')
                metadata = {}
                if metadata_path.exists():
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                    except Exception as e:
                        self.logger.warning(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ {metadata_path}: {e}")
                
                face_data = {
                    'face_path': str(image_path),
                    'source': source_name,
                    'metadata': metadata,
                    'filename': image_path.name
                }
                
                face_data_list.append(face_data)
        
        self.logger.info(f"ì´ {len(face_data_list)}ê°œ ì–¼êµ´ ë°ì´í„° ìˆ˜ì§‘ë¨ (ìƒˆë¡œìš´ ì–¼êµ´ë§Œ)")
        return face_data_list
    
    def _get_grouped_faces(self) -> Set[str]:
        """ì´ë¯¸ ê·¸ë£¹í•‘ëœ ì–¼êµ´ë“¤ì˜ ê²½ë¡œ ìˆ˜ì§‘"""
        grouped_faces = set()
        
        if not self.grouped_dir.exists():
            return grouped_faces
        
        # ëª¨ë“  ê·¸ë£¹ í´ë”ì—ì„œ ì–¼êµ´ íŒŒì¼ë“¤ ìˆ˜ì§‘
        for group_dir in self.grouped_dir.iterdir():
            if group_dir.is_dir():
                # ê·¸ë£¹ ë©”íƒ€ë°ì´í„° í™•ì¸
                metadata_path = group_dir / 'group_metadata.json'
                if metadata_path.exists():
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            group_metadata = json.load(f)
                        
                        # ê·¸ë£¹ì— í¬í•¨ëœ ì–¼êµ´ë“¤ì˜ ì›ë³¸ ê²½ë¡œ ìˆ˜ì§‘
                        for face_info in group_metadata.get('faces', []):
                            original_path = face_info.get('original_path', '')
                            if original_path:
                                grouped_faces.add(original_path)
                    except Exception as e:
                        self.logger.debug(f"ê·¸ë£¹ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ {metadata_path}: {e}")
        
        return grouped_faces
    
    def _is_already_grouped(self, face_path: Path, grouped_faces: Set[str]) -> bool:
        """ì–¼êµ´ì´ ì´ë¯¸ ê·¸ë£¹í•‘ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return str(face_path) in grouped_faces
    
    def _save_groups(self, groups: List[List[Dict]]):
        """ê·¸ë£¹ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥ (ê·¸ë£¹í•‘ ì—”ì§„ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì €ì¥)"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ê¸°ì¡´ ê·¸ë£¹ë“¤ ë¡œë“œ (ì°¸ê³ ìš©)
        existing_groups = self._load_existing_groups()
        
        for group_idx, group in enumerate(groups):
            # ê·¸ë£¹í•‘ ì—”ì§„ì˜ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ìƒˆ ê·¸ë£¹ìœ¼ë¡œ ìƒì„±
            group_name = f"group_{len(existing_groups) + group_idx:03d}_{timestamp}"
            self._create_new_group(group, group_name)
            self.logger.info(f"ìƒˆ ê·¸ë£¹ ìƒì„±: {group_name} ({len(group)}ê°œ ì–¼êµ´)")
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            if len(group) > self.stats['largest_group_size']:
                self.stats['largest_group_size'] = len(group)
        
        self.stats['groups_created'] = len(groups)
    
    def _load_existing_groups(self) -> List[Dict]:
        """ê¸°ì¡´ ê·¸ë£¹ë“¤ ë¡œë“œ"""
        existing_groups = []
        
        if not self.grouped_dir.exists():
            return existing_groups
        
        for group_dir in self.grouped_dir.iterdir():
            if group_dir.is_dir():
                metadata_path = group_dir / 'group_metadata.json'
                if metadata_path.exists():
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            group_metadata = json.load(f)
                        existing_groups.append(group_metadata)
                    except Exception as e:
                        self.logger.debug(f"ê·¸ë£¹ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ {metadata_path}: {e}")
        
        return existing_groups
    
    def _create_new_group(self, group: List[Dict], group_name: str):
        """ìƒˆ ê·¸ë£¹ ìƒì„±"""
        group_dir = self.grouped_dir / group_name
        
        if self.dry_run:
            self.logger.info(f"DRY RUN: ê·¸ë£¹ ë””ë ‰í† ë¦¬ ìƒì„± ì‹œë®¬ë ˆì´ì…˜ - {group_dir}")
        else:
            group_dir.mkdir(exist_ok=True)
        
        # ê·¸ë£¹ ë©”íƒ€ë°ì´í„°
        group_metadata = {
            'group_id': group_name,
            'created_at': datetime.now().strftime('%Y%m%d_%H%M%S'),
            'face_count': len(group),
            'recognition_model': self.recognizer.model_config['name'],
            'dry_run': self.dry_run,
            'faces': []
        }
        
        # ê° ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ê·¸ë£¹ í´ë”ë¡œ ë³µì‚¬
        for face_idx, face_data in enumerate(group):
            source_path = Path(face_data['face_path'])
            
            # ìƒˆ íŒŒì¼ëª… ìƒì„±
            new_filename = f"face_{face_idx:03d}_{source_path.name}"
            dest_path = group_dir / new_filename
            
            # ê·¸ë£¹ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
            face_info = {
                'original_path': str(source_path),
                'new_path': str(dest_path),
                'source': face_data['source'],
                'original_metadata': face_data['metadata']
            }
            group_metadata['faces'].append(face_info)
            
            if self.dry_run:
                # DRY RUN ëª¨ë“œ: ì‹¤ì œ ë³µì‚¬ëŠ” í•˜ì§€ ì•Šê³  ë¡œê·¸ë§Œ
                self.logger.debug(f"DRY RUN: íŒŒì¼ ë³µì‚¬ ì‹œë®¬ë ˆì´ì…˜ - {source_path} -> {dest_path}")
                self.stats['processed_faces'] += 1
            else:
                # ì‹¤ì œ íŒŒì¼ ë³µì‚¬
                try:
                    if not source_path.exists():
                        self.logger.error(f"ì†ŒìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {source_path}")
                        self.stats['errors'] += 1
                        continue
                    
                    shutil.copy2(source_path, dest_path)
                    self.stats['processed_faces'] += 1
                    
                    if self.verbose:
                        self.logger.debug(f"íŒŒì¼ ë³µì‚¬ ì™„ë£Œ: {source_path} -> {dest_path}")
                    
                except Exception as e:
                    self.logger.error(f"íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨ {source_path} -> {dest_path}: {e}")
                    self.stats['errors'] += 1
        
        # ê·¸ë£¹ ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_path = group_dir / 'group_metadata.json'
        
        if self.dry_run:
            self.logger.info(f"DRY RUN: ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹œë®¬ë ˆì´ì…˜ - {metadata_path}")
            if self.verbose:
                self.logger.debug(f"DRY RUN: ë©”íƒ€ë°ì´í„° ë‚´ìš© - {len(group_metadata['faces'])}ê°œ ì–¼êµ´")
        else:
            try:
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(group_metadata, f, indent=2, ensure_ascii=False)
                
                if self.verbose:
                    self.logger.debug(f"ê·¸ë£¹ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_path}")
                    
            except Exception as e:
                self.logger.error(f"ê·¸ë£¹ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ {metadata_path}: {e}")
                self.stats['errors'] += 1
    
    def _print_summary(self):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½"""
        print("\n" + "="*60)
        print("ğŸ“Š AI ê·¸ë£¹í•‘ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
        if self.dry_run:
            print("ğŸ§ª DRY RUN ëª¨ë“œ - ì‹¤ì œ íŒŒì¼ ì´ë™ ì—†ìŒ")
        print("="*60)
        print(f"ğŸ¤– ì‚¬ìš© ì¸ì‹ ëª¨ë¸: {self.recognizer.model_config['name']}")
        print(f"ğŸ‘¤ ì´ ì–¼êµ´ ìˆ˜: {self.stats['total_faces']}ê°œ")
        print(f"âœ… ì²˜ë¦¬ëœ ì–¼êµ´: {self.stats['processed_faces']}ê°œ")
        print(f"ğŸ‘¥ {'ì‹œë®¬ë ˆì´ì…˜ëœ' if self.dry_run else 'ìƒì„±ëœ'} ê·¸ë£¹: {self.stats['groups_created']}ê°œ")
        print(f"ğŸ“ˆ ìµœëŒ€ ê·¸ë£¹ í¬ê¸°: {self.stats['largest_group_size']}ê°œ")
        print(f"âŒ ì˜¤ë¥˜: {self.stats['errors']}ê°œ")
        if self.dry_run:
            print("ğŸ’¡ ì‹¤ì œ ì‹¤í–‰í•˜ë ¤ë©´ --dry-run ì˜µì…˜ì„ ì œê±°í•˜ì„¸ìš”")
        print("="*60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í†µí•© AI ì–¼êµ´ ê·¸ë£¹í•‘ ì²˜ë¦¬ê¸°")
    parser.add_argument(
        "--source", 
        choices=['from_captured', 'from_uploads', 'from_manual', 'auto_collected', 'all'], 
        default='all',
        help="ì²˜ë¦¬í•  ì†ŒìŠ¤ í´ë” (ê¸°ë³¸ê°’: all)"
    )
    parser.add_argument(
        "--config", 
        type=str, 
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ íŒŒì¼ ì´ë™ ì—†ì´ í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="ë””ë²„ê¹… ëª¨ë“œ í™œì„±í™” (ìœ ì‚¬ë„ ìƒì„¸ ë¶„ì„)"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.75,
        help="ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì • (ê¸°ë³¸ê°’: 0.75)"
    )
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    setup_logging()
    logger = get_logger(__name__)
    
    # ì˜ì¡´ì„± ê²€ì‚¬ ê²°ê³¼ ì¶œë ¥
    print("\nğŸ” ì‹œìŠ¤í…œ ì˜ì¡´ì„± ê²€ì‚¬:")
    print(f"  - OpenCV: âœ… ì‚¬ìš© ê°€ëŠ¥")
    print(f"  - MediaPipe: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if MP_AVAILABLE else 'âŒ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ'}")
    print(f"  - Dlib: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if DLIB_AVAILABLE else 'âŒ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ'}")
    print(f"  - ONNX Runtime: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if ONNX_AVAILABLE else 'âŒ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ'}")
    
    if not MP_AVAILABLE and not DLIB_AVAILABLE:
        print("\nâš ï¸ ê²½ê³ : ì–¼êµ´ ì •ë ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•´ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("   - pip install mediapipe")
        print("   - pip install dlib")
    
    if not ONNX_AVAILABLE:
        print("\nâš ï¸ ê²½ê³ : ONNX Runtimeì´ ì—†ì–´ ê³ ì„±ëŠ¥ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜: pip install onnxruntime")
    
    try:
        # ì„¤ì • ë¡œë“œ
        config = {}
        if args.config:
            if Path(args.config).exists():
                config = load_config(args.config)
                logger.info(f"ì„¤ì • íŒŒì¼ ë¡œë“œë¨: {args.config}")
            else:
                logger.warning(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
        
        # ê¸°ë³¸ ì„¤ì • ì¶”ê°€
        config.setdefault('dry_run', args.dry_run)
        config.setdefault('verbose', args.verbose)
        config.setdefault('debug', args.debug)
        config.setdefault('similarity_threshold', args.threshold)
        
        if args.debug:
            logger.info("ğŸ” ë””ë²„ê¹… ëª¨ë“œ í™œì„±í™” - ìœ ì‚¬ë„ ìƒì„¸ ë¶„ì„ì„ ì¶œë ¥í•©ë‹ˆë‹¤")
        
        logger.info(f"ğŸ¯ ìœ ì‚¬ë„ ì„ê³„ê°’: {args.threshold}")
        
        print(f"\nğŸš€ AI ê·¸ë£¹í•‘ ì²˜ë¦¬ ì‹œì‘ (ì†ŒìŠ¤: {args.source})")
        if args.dry_run:
            print("   ğŸ“‹ DRY RUN ëª¨ë“œ: ì‹¤ì œ íŒŒì¼ ì´ë™ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # AI ê·¸ë£¹í•‘ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
        processor = UnifiedAIGroupingProcessor(config)
        processor.process_grouping(args.source)
        
        logger.info("AI ê·¸ë£¹í•‘ ì²˜ë¦¬ ì™„ë£Œ")
        print("\nâœ… AI ê·¸ë£¹í•‘ ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 130  # SIGINT exit code
        
    except FileNotFoundError as e:
        logger.error(f"íŒŒì¼/í´ë” ì—†ìŒ: {e}")
        print(f"âŒ íŒŒì¼/í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return 2
        
    except PermissionError as e:
        logger.error(f"ê¶Œí•œ ì˜¤ë¥˜: {e}")
        print(f"âŒ íŒŒì¼ ê¶Œí•œ ì˜¤ë¥˜: {e}")
        return 3
        
    except Exception as e:
        logger.error(f"AI ê·¸ë£¹í•‘ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
        print(f"âŒ ì˜¤ë¥˜: {e}")
        
        if args.verbose:
            import traceback
            print("\nìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
            traceback.print_exc()
        
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 