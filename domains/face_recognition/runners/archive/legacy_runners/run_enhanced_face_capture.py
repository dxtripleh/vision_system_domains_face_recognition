#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ

ìƒˆë¡œìš´ ë„ë©”ì¸ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ê³  ëª¨ë“  ìš”ì²­ì‚¬í•­ì„ êµ¬í˜„:
1. ií‚¤ë¡œ ì •ë³´ í† ê¸€ (ê°™ì€ ì°½ ì‚¬ìš©)
2. ìë™ ëª¨ë“œì—ì„œ ì–¼êµ´ ìë™ ì €ì¥ (ê°ë„/ì¡°ëª… ë³€í™” ê°ì§€)
3. ëª…í™•í•œ ë°ì´í„° í”Œë¡œìš°
4. ì²´ê³„ì ì¸ í´ë” êµ¬ì¡°
5. í™˜ê²½ ë¶„ì„ì„ í†µí•œ ìµœì  ëª¨ë¸ ì„ íƒ
6. GUI ì´ë¦„ ì…ë ¥
7. ëª¨ë¸ ì„¤ì • ì‹¤ì‹œê°„ ì¡°ì ˆ
"""

import os
import sys
import cv2
import time
import json
import numpy as np
import tkinter as tk
from tkinter import simpledialog
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import threading

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent.parent
sys.path.append(str(project_root))

from common.logging import setup_logging, get_logger
from common.config import load_config

class EnvironmentAnalyzer:
    """í™˜ê²½ ë¶„ì„ì„ í†µí•œ ìµœì  ëª¨ë¸ ì„ íƒ"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
    def analyze_environment(self) -> Dict:
        """í™˜ê²½ ë¶„ì„ ë° ìµœì  ëª¨ë¸ ì¶”ì²œ"""
        env_info = {
            'gpu_available': self._check_gpu(),
            'cpu_cores': os.cpu_count(),
            'available_models': self._scan_available_models(),
            'recommended_model': None
        }
        
        # ìµœì  ëª¨ë¸ ì„ íƒ
        env_info['recommended_model'] = self._select_optimal_model(env_info)
        
        return env_info
    
    def _check_gpu(self) -> bool:
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            try:
                # OpenCV DNN GPU ì§€ì› í™•ì¸
                return cv2.cuda.getCudaEnabledDeviceCount() > 0
            except:
                return False
    
    def _scan_available_models(self) -> List[Dict]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìŠ¤ìº”"""
        models_dir = project_root / 'models' / 'weights'
        available_models = []
        yunet_path = models_dir / 'face_detection_yunet_2023mar.onnx'
        print('[DEBUG] YuNet ONNX exists:', yunet_path.exists(), yunet_path)
        if yunet_path.exists():
            available_models.append({
                'name': 'YuNet',
                'type': 'yunet',
                'path': str(yunet_path),
                'speed': 'fast',
                'accuracy': 'high',
                'gpu_required': False,
                'supports_landmarks': True
            })
        print('[DEBUG] Available models:', available_models)
        return available_models
    
    def _select_optimal_model(self, env_info: Dict) -> Dict:
        """ìµœì  ëª¨ë¸ ì„ íƒ"""
        available_models = env_info['available_models']
        print('[DEBUG] Selecting model from:', available_models)
        if available_models:
            print('[DEBUG] Selected model:', available_models[0])
            return available_models[0]
        return None

class FaceDetector:
    """í†µí•© ì–¼êµ´ ê²€ì¶œê¸°"""
    
    def __init__(self, model_config: Dict):
        self.model_config = model_config
        self.model_type = model_config['type']
        self.logger = get_logger(__name__)
        
        # ëª¨ë¸ë³„ ì„¤ì •
        self.confidence_threshold = 0.5
        self.scale_factor = 1.1
        self.min_neighbors = 5
        self.min_size = (30, 30)
        
        self._initialize_model()
    
    def _initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print('[DEBUG] Initializing model:', self.model_config)
        try:
            if self.model_type == 'yunet':
                self.detector = cv2.dnn.readNetFromONNX(self.model_config['path'])
                print('[DEBUG] YuNet loaded:', not self.detector.empty())
                return not self.detector.empty()
            else:
                print('[DEBUG] Unknown model type:', self.model_type)
                return False
        except Exception as e:
            print('[DEBUG] Model init error:', e)
            return False
    
    def detect_faces(self, image: np.ndarray) -> List[Dict]:
        """ì–¼êµ´ ê²€ì¶œ"""
        if self.detector is None:
            self.logger.error("ê²€ì¶œê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return []
        
        try:
            if self.model_type == 'yunet':
                return self._detect_yunet(image)
            else:
                self.logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
                return []
        except Exception as e:
            self.logger.error(f"ê²€ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _detect_yunet(self, image: np.ndarray) -> List[Dict]:
        """YuNet ê²€ì¶œ - ONNX Runtime ë²„ì „"""
        import onnxruntime as ort
        
        h, w = image.shape[:2]
        
        # ONNX Runtime ì„¸ì…˜ ìƒì„± (í•œ ë²ˆë§Œ)
        if not hasattr(self, 'onnx_session'):
            try:
                self.onnx_session = ort.InferenceSession(self.model_config['path'])
                print('[DEBUG] YuNet ONNX Runtime session created')
            except Exception as e:
                print(f'[DEBUG] YuNet ONNX Runtime session error: {e}')
                return []
        
        # ì…ë ¥ ì „ì²˜ë¦¬
        input_size = (640, 640)
        blob = cv2.dnn.blobFromImage(image, 1.0, input_size, (104, 117, 123), swapRB=True, crop=False)
        
        try:
            # ONNX Runtimeìœ¼ë¡œ ì¶”ë¡  - ëª¨ë“  ì¶œë ¥ ê°€ì ¸ì˜¤ê¸°
            input_name = self.onnx_session.get_inputs()[0].name
            output_names = [output.name for output in self.onnx_session.get_outputs()]
            
            print(f'[DEBUG] YuNet input name: {input_name}')
            print(f'[DEBUG] YuNet output names: {output_names}')
            
            outputs = self.onnx_session.run(output_names, {input_name: blob})
            
            print('[DEBUG] YuNet ONNX Runtime outputs:')
            for i, (name, output) in enumerate(zip(output_names, outputs)):
                print(f'  {name}: shape={output.shape}, sample={output.flatten()[:5]}')
            
            # ì²« ë²ˆì§¸ ì¶œë ¥ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
            outputs = outputs[0]
        except Exception as e:
            print(f'[DEBUG] YuNet ONNX Runtime inference error: {e}')
            return []
        
        detections = []
        
        # ì¶œë ¥ í˜•íƒœ í™•ì¸ ë° ì²˜ë¦¬
        if len(outputs.shape) == 3:
            detections_data = outputs[0] if outputs.shape[0] == 1 else outputs
        elif len(outputs.shape) == 2:
            detections_data = outputs
        else:
            print('[DEBUG] Unexpected YuNet output shape:', outputs.shape)
            return []
        
        for i, detection in enumerate(detections_data):
            if i < 2:
                print(f'[DEBUG] detection[{i}]:', detection)
            
            # ì¶œë ¥ ê¸¸ì´ì— ë”°ë¥¸ ì²˜ë¦¬
            if len(detection) < 5:
                continue
                
            # ë‹¤ì–‘í•œ ì¶œë ¥ êµ¬ì¡° ì‹œë„
            confidence = None
            x1, y1, x2, y2 = 0, 0, 0, 0
            
            # êµ¬ì¡° 1: [x, y, w, h, confidence, ...]
            if len(detection) >= 5:
                try:
                    x = float(detection[0])
                    y = float(detection[1])
                    width = float(detection[2])
                    height = float(detection[3])
                    confidence = float(detection[4])
                    
                    # ì¢Œí‘œë¥¼ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§
                    x1 = int(x * w)
                    y1 = int(y * h)
                    x2 = int((x + width) * w)
                    y2 = int((y + height) * h)
                except:
                    pass
            
            # êµ¬ì¡° 2: [x1, y1, x2, y2, confidence, ...]
            if confidence is None and len(detection) >= 5:
                try:
                    x1 = int(float(detection[0]) * w)
                    y1 = int(float(detection[1]) * h)
                    x2 = int(float(detection[2]) * w)
                    y2 = int(float(detection[3]) * h)
                    confidence = float(detection[4])
                except:
                    pass
            
            if confidence is None:
                continue
                
            # ì¢Œí‘œ ê²€ì¦
            if x1 < 0 or y1 < 0 or x2 > w or y2 > h:
                continue
            if x2 <= x1 or y2 <= y1:
                continue
            if (x2 - x1) < 20 or (y2 - y1) < 20:
                continue
            
            # ëœë“œë§ˆí¬ ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
            landmarks = []
            if len(detection) >= 15:
                try:
                    for j in range(5):
                        lx = int(detection[5 + j * 2] * w)
                        ly = int(detection[6 + j * 2] * h)
                        landmarks.append((lx, ly))
                except:
                    pass
            
            if i < 2:
                print(f'[DEBUG] detection[{i}] íŒŒì‹± ê²°ê³¼:')
                print(f'  confidence={confidence:.3f}')
                print(f'  bbox=({x1}, {y1}, {x2-x1}, {y2-y1})')
                print(f'  landmarks={landmarks}')
            
            # ì‹ ë¢°ë„ í•„í„°ë§
            if confidence > self.confidence_threshold:
                bbox = (x1, y1, x2 - x1, y2 - y1)
                detections.append({
                    'bbox': bbox,
                    'confidence': confidence,
                    'landmarks': landmarks,
                    'quality_score': self._calculate_quality_score(x1, y1, x2 - x1, y2 - y1, image.shape)
                })
        
        return detections
    
    def _generate_haar_landmarks(self, x: int, y: int, w: int, h: int) -> List[Tuple[int, int]]:
        """Haar Cascadeìš© ëœë“œë§ˆí¬ ìƒì„± (5ì )"""
        landmarks = []
        # ì™¼ìª½ ëˆˆ
        landmarks.append((x + int(w * 0.3), y + int(h * 0.35)))
        # ì˜¤ë¥¸ìª½ ëˆˆ
        landmarks.append((x + int(w * 0.7), y + int(h * 0.35)))
        # ì½”
        landmarks.append((x + int(w * 0.5), y + int(h * 0.5)))
        # ì™¼ìª½ ì…ê¼¬ë¦¬
        landmarks.append((x + int(w * 0.3), y + int(h * 0.7)))
        # ì˜¤ë¥¸ìª½ ì…ê¼¬ë¦¬
        landmarks.append((x + int(w * 0.7), y + int(h * 0.7)))
        return landmarks
    
    def _generate_ultraface_landmarks(self, x: int, y: int, w: int, h: int) -> List[Tuple[int, int]]:
        """UltraFaceìš© ëœë“œë§ˆí¬ ìƒì„± (5ì )"""
        landmarks = []
        # ì™¼ìª½ ëˆˆ
        landmarks.append((x + int(w * 0.3), y + int(h * 0.35)))
        # ì˜¤ë¥¸ìª½ ëˆˆ
        landmarks.append((x + int(w * 0.7), y + int(h * 0.35)))
        # ì½”
        landmarks.append((x + int(w * 0.5), y + int(h * 0.5)))
        # ì™¼ìª½ ì…ê¼¬ë¦¬
        landmarks.append((x + int(w * 0.3), y + int(h * 0.7)))
        # ì˜¤ë¥¸ìª½ ì…ê¼¬ë¦¬
        landmarks.append((x + int(w * 0.7), y + int(h * 0.7)))
        return landmarks
    
    def _calculate_quality_score(self, x: int, y: int, w: int, h: int, image_shape: Tuple) -> float:
        """ì–¼êµ´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        # í¬ê¸° ì ìˆ˜
        face_area = w * h
        image_area = image_shape[0] * image_shape[1]
        size_ratio = face_area / image_area if image_area > 0 else 0
        size_score = min(size_ratio * 10, 1.0)
        
        # ìœ„ì¹˜ ì ìˆ˜ (ì¤‘ì•™ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        center_x, center_y = x + w//2, y + h//2
        img_center_x, img_center_y = image_shape[1]//2, image_shape[0]//2
        distance = ((center_x - img_center_x)**2 + (center_y - img_center_y)**2)**0.5
        max_distance = (img_center_x**2 + img_center_y**2)**0.5
        position_score = 1.0 - (distance / max_distance) if max_distance > 0 else 1.0
        
        return (size_score * 0.7 + position_score * 0.3)
    
    def adjust_confidence_threshold(self, delta: float):
        """ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì ˆ"""
        self.confidence_threshold = max(0.1, min(0.9, self.confidence_threshold + delta))
        self.logger.info(f"ì‹ ë¢°ë„ ì„ê³„ê°’: {self.confidence_threshold:.2f}")
    
    def adjust_scale_factor(self, delta: float):
        """ìŠ¤ì¼€ì¼ íŒ©í„° ì¡°ì ˆ (Haar Cascadeìš©)"""
        if self.model_type in ['haar', 'haar_default']:
            self.scale_factor = max(1.05, min(2.0, self.scale_factor + delta))
            self.logger.info(f"ìŠ¤ì¼€ì¼ íŒ©í„°: {self.scale_factor:.2f}")
    
    def adjust_min_neighbors(self, delta: int):
        """ìµœì†Œ ì´ì›ƒ ìˆ˜ ì¡°ì ˆ (Haar Cascadeìš©)"""
        if self.model_type in ['haar', 'haar_default']:
            self.min_neighbors = max(1, min(20, self.min_neighbors + delta))
            self.logger.info(f"ìµœì†Œ ì´ì›ƒ ìˆ˜: {self.min_neighbors}")

class FaceQualityAnalyzer:
    """ì–¼êµ´ í’ˆì§ˆ ë¶„ì„ê¸° (ìë™ ëª¨ë“œìš©)"""
    
    def __init__(self):
        self.face_history = []
        self.similarity_threshold = 0.8
        self.quality_threshold = 0.6
        
    def should_save_face(self, face_data: Dict, current_faces: List[Dict]) -> bool:
        """ì–¼êµ´ ì €ì¥ ì—¬ë¶€ ê²°ì •"""
        # í’ˆì§ˆ ì ìˆ˜ í™•ì¸
        if face_data['quality_score'] < self.quality_threshold:
            return False
        
        # ê¸°ì¡´ ì €ì¥ëœ ì–¼êµ´ê³¼ ë¹„êµ
        for saved_face in self.face_history[-10:]:  # ìµœê·¼ 10ê°œë§Œ ë¹„êµ
            if self._calculate_similarity(face_data, saved_face) > self.similarity_threshold:
                # ìœ ì‚¬í•œ ì–¼êµ´ì´ ìˆì§€ë§Œ í’ˆì§ˆì´ ë” ì¢‹ìœ¼ë©´ ì €ì¥
                if face_data['quality_score'] > saved_face['quality_score'] + 0.1:
                    return True
                return False
        
        return True
    
    def _calculate_similarity(self, face1: Dict, face2: Dict) -> float:
        """ì–¼êµ´ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë°”ìš´ë”© ë°•ìŠ¤ ê¸°ë°˜)"""
        bbox1 = face1['bbox']
        bbox2 = face2['bbox']
        
        # IoU ê³„ì‚°
        x1 = max(bbox1[0], bbox2[0])
        y1 = max(bbox1[1], bbox2[1])
        x2 = min(bbox1[0] + bbox1[2], bbox2[0] + bbox2[2])
        y2 = min(bbox1[1] + bbox1[3], bbox2[1] + bbox2[3])
        
        if x2 <= x1 or y2 <= y1:
            return 0.0
        
        intersection = (x2 - x1) * (y2 - y1)
        area1 = bbox1[2] * bbox1[3]
        area2 = bbox2[2] * bbox2[3]
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def add_saved_face(self, face_data: Dict):
        """ì €ì¥ëœ ì–¼êµ´ ê¸°ë¡"""
        self.face_history.append(face_data)
        if len(self.face_history) > 50:  # ìµœëŒ€ 50ê°œ ê¸°ë¡
            self.face_history.pop(0)

class NameInputDialog:
    """GUI ì´ë¦„ ì…ë ¥ ëŒ€í™”ìƒì"""
    
    @staticmethod
    def get_person_name(face_count: int = 1) -> Optional[str]:
        """ì´ë¦„ ì…ë ¥ ë°›ê¸°"""
        try:
            root = tk.Tk()
            root.withdraw()  # ë©”ì¸ ì°½ ìˆ¨ê¸°ê¸°
            
            title = f"{face_count}ê°œ ì–¼êµ´ì˜ ì¸ë¬¼ ì´ë¦„ ì…ë ¥"
            prompt = "ì¸ë¬¼ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ì·¨ì†Œí•˜ë©´ ê±´ë„ˆë›°ê¸°):"
            
            name = simpledialog.askstring(title, prompt)
            root.destroy()
            
            return name.strip() if name else None
            
        except Exception as e:
            print(f"GUI ì…ë ¥ ì˜¤ë¥˜: {e}")
            return None

class EnhancedFaceCaptureSystem:
    """ê°œì„ ëœ ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ"""
    
    def __init__(self, camera_id: int = 0):
        # ë¡œê¹… ì„¤ì •
        setup_logging()
        self.logger = get_logger(__name__)
        
        # í™˜ê²½ ë¶„ì„ ë° ìµœì  ëª¨ë¸ ì„ íƒ
        self.env_analyzer = EnvironmentAnalyzer()
        self.env_info = self.env_analyzer.analyze_environment()
        
        # ì¹´ë©”ë¼ ì„¤ì •
        self.camera_id = camera_id
        self.cap = None
        
        # ëª¨ë¸ ê´€ë¦¬
        self.available_models = self.env_info['available_models']
        self.current_model_index = 0
        self.current_model = self.env_info['recommended_model']
        if self.current_model:
            for i, model in enumerate(self.available_models):
                if model['name'] == self.current_model['name']:
                    self.current_model_index = i
                    break
        
        # ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”
        self.face_detector = FaceDetector(self.current_model)
        if not self.face_detector._initialize_model():
            print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {self.current_model['name']}")
            # ë‹¤ìŒ ëª¨ë¸ë¡œ ì‹œë„
            for i, model in enumerate(self.available_models):
                if i != self.current_model_index:
                    print(f"ğŸ”„ ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„: {model['name']}")
                    self.current_model = model
                    self.current_model_index = i
                    self.face_detector = FaceDetector(self.current_model)
                    if self.face_detector._initialize_model():
                        print(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ: {model['name']}")
                        break
                    else:
                        print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {model['name']}")
            else:
                print("âŒ ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return
        
        # ì–¼êµ´ í’ˆì§ˆ ë¶„ì„ê¸°
        self.quality_analyzer = FaceQualityAnalyzer()
        
        # ê²½ë¡œ ì„¤ì •
        self.domain_root = project_root / 'data' / 'domains' / 'face_recognition'
        
        self.paths = {
            # ì…ë ¥ ê²½ë¡œ (sí‚¤ìš©)
            'raw_captured': self.domain_root / 'raw_input' / 'captured',
            
            # ê²€ì¶œëœ ì–¼êµ´ ì €ì¥ ê²½ë¡œ
            'detected_manual': self.domain_root / 'detected_faces' / 'from_manual',
            'detected_auto': self.domain_root / 'detected_faces' / 'auto_collected',
            
            # ìŠ¤í…Œì´ì§• ê²½ë¡œ (cí‚¤ + ì´ë¦„ ì…ë ¥ìš©)
            'staging_named': self.domain_root / 'staging' / 'named'
        }
        
        # í´ë” ìƒì„±
        for path in self.paths.values():
            path.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë“œ ì„¤ì •
        self.is_auto_mode = False  # ê¸°ë³¸ê°’: ìˆ˜ë™ ëª¨ë“œ
        self.show_info = True
        self.show_landmarks = False  # ê¸°ë³¸ê°’: ëœë“œë§ˆí¬ í‘œì‹œ ì•ˆí•¨
        self.auto_save_enabled = True  # ìë™ ëª¨ë“œì—ì„œ ìë™ ì €ì¥ í™œì„±í™”
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.fps_counter = 0
        self.fps_timer = time.time()
        self.current_fps = 0
        
        # ì„¸ì…˜ í†µê³„
        self.session_stats = {
            'auto_saved': 0,
            'manual_captured': 0,
            'named_saved': 0
        }
        
        self.logger.info("âœ… ê°œì„ ëœ ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ“ ë„ë©”ì¸ ë£¨íŠ¸: {self.domain_root}")
        self.logger.info(f"ğŸ¤– ì„ íƒëœ ëª¨ë¸: {self.current_model['name'] if self.current_model else 'None'}")
    
    def start_capture(self):
        """ìº¡ì²˜ ì‹œì‘"""
        self.logger.info("ğŸš€ ê°œì„ ëœ ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ ì‹œì‘")
        print("="*60)
        
        # í™˜ê²½ ì •ë³´ ì¶œë ¥
        self._print_environment_info()
        
        # ì¹´ë©”ë¼ ì´ˆê¸°í™”
        if not self._initialize_camera():
            return False
        
        print("\nğŸ“¹ ì¹´ë©”ë¼ ì‹œì‘ë¨. í‚¤ë³´ë“œ ëª…ë ¹ì–´:")
        self._print_help()
        
        try:
            # ë©”ì¸ ìº¡ì²˜ ë£¨í”„
            self._capture_loop()
            
        except KeyboardInterrupt:
            print("\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            self.logger.error(f"ìº¡ì²˜ ë£¨í”„ ì˜¤ë¥˜: {e}")
        finally:
            self._cleanup()
        
        return True
    
    def _print_environment_info(self):
        """í™˜ê²½ ì •ë³´ ì¶œë ¥"""
        print(f"ğŸ” í™˜ê²½ ì •ë³´:")
        print(f"   ì¹´ë©”ë¼ ID: {self.camera_id}")
        print(f"   ê²€ì¶œ ëª¨ë¸: {self.current_model['name'] if self.current_model else 'None'}")
        print(f"   GPU ì‚¬ìš© ê°€ëŠ¥: {'YES' if self.env_info['gpu_available'] else 'NO'}")
        print(f"   CPU ì½”ì–´ ìˆ˜: {self.env_info['cpu_cores']}")
        print(f"   ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸ ìˆ˜: {len(self.available_models)}")
        print(f"   ëª¨ë“œ: {'ğŸ¤– ìë™' if self.is_auto_mode else 'ğŸ‘¤ ìˆ˜ë™'}")
        print(f"   ì •ë³´ í‘œì‹œ: {'ON' if self.show_info else 'OFF'}")
        print(f"   ëœë“œë§ˆí¬ í‘œì‹œ: {'ON' if self.show_landmarks else 'OFF'}")
        print(f"   ìë™ ì €ì¥: {'ON' if self.auto_save_enabled else 'OFF'}")
    
    def _initialize_camera(self) -> bool:
        """ì¹´ë©”ë¼ ì´ˆê¸°í™”"""
        try:
            self.cap = cv2.VideoCapture(self.camera_id, cv2.CAP_DSHOW)
            if not self.cap.isOpened():
                print(f"âŒ ì¹´ë©”ë¼ {self.camera_id} ì—°ê²° ì‹¤íŒ¨")
                return False
            
            # ì¹´ë©”ë¼ ì„¤ì •
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            
            # í…ŒìŠ¤íŠ¸ í”„ë ˆì„ ì½ê¸°
            ret, frame = self.cap.read()
            if not ret:
                print("âŒ ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            print(f"âœ… ì¹´ë©”ë¼ {self.camera_id} ì—°ê²° ì„±ê³µ ({frame.shape[1]}x{frame.shape[0]})")
            return True
            
        except Exception as e:
            print(f"âŒ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _capture_loop(self):
        """ë©”ì¸ ìº¡ì²˜ ë£¨í”„"""
        window_name = "Enhanced Face Capture System"  # ê³ ì •ëœ ì°½ ì´ë¦„
        
        while True:
            ret, frame = self.cap.read()
            if not ret:
                print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                break
            
            # ì–¼êµ´ ê²€ì¶œ
            detections = self.face_detector.detect_faces(frame)
            
            # ğŸ¤– ìë™ ëª¨ë“œì—ì„œ ìë™ ì €ì¥ ì²˜ë¦¬
            if self.is_auto_mode and self.auto_save_enabled:
                self._handle_auto_save(frame, detections)
            
            # ê²€ì¶œ ê²°ê³¼ ì‹œê°í™”
            display_frame = self._visualize_detections(frame, detections)
            
            # ì •ë³´ ì˜¤ë²„ë ˆì´
            if self.show_info:
                display_frame = self._draw_info_overlay(display_frame, detections)
            
            # í”„ë ˆì„ í‘œì‹œ
            cv2.imshow(window_name, display_frame)
            
            # FPS ì—…ë°ì´íŠ¸
            self._update_fps()
            
            # í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬
            action = self._handle_keyboard_input(cv2.waitKey(1) & 0xFF, frame, detections)
            if action == 'quit':
                break
            elif action == 'save_frame':
                self._save_frame_manual(frame, detections)
            elif action == 'capture_face':
                self._capture_face_with_name(frame, detections)
            elif action == 'toggle_mode':
                self._toggle_auto_manual_mode()
            elif action == 'toggle_info':
                self.show_info = not self.show_info
            elif action == 'increase_threshold':
                self.face_detector.adjust_confidence_threshold(0.05)
            elif action == 'decrease_threshold':
                self.face_detector.adjust_confidence_threshold(-0.05)
            elif action == 'increase_scale':
                self.face_detector.adjust_scale_factor(0.05)
            elif action == 'decrease_scale':
                self.face_detector.adjust_scale_factor(-0.05)
            elif action == 'increase_neighbors':
                self.face_detector.adjust_min_neighbors(1)
            elif action == 'decrease_neighbors':
                self.face_detector.adjust_min_neighbors(-1)
            elif action == 'next_model':
                self._switch_to_next_model()
            elif action == 'prev_model':
                self._switch_to_previous_model()
            elif action == 'help':
                self._print_help()
    
    def _handle_auto_save(self, frame: np.ndarray, detections: List[Dict]):
        """ìë™ ëª¨ë“œì—ì„œ ì–¼êµ´ ìë™ ì €ì¥"""
        for detection in detections:
            if self.quality_analyzer.should_save_face(detection, detections):
                self._auto_save_detected_face(frame, detection)
                self.quality_analyzer.add_saved_face(detection)
    
    def _save_full_frame(self, frame: np.ndarray):
        """ì „ì²´ í”„ë ˆì„ ì €ì¥ (sí‚¤ ì „ìš©)"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]
        
        # raw_input/capturedì— ì „ì²´ í”„ë ˆì„ ì €ì¥
        frame_filename = f"captured_frame_{timestamp}.jpg"
        frame_path = self.paths['raw_captured'] / frame_filename
        cv2.imwrite(str(frame_path), frame)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'timestamp': timestamp,
            'capture_type': 'manual_frame',
            'frame_path': str(frame_path),
            'frame_size': list(frame.shape)
        }
        
        metadata_path = frame_path.with_suffix('.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ì „ì²´ í”„ë ˆì„ ì €ì¥: {frame_filename}")
        self.logger.info(f"Full frame saved: {frame_filename}")
    
    def _handle_manual_face_capture(self, frame: np.ndarray, detections: List[Dict]):
        """ìˆ˜ë™ ì–¼êµ´ ìº¡ì²˜ (cí‚¤)"""
        if not detections:
            print("âŒ ê²€ì¶œëœ ì–¼êµ´ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]
        
        # 1ë‹¨ê³„: detected_faces/from_manualì— ì–¼êµ´ ì €ì¥
        saved_faces = []
        for i, detection in enumerate(detections):
            bbox = detection['bbox']
            x, y, w, h = bbox
            face_crop = frame[y:y+h, x:x+w]
            
            face_filename = f"manual_face_{timestamp}_{i:02d}_conf{detection['confidence']:.2f}.jpg"
            face_path = self.paths['detected_manual'] / face_filename
            cv2.imwrite(str(face_path), face_crop)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                'timestamp': timestamp,
                'capture_type': 'manual',
                'bbox': [int(x), int(y), int(w), int(h)],
                'confidence': float(detection['confidence']),
                'quality_score': float(detection['quality_score']),
                'face_path': str(face_path)
            }
            
            metadata_path = face_path.with_suffix('.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            saved_faces.append({
                'face_path': face_path,
                'face_crop': face_crop,
                'metadata': metadata
            })
        
        print(f"âœ… {len(saved_faces)}ê°œ ì–¼êµ´ì„ from_manualì— ì €ì¥")
        
        # 2ë‹¨ê³„: GUIë¡œ ì´ë¦„ ì…ë ¥ í›„ staging/namedì— ì €ì¥
        self._prompt_name_and_save_to_staging(saved_faces)
    
    def _prompt_name_and_save_to_staging(self, saved_faces: List[Dict]):
        """GUIë¡œ ì´ë¦„ ì…ë ¥ í›„ staging/namedì— ì €ì¥"""
        try:
            # tkinter ì°½ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            def get_name():
                root = tk.Tk()
                root.withdraw()  # ë©”ì¸ ì°½ ìˆ¨ê¸°ê¸°
                root.attributes('-topmost', True)  # í•­ìƒ ìœ„ì— í‘œì‹œ
                
                name = simpledialog.askstring(
                    "ì´ë¦„ ì…ë ¥", 
                    f"{len(saved_faces)}ê°œ ì–¼êµ´ì˜ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”:",
                    parent=root
                )
                root.destroy()
                return name
            
            # ì´ë¦„ ì…ë ¥ ë°›ê¸°
            person_name = get_name()
            
            if not person_name or not person_name.strip():
                print("âŒ ì´ë¦„ì´ ì…ë ¥ë˜ì§€ ì•Šì•„ staging ì €ì¥ì„ ì·¨ì†Œí•©ë‹ˆë‹¤")
                return
            
            person_name = person_name.strip()
            
            # staging/named/{person_name} í´ë” ìƒì„±
            person_dir = self.paths['staging_named'] / person_name
            person_dir.mkdir(exist_ok=True)
            
            # ê° ì–¼êµ´ì„ person í´ë”ì— ì €ì¥
            for i, face_data in enumerate(saved_faces):
                face_crop = face_data['face_crop']
                original_metadata = face_data['metadata']
                
                # ìƒˆ íŒŒì¼ëª… ìƒì„±
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]
                staged_filename = f"{person_name}_{timestamp}_{i:02d}.jpg"
                staged_path = person_dir / staged_filename
                
                # ì–¼êµ´ ì´ë¯¸ì§€ ì €ì¥
                cv2.imwrite(str(staged_path), face_crop)
                
                # staging ë©”íƒ€ë°ì´í„° ìƒì„±
                staged_metadata = {
                    'person_name': person_name,
                    'staged_at': timestamp,
                    'original_face_path': str(face_data['face_path']),
                    'staged_face_path': str(staged_path),
                    'original_metadata': original_metadata
                }
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                staged_metadata_path = staged_path.with_suffix('.json')
                with open(staged_metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(staged_metadata, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… {len(saved_faces)}ê°œ ì–¼êµ´ì„ staging/named/{person_name}ì— ì €ì¥")
            self.session_stats['named_saved'] += len(saved_faces)
            self.logger.info(f"Manual capture with name: {person_name} ({len(saved_faces)} faces)")
            
        except Exception as e:
            print(f"âŒ ì´ë¦„ ì…ë ¥ ì¤‘ ì˜¤ë¥˜: {e}")
            self.logger.error(f"Name input error: {e}")
    
    def _delete_unwanted_faces(self):
        """ë¶ˆí•„ìš”í•œ ì–¼êµ´ ì‚­ì œ ê¸°ëŠ¥ (ë³„ë„ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰)"""
        def delete_in_thread():
            try:
                # ì‚­ì œ ê°€ëŠ¥í•œ í´ë”ë“¤
                deletable_folders = {
                    'detected_manual': self.paths['detected_manual'],
                    'detected_auto': self.paths['detected_auto'],
                    'staging_named': self.paths['staging_named']
                }
                
                print("\nğŸ—‘ï¸  ì–¼êµ´ ì‚­ì œ ëª¨ë“œ (ë³„ë„ ì°½ì—ì„œ ì‹¤í–‰)")
                print("ì‚­ì œ ê°€ëŠ¥í•œ í´ë”:")
                for i, (name, path) in enumerate(deletable_folders.items(), 1):
                    file_count = len(list(path.glob('*.jpg')))
                    print(f"  {i}. {name} ({file_count}ê°œ íŒŒì¼)")
                
                print("  0. ì·¨ì†Œ")
                
                choice = input("í´ë” ì„ íƒ (ë²ˆí˜¸): ").strip()
                
                if choice == '0':
                    print("âŒ ì‚­ì œ ì·¨ì†Œ")
                    return
                
                try:
                    folder_idx = int(choice) - 1
                    folder_names = list(deletable_folders.keys())
                    
                    if 0 <= folder_idx < len(folder_names):
                        selected_folder = folder_names[folder_idx]
                        selected_path = deletable_folders[selected_folder]
                        
                        self._delete_faces_in_folder(selected_folder, selected_path)
                    else:
                        print("âŒ ì˜ëª»ëœ ì„ íƒ")
                
                except ValueError:
                    print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”")
            
            except Exception as e:
                print(f"âŒ ì‚­ì œ ëª¨ë“œ ì˜¤ë¥˜: {e}")
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‚­ì œ ê¸°ëŠ¥ ì‹¤í–‰
        thread = threading.Thread(target=delete_in_thread, daemon=True)
        thread.start()
        print("ğŸ—‘ï¸  ì‚­ì œ ëª¨ë“œê°€ ë³„ë„ ì°½ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ì¹´ë©”ë¼ í™”ë©´ ìœ ì§€)")
    
    def _delete_faces_in_folder(self, folder_name: str, folder_path: Path):
        """íŠ¹ì • í´ë”ì˜ ì–¼êµ´ë“¤ ì‚­ì œ"""
        try:
            # ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸°
            image_files = list(folder_path.glob('*.jpg'))
            
            if not image_files:
                print(f"âŒ {folder_name}ì— ì‚­ì œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return
            
            print(f"\nğŸ“ {folder_name} í´ë”ì˜ íŒŒì¼ë“¤:")
            for i, img_file in enumerate(image_files[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                print(f"  {i}. {img_file.name}")
            
            if len(image_files) > 10:
                print(f"  ... ì´ {len(image_files)}ê°œ íŒŒì¼")
            
            print("\nì‚­ì œ ì˜µì…˜:")
            print("  1. ì „ì²´ ì‚­ì œ")
            print("  2. ê°œë³„ ì„ íƒ ì‚­ì œ")
            print("  0. ì·¨ì†Œ")
            
            option = input("ì˜µì…˜ ì„ íƒ: ").strip()
            
            if option == '1':
                # ì „ì²´ ì‚­ì œ
                confirm = input(f"ì •ë§ë¡œ {len(image_files)}ê°œ íŒŒì¼ì„ ëª¨ë‘ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if confirm == 'y':
                    deleted_count = 0
                    for img_file in image_files:
                        try:
                            img_file.unlink()  # ì´ë¯¸ì§€ ì‚­ì œ
                            # ë©”íƒ€ë°ì´í„°ë„ ì‚­ì œ
                            metadata_file = img_file.with_suffix('.json')
                            if metadata_file.exists():
                                metadata_file.unlink()
                            deleted_count += 1
                        except Exception as e:
                            print(f"âŒ {img_file.name} ì‚­ì œ ì‹¤íŒ¨: {e}")
                    
                    print(f"âœ… {deleted_count}ê°œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
                else:
                    print("âŒ ì‚­ì œ ì·¨ì†Œ")
            
            elif option == '2':
                # ê°œë³„ ì„ íƒ ì‚­ì œ
                print("ì‚­ì œí•  íŒŒì¼ ë²ˆí˜¸ë“¤ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 1,3,5 ë˜ëŠ” 1-5)")
                indices_input = input("ë²ˆí˜¸: ").strip()
                
                try:
                    indices = self._parse_indices(indices_input, len(image_files))
                    
                    if indices:
                        deleted_count = 0
                        for idx in sorted(indices, reverse=True):  # ì—­ìˆœìœ¼ë¡œ ì‚­ì œ
                            try:
                                img_file = image_files[idx]
                                img_file.unlink()  # ì´ë¯¸ì§€ ì‚­ì œ
                                # ë©”íƒ€ë°ì´í„°ë„ ì‚­ì œ
                                metadata_file = img_file.with_suffix('.json')
                                if metadata_file.exists():
                                    metadata_file.unlink()
                                deleted_count += 1
                                print(f"âœ… {img_file.name} ì‚­ì œ")
                            except Exception as e:
                                print(f"âŒ {image_files[idx].name} ì‚­ì œ ì‹¤íŒ¨: {e}")
                        
                        print(f"âœ… ì´ {deleted_count}ê°œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
                    else:
                        print("âŒ ìœ íš¨í•œ ë²ˆí˜¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                
                except Exception as e:
                    print(f"âŒ ë²ˆí˜¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
            
            else:
                print("âŒ ì‚­ì œ ì·¨ì†Œ")
        
        except Exception as e:
            print(f"âŒ í´ë” ì‚­ì œ ì˜¤ë¥˜: {e}")
    
    def _parse_indices(self, indices_input: str, max_count: int) -> List[int]:
        """ì¸ë±ìŠ¤ ë¬¸ìì—´ íŒŒì‹± (ì˜ˆ: "1,3,5" ë˜ëŠ” "1-5")"""
        indices = []
        
        for part in indices_input.split(','):
            part = part.strip()
            
            if '-' in part:
                # ë²”ìœ„ (ì˜ˆ: "1-5")
                try:
                    start, end = map(int, part.split('-'))
                    indices.extend(range(start-1, min(end, max_count)))
                except ValueError:
                    continue
            else:
                # ë‹¨ì¼ ë²ˆí˜¸
                try:
                    idx = int(part) - 1
                    if 0 <= idx < max_count:
                        indices.append(idx)
                except ValueError:
                    continue
        
        return list(set(indices))  # ì¤‘ë³µ ì œê±°
    
    def _adjust_confidence_threshold(self, delta: float):
        """ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì ˆ"""
        self.face_detector.adjust_confidence_threshold(delta)
        print(f"ğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’: {self.face_detector.confidence_threshold:.2f}")
    
    def _adjust_scale_factor(self, delta: float):
        """ìŠ¤ì¼€ì¼ íŒ©í„° ì¡°ì ˆ"""
        self.face_detector.adjust_scale_factor(delta)
        print(f"ğŸ“ ìŠ¤ì¼€ì¼ íŒ©í„°: {self.face_detector.scale_factor:.2f}")
    
    def _adjust_min_neighbors(self, delta: int):
        """ìµœì†Œ ì´ì›ƒ ìˆ˜ ì¡°ì ˆ"""
        self.face_detector.adjust_min_neighbors(delta)
        print(f"ğŸ‘¥ ìµœì†Œ ì´ì›ƒ ìˆ˜: {self.face_detector.min_neighbors}")
    
    def _switch_to_next_model(self):
        """ë‹¤ìŒ ëª¨ë¸ë¡œ ë³€ê²½"""
        self._switch_model(1)
    
    def _switch_to_previous_model(self):
        """ì´ì „ ëª¨ë¸ë¡œ ë³€ê²½"""
        self._switch_model(-1)
    
    def _switch_model(self, direction: int):
        """ëª¨ë¸ ì „í™˜"""
        if len(self.available_models) <= 1:
            print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ 1ê°œë¿ì…ë‹ˆë‹¤.")
            return
        
        self.current_model_index = (self.current_model_index + direction) % len(self.available_models)
        self.current_model = self.available_models[self.current_model_index]
        
        # ìƒˆ ëª¨ë¸ë¡œ ê²€ì¶œê¸° ì¬ì´ˆê¸°í™”
        self.face_detector = FaceDetector(self.current_model)
        
        print(f"ğŸ”„ ëª¨ë¸ ë³€ê²½: {self.current_model['name']}")
        self.logger.info(f"ëª¨ë¸ ë³€ê²½: {self.current_model['name']}")
    
    def _update_fps(self):
        """FPS ì—…ë°ì´íŠ¸"""
        self.fps_counter += 1
        current_time = time.time()
        
        if current_time - self.fps_timer >= 1.0:
            self.current_fps = self.fps_counter / (current_time - self.fps_timer)
            self.fps_counter = 0
            self.fps_timer = current_time
    
    def _auto_save_detected_face(self, frame: np.ndarray, detection: Dict):
        """ìë™ ëª¨ë“œì—ì„œ ê²€ì¶œëœ ì–¼êµ´ ì €ì¥ (ì–¼êµ´ë§Œ ì €ì¥, ì „ì²´ í”„ë ˆì„ ì €ì¥ ì•ˆí•¨)"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]
        
        # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
        bbox = detection['bbox']
        x, y, w, h = bbox
        face_crop = frame[y:y+h, x:x+w]
        
        # detected_faces/auto_collectedì—ë§Œ ì €ì¥ (ì „ì²´ í”„ë ˆì„ ì €ì¥ ì•ˆí•¨)
        face_filename = f"auto_face_{timestamp}_conf{detection['confidence']:.2f}_qual{detection['quality_score']:.2f}.jpg"
        face_path = self.paths['detected_auto'] / face_filename
        cv2.imwrite(str(face_path), face_crop)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'timestamp': timestamp,
            'capture_type': 'auto',
            'bbox': [int(x), int(y), int(w), int(h)],
            'confidence': float(detection['confidence']),
            'quality_score': float(detection['quality_score']),
            'face_path': str(face_path)
        }
        
        metadata_path = face_path.with_suffix('.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        self.session_stats['auto_saved'] += 1
        self.logger.info(f"Auto saved face: {face_filename}")
    
    def _visualize_detections(self, frame: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """ê²€ì¶œ ê²°ê³¼ ì‹œê°í™”"""
        display_frame = frame.copy()
        
        for detection in detections:
            bbox = detection['bbox']
            confidence = detection['confidence']
            quality_score = detection.get('quality_score', 0.0)
            landmarks = detection.get('landmarks', [])
            
            x, y, w, h = bbox
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            color = (0, 255, 0) if quality_score > 0.6 else (0, 255, 255)
            cv2.rectangle(display_frame, (x, y), (x + w, y + h), color, 2)
            
            # ì‹ ë¢°ë„ì™€ í’ˆì§ˆ ì ìˆ˜ í‘œì‹œ
            label = f"Conf: {confidence:.2f}, Qual: {quality_score:.2f}"
            cv2.putText(display_frame, label, (x, y - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
            # ëœë“œë§ˆí¬ í‘œì‹œ (Lí‚¤ í† ê¸€ ìƒíƒœì— ë”°ë¼)
            if hasattr(self, 'show_landmarks') and self.show_landmarks and landmarks:
                landmark_colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]
                for i, (lx, ly) in enumerate(landmarks):
                    color = landmark_colors[i % len(landmark_colors)]
                    cv2.circle(display_frame, (lx, ly), 3, color, -1)
                    cv2.putText(display_frame, str(i+1), (lx+5, ly-5), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)
        
        return display_frame
    
    def _draw_info_overlay(self, frame: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """ì •ë³´ ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸°"""
        overlay_frame = frame.copy()
        height, width = overlay_frame.shape[:2]
        
        # ìƒë‹¨ ì •ë³´ ì˜¤ë²„ë ˆì´ (í¬ê¸° ì¶•ì†Œ)
        overlay = np.zeros_like(frame)
        cv2.rectangle(overlay, (10, 10), (500, 80), (0, 0, 0), -1)
        overlay_frame = cv2.addWeighted(overlay_frame, 0.7, overlay, 0.3, 0)
        
        # í…ìŠ¤íŠ¸ ì •ë³´ (ê°„ì†Œí™”)
        texts = [
            f"Model: {self.current_model['name'] if self.current_model else 'None'} | FPS: {self.current_fps:.1f}",
            f"Faces: {len(detections)} | Auto: {'ON' if self.auto_save_enabled else 'OFF'} | Landmarks: {'ON' if getattr(self, 'show_landmarks', False) else 'OFF'}",
            f"Conf: {self.face_detector.confidence_threshold:.2f} | Auto: {self.session_stats['auto_saved']} | Manual: {self.session_stats['manual_captured']}"
        ]
        
        for i, text in enumerate(texts):
            y_pos = 25 + i * 18
            cv2.putText(overlay_frame, text, (20, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # í•˜ë‹¨ í† ê¸€í‚¤ ì •ë³´
        self._draw_toggle_keys(overlay_frame, width, height)
        
        return overlay_frame
    
    def _draw_toggle_keys(self, frame: np.ndarray, width: int, height: int):
        """í† ê¸€í‚¤ ì •ë³´ë¥¼ í™”ë©´ í•˜ë‹¨ì— í‘œì‹œ"""
        # ë°°ê²½ ë°•ìŠ¤ (ë°˜íˆ¬ëª…)
        box_height = 140
        box_y = height - box_height - 10
        overlay = frame.copy()
        cv2.rectangle(overlay, (0, box_y), (width, height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # í† ê¸€í‚¤ ì •ë³´
        y_start = box_y + 20
        line_height = 25
        
        # ì²« ë²ˆì§¸ ì¤„: ê¸°ë³¸ ëª…ë ¹ì–´
        cv2.putText(frame, "Toggle Keys:", (10, y_start), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
        
        # ë‘ ë²ˆì§¸ ì¤„: ëª¨ë“œ ë° ì •ë³´
        cv2.putText(frame, "A: Auto/Manual  I: Info  L: Landmarks  H: Help  Q: Quit", (10, y_start + line_height), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # ì„¸ ë²ˆì§¸ ì¤„: ìº¡ì²˜ ëª…ë ¹ì–´
        cv2.putText(frame, "S: Save Frame  C: Capture Face  D: Delete Faces", (10, y_start + line_height * 2), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # ë„¤ ë²ˆì§¸ ì¤„: ëª¨ë¸ ì„¤ì •
        cv2.putText(frame, "Model Settings: +/- Confidence  [/] Scale  ,/. Neighbors  N/M: Model", (10, y_start + line_height * 3), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # ë‹¤ì„¯ ë²ˆì§¸ ì¤„: í˜„ì¬ ì„¤ì •ê°’
        current_settings = f"Current: Conf={self.face_detector.confidence_threshold:.2f} Scale={self.face_detector.scale_factor:.2f} Neighbors={self.face_detector.min_neighbors}"
        cv2.putText(frame, current_settings, (10, y_start + line_height * 4), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
    
    def _handle_keyboard_input(self, key: int, frame: np.ndarray, detections: List[Dict]) -> str:
        """í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬"""
        if key == ord('q'):  # ì¢…ë£Œ
            return 'quit'
        
        elif key == ord('i'):  # ì •ë³´ í‘œì‹œ í† ê¸€
            self.show_info = not self.show_info
            status = "ON" if self.show_info else "OFF"
            print(f"â„¹ï¸  ì •ë³´ í‘œì‹œ: {status}")
            return 'continue'
        
        elif key == ord('l'):  # ëœë“œë§ˆí¬ í‘œì‹œ í† ê¸€
            self.show_landmarks = not getattr(self, 'show_landmarks', False)
            status = "ON" if self.show_landmarks else "OFF"
            print(f"ğŸ¯ ëœë“œë§ˆí¬ í‘œì‹œ: {status}")
            return 'continue'
        
        elif key == ord('a'):  # ìë™/ìˆ˜ë™ ëª¨ë“œ ì „í™˜
            self.is_auto_mode = not self.is_auto_mode
            mode = "ğŸ¤– ìë™" if self.is_auto_mode else "ğŸ‘¤ ìˆ˜ë™"
            print(f"ğŸ”„ ëª¨ë“œ ë³€ê²½: {mode}")
            return 'continue'
        
        elif key == ord('s'):  # ì „ì²´ í”„ë ˆì„ ì €ì¥
            self._save_full_frame(frame)
            return 'continue'
        
        elif key == ord('c'):  # ì–¼êµ´ ìº¡ì²˜ + ì´ë¦„ ì…ë ¥
            self._handle_manual_face_capture(frame, detections)
            return 'continue'
        
        elif key == ord('d'):  # ì–¼êµ´ ì‚­ì œ ëª¨ë“œ
            self._delete_unwanted_faces()
            return 'continue'
        
        elif key == ord('h'):  # ë„ì›€ë§
            self._print_help()
            return 'continue'
        
        # ëª¨ë¸ ì„¤ì • ì¡°ì ˆ
        elif key == ord('+') or key == ord('='):  # ì‹ ë¢°ë„ ì¦ê°€
            self._adjust_confidence_threshold(0.05)
            return 'continue'
        
        elif key == ord('-'):  # ì‹ ë¢°ë„ ê°ì†Œ
            self._adjust_confidence_threshold(-0.05)
            return 'continue'
        
        elif key == ord('['):  # ìŠ¤ì¼€ì¼ íŒ©í„° ê°ì†Œ
            self._adjust_scale_factor(-0.05)
            return 'continue'
        
        elif key == ord(']'):  # ìŠ¤ì¼€ì¼ íŒ©í„° ì¦ê°€
            self._adjust_scale_factor(0.05)
            return 'continue'
        
        elif key == ord(','):  # ìµœì†Œ ì´ì›ƒ ìˆ˜ ê°ì†Œ
            self._adjust_min_neighbors(-1)
            return 'continue'
        
        elif key == ord('.'):  # ìµœì†Œ ì´ì›ƒ ìˆ˜ ì¦ê°€
            self._adjust_min_neighbors(1)
            return 'continue'
        
        # ëª¨ë¸ ë³€ê²½
        elif key == ord('n'):  # ë‹¤ìŒ ëª¨ë¸
            self._switch_to_next_model()
            return 'continue'
        
        elif key == ord('m'):  # ì´ì „ ëª¨ë¸
            self._switch_to_previous_model()
            return 'continue'
        
        return 'continue'
    
    def _print_help(self):
        """ë„ì›€ë§ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“‹ ê°œì„ ëœ ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ - í‚¤ë³´ë“œ ëª…ë ¹ì–´")
        print("="*60)
        print("ğŸ”§ ê³µí†µ ëª…ë ¹ì–´:")
        print("   i  - ì •ë³´ í‘œì‹œ í† ê¸€")
        print("   l  - ëœë“œë§ˆí¬ í‘œì‹œ í† ê¸€")
        print("   a  - ìë™/ìˆ˜ë™ ëª¨ë“œ ì „í™˜")
        print("   h  - ë„ì›€ë§ í‘œì‹œ")
        print("   q  - ì¢…ë£Œ")
        print()
        print("ğŸ‘¤ ìº¡ì²˜ ëª…ë ¹ì–´:")
        print("   s  - ì „ì²´ í”„ë ˆì„ ì €ì¥ (raw_input/captured/)")
        print("   c  - ì–¼êµ´ ìº¡ì²˜ + GUI ì´ë¦„ ì…ë ¥ (staging/named/)")
        print("   d  - ë¶ˆí•„ìš”í•œ ì–¼êµ´ ì‚­ì œ")
        print()
        print("âš™ï¸  ëª¨ë¸ ì„¤ì • ì¡°ì ˆ:")
        print("   +/- - ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì ˆ")
        print("   [/] - ìŠ¤ì¼€ì¼ íŒ©í„° ì¡°ì ˆ (Haar Cascade)")
        print("   ,/. - ìµœì†Œ ì´ì›ƒ ìˆ˜ ì¡°ì ˆ (Haar Cascade)")
        print()
        print("ğŸ”„ ëª¨ë¸ ë³€ê²½:")
        print("   n  - ë‹¤ìŒ ëª¨ë¸")
        print("   m  - ì´ì „ ëª¨ë¸")
        print()
        print("ğŸ“ ë°ì´í„° í”Œë¡œìš°:")
        print("   1. sí‚¤: raw_input/captured/ (ì „ì²´ í”„ë ˆì„)")
        print("   2. cí‚¤: detected_faces/from_manual/ â†’ staging/named/{ì´ë¦„}/")
        print("   3. ìë™ëª¨ë“œ: detected_faces/auto_collected/ (ì–¼êµ´ë§Œ)")
        print("   4. dí‚¤: ì €ì¥ëœ ì–¼êµ´ ì‚­ì œ")
        print("="*60)
    
    def _print_session_stats(self):
        """ì„¸ì…˜ í†µê³„ ì¶œë ¥"""
        print("\nğŸ“Š ì„¸ì…˜ í†µê³„:")
        print(f"   ìë™ ì €ì¥: {self.session_stats['auto_saved']}ê°œ")
        print(f"   ìˆ˜ë™ ìº¡ì²˜: {self.session_stats['manual_captured']}ê°œ")
        print(f"   ì´ë¦„ ì§€ì •: {self.session_stats['named_saved']}ê°œ")
        print(f"   ì‚¬ìš©ëœ ëª¨ë¸: {self.current_model['name'] if self.current_model else 'None'}")
    
    def _cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()
        
        # ì„¸ì…˜ í†µê³„ ì¶œë ¥
        self._print_session_stats()
        
        self.logger.info("ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ ì¢…ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ê°œì„ ëœ ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ")
    parser.add_argument("--camera", type=int, default=0, help="ì¹´ë©”ë¼ ID (ê¸°ë³¸ê°’: 0)")
    args = parser.parse_args()
    
    try:
        # í•˜ë“œì›¨ì–´ ì—°ê²° í™•ì¸
        print("ğŸ” í•˜ë“œì›¨ì–´ ì—°ê²° ìƒíƒœ í™•ì¸ ì¤‘...")
        cap_test = cv2.VideoCapture(args.camera)
        if cap_test.isOpened():
            ret, _ = cap_test.read()
            cap_test.release()
            if ret:
                print(f"âœ… ì¹´ë©”ë¼ {args.camera} ì—°ê²° í™•ì¸ë¨")
            else:
                print(f"âŒ ì¹´ë©”ë¼ {args.camera}ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŒ")
                return
        else:
            print(f"âŒ ì¹´ë©”ë¼ {args.camera} ì—°ê²° ì‹¤íŒ¨")
            return
        
        # ìº¡ì²˜ ì‹œìŠ¤í…œ ì‹œì‘
        capture_system = EnhancedFaceCaptureSystem(camera_id=args.camera)
        capture_system.start_capture()
        
    except KeyboardInterrupt:
        print("\nâš ï¸  í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main() 