#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
í†µí•© ë°°ì¹˜ ì–¼êµ´ ì²˜ë¦¬ê¸°

capturedì™€ uploads í´ë”ì˜ ëª¨ë“  ì´ë¯¸ì§€/ë™ì˜ìƒì—ì„œ ì–¼êµ´ì„ ì¶”ì¶œí•˜ì—¬
detected_faces í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python run_unified_batch_processor.py [--source captured|uploads|both] [--auto-save]
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Optional
import cv2
import numpy as np
from datetime import datetime
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent.parent
sys.path.append(str(project_root))

from common.logging import setup_logging, get_logger
from common.config import load_config

class EnvironmentAnalyzer:
    """í™˜ê²½ ë¶„ì„ì„ í†µí•œ ìµœì  ëª¨ë¸ ì„ íƒ"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
    def analyze_environment(self) -> Dict:
        """í™˜ê²½ ë¶„ì„ ë° ìµœì  ëª¨ë¸ ì¶”ì²œ"""
        env_info = {
            'gpu_available': self._check_gpu(),
            'cpu_cores': os.cpu_count(),
            'available_models': self._scan_available_models(),
            'recommended_model': None
        }
        
        # ìµœì  ëª¨ë¸ ì„ íƒ
        env_info['recommended_model'] = self._select_optimal_model(env_info)
        
        return env_info
    
    def _check_gpu(self) -> bool:
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            try:
                # OpenCV DNN GPU ì§€ì› í™•ì¸
                return cv2.cuda.getCudaEnabledDeviceCount() > 0
            except:
                return False
    
    def _scan_available_models(self) -> List[Dict]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìŠ¤ìº”"""
        models_dir = project_root / 'models' / 'weights'
        available_models = []
        
        # OpenCV Haar Cascade
        haar_path = models_dir / 'face_detection_opencv_haarcascade_20250628.xml'
        if haar_path.exists():
            available_models.append({
                'name': 'OpenCV Haar Cascade',
                'type': 'haar',
                'path': str(haar_path),
                'speed': 'fast',
                'accuracy': 'medium',
                'gpu_required': False
            })
        
        # ê¸°ë³¸ Haar Cascade (í•­ìƒ ì‚¬ìš© ê°€ëŠ¥)
        available_models.append({
            'name': 'OpenCV Default Haar',
            'type': 'haar_default',
            'path': cv2.data.haarcascades + 'haarcascade_frontalface_default.xml',
            'speed': 'fast',
            'accuracy': 'medium',
            'gpu_required': False
        })
        
        # RetinaFace ëª¨ë¸ë“¤ ìŠ¤ìº”
        for model_file in models_dir.glob('*retinaface*.onnx'):
            available_models.append({
                'name': f'RetinaFace ({model_file.stem})',
                'type': 'retinaface',
                'path': str(model_file),
                'speed': 'medium',
                'accuracy': 'high',
                'gpu_required': False
            })
        
        # YOLO ëª¨ë¸ë“¤ ìŠ¤ìº”
        for model_file in models_dir.glob('*yolo*.pt'):
            available_models.append({
                'name': f'YOLO ({model_file.stem})',
                'type': 'yolo',
                'path': str(model_file),
                'speed': 'medium',
                'accuracy': 'high',
                'gpu_required': True
            })
        
        return available_models
    
    def _select_optimal_model(self, env_info: Dict) -> Dict:
        """ìµœì  ëª¨ë¸ ì„ íƒ"""
        available_models = env_info['available_models']
        gpu_available = env_info['gpu_available']
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ê³ ì„±ëŠ¥ ëª¨ë¸ ìš°ì„ 
        if gpu_available:
            for model in available_models:
                if model['accuracy'] == 'high':
                    return model
        
        # CPUë§Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ë¹ ë¥¸ ëª¨ë¸ ìš°ì„ 
        for model in available_models:
            if not model['gpu_required'] and model['speed'] == 'fast':
                return model
        
        # ê¸°ë³¸ê°’
        return available_models[0] if available_models else None

class FaceDetector:
    """í†µí•© ì–¼êµ´ ê²€ì¶œê¸°"""
    
    def __init__(self, model_config: Dict):
        self.model_config = model_config
        self.model_type = model_config['type']
        self.logger = get_logger(__name__)
        
        # ëª¨ë¸ë³„ ì„¤ì • (ë” ë¯¼ê°í•œ íŒŒë¼ë¯¸í„°ë¡œ ì¡°ì •)
        self.confidence_threshold = 0.3  # ë” ë‚®ì€ ì‹ ë¢°ë„ ì„ê³„ê°’
        self.scale_factor = 1.05  # ë” ì„¸ë°€í•œ ê²€ì¶œ
        self.min_neighbors = 2   # ë” ë¯¼ê°í•œ ê²€ì¶œ (5ì—ì„œ 2ë¡œ ë³€ê²½)
        self.min_size = (20, 20)  # ë” ì‘ì€ ì–¼êµ´ë„ ê²€ì¶œ
        
        self._initialize_model()
    
    def _initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        if self.model_type in ['haar', 'haar_default']:
            self.detector = cv2.CascadeClassifier(self.model_config['path'])
            if self.detector.empty():
                self.logger.error(f"Haar Cascade ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {self.model_config['path']}")
                self._fallback_to_default_haar()
            else:
                self.logger.info(f"Haar Cascade ëª¨ë¸ ë¡œë“œ: {self.model_config['name']}")
        
        elif self.model_type == 'retinaface':
            try:
                # RetinaFace ONNX ëª¨ë¸ ë¡œë“œ
                import onnxruntime as ort
                
                # ONNX Runtime ì„¸ì…˜ ìƒì„±
                self.ort_session = ort.InferenceSession(
                    self.model_config['path'],
                    providers=['CPUExecutionProvider']  # CPU ì‚¬ìš©
                )
                
                # ì…ë ¥/ì¶œë ¥ ì •ë³´ í™•ì¸
                self.input_name = self.ort_session.get_inputs()[0].name
                self.input_shape = self.ort_session.get_inputs()[0].shape
                
                self.logger.info(f"RetinaFace ONNX ëª¨ë¸ ë¡œë“œ: {self.model_config['name']}")
                self.logger.info(f"ì…ë ¥ í˜•íƒœ: {self.input_shape}")
                
            except ImportError:
                self.logger.error("onnxruntime íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install onnxruntime")
                self._fallback_to_default_haar()
            except Exception as e:
                self.logger.error(f"RetinaFace ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self._fallback_to_default_haar()
        
        elif self.model_type == 'yolo':
            try:
                # YOLO ëª¨ë¸ ì´ˆê¸°í™” (ì‹¤ì œ êµ¬í˜„ í•„ìš”)
                # í˜„ì¬ëŠ” êµ¬í˜„ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ Haarë¡œ í´ë°±
                self.logger.warning(f"YOLO ëª¨ë¸ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ: {self.model_config['name']}")
                self._fallback_to_default_haar()
            except Exception as e:
                self.logger.error(f"YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self._fallback_to_default_haar()
        else:
            self.logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
            self._fallback_to_default_haar()
    
    def _fallback_to_default_haar(self):
        """ê¸°ë³¸ Haar Cascadeë¡œ í´ë°±"""
        self.model_type = 'haar_default'
        self.model_config = {
            'name': 'OpenCV Default Haar (Fallback)',
            'type': 'haar_default',
            'path': cv2.data.haarcascades + 'haarcascade_frontalface_default.xml',
            'speed': 'fast',
            'accuracy': 'medium',
            'gpu_required': False
        }
        self.detector = cv2.CascadeClassifier(self.model_config['path'])
        self.logger.warning("ê¸°ë³¸ Haar Cascadeë¡œ í´ë°±")
    
    def detect_faces(self, image: np.ndarray) -> List[Dict]:
        """ì–¼êµ´ ê²€ì¶œ"""
        if self.detector is None:
            self.logger.error("ê²€ì¶œê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return []
        
        if self.model_type in ['haar', 'haar_default']:
            return self._detect_haar(image)
        elif self.model_type == 'retinaface':
            return self._detect_retinaface(image)
        elif self.model_type == 'yolo':
            # í˜„ì¬ëŠ” Haarë¡œ í´ë°±ë˜ë¯€ë¡œ ì´ ì½”ë“œëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
            return self._detect_yolo(image)
        else:
            self.logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
            return []
    
    def _detect_haar(self, image: np.ndarray) -> List[Dict]:
        """Haar Cascade ê²€ì¶œ"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”ë¡œ ì¡°ëª… ê°œì„ 
        gray = cv2.equalizeHist(gray)
        
        faces = self.detector.detectMultiScale(
            gray, 
            scaleFactor=self.scale_factor,
            minNeighbors=self.min_neighbors,
            minSize=self.min_size,
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        
        detections = []
        for (x, y, w, h) in faces:
            detections.append({
                'bbox': (x, y, w, h),
                'confidence': 1.0,  # HaarëŠ” ì‹ ë¢°ë„ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ
                'quality_score': self._calculate_quality_score(x, y, w, h, image.shape)
            })
        
        return detections
    
    def _detect_retinaface(self, image: np.ndarray) -> List[Dict]:
        """RetinaFace ONNX ëª¨ë¸ì„ ì´ìš©í•œ ê²€ì¶œ"""
        try:
            # ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            input_size = (640, 640)  # RetinaFace í‘œì¤€ ì…ë ¥ í¬ê¸°
            
            # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
            resized = cv2.resize(image, input_size)
            
            # BGR to RGB
            rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
            
            # ì •ê·œí™” (0-1)
            normalized = rgb_image.astype(np.float32) / 255.0
            
            # HWC to CHW
            input_tensor = np.transpose(normalized, (2, 0, 1))
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            input_tensor = np.expand_dims(input_tensor, axis=0)
            
            # ì¶”ë¡  ì‹¤í–‰
            outputs = self.ort_session.run(None, {self.input_name: input_tensor})
            
            # ì¶œë ¥ í›„ì²˜ë¦¬
            detections = self._postprocess_retinaface_outputs(outputs, image.shape, input_size)
            
            return detections
            
        except Exception as e:
            self.logger.error(f"RetinaFace ì¶”ë¡  ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ Haar Cascade ì‚¬ìš©
            return self._detect_haar(image)
    
    def _postprocess_retinaface_outputs(self, outputs, original_shape, input_size) -> List[Dict]:
        """RetinaFace ì¶œë ¥ í›„ì²˜ë¦¬"""
        try:
            detections = []
            
            # RetinaFace ì¶œë ¥ í˜•íƒœì— ë”°ë¼ ì¡°ì • í•„ìš”
            # ì¼ë°˜ì ìœ¼ë¡œ [boxes, scores, landmarks] í˜•íƒœ
            if len(outputs) >= 2:
                boxes = outputs[0]  # [N, 4]
                scores = outputs[1]  # [N, 1]
                
                # ì‹ ë¢°ë„ ì„ê³„ê°’ ì ìš©
                valid_indices = scores[:, 0] > self.confidence_threshold
                
                if np.any(valid_indices):
                    valid_boxes = boxes[valid_indices]
                    valid_scores = scores[valid_indices]
                    
                    # ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§ (ì…ë ¥ í¬ê¸° -> ì›ë³¸ í¬ê¸°)
                    scale_x = original_shape[1] / input_size[0]
                    scale_y = original_shape[0] / input_size[1]
                    
                    for box, score in zip(valid_boxes, valid_scores):
                        x1, y1, x2, y2 = box
                        
                        # ìŠ¤ì¼€ì¼ë§ ì ìš©
                        x1 = int(x1 * scale_x)
                        y1 = int(y1 * scale_y)
                        x2 = int(x2 * scale_x)
                        y2 = int(y2 * scale_y)
                        
                        # bbox í˜•íƒœë¥¼ (x, y, w, h)ë¡œ ë³€í™˜
                        w = x2 - x1
                        h = y2 - y1
                        
                        detections.append({
                            'bbox': (x1, y1, w, h),
                            'confidence': float(score[0]),
                            'quality_score': self._calculate_quality_score(x1, y1, w, h, original_shape)
                        })
            
            return detections
            
        except Exception as e:
            self.logger.error(f"RetinaFace í›„ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return []
    
    def _detect_yolo(self, image: np.ndarray) -> List[Dict]:
        """YOLO ê²€ì¶œ (êµ¬í˜„ í•„ìš”)"""
        # ì‹¤ì œ YOLO êµ¬í˜„
        return self._detect_haar(image)  # ì„ì‹œë¡œ Haar ì‚¬ìš©
    
    def _calculate_quality_score(self, x: int, y: int, w: int, h: int, image_shape: tuple) -> float:
        """ì–¼êµ´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        # í¬ê¸° ì ìˆ˜
        face_area = w * h
        image_area = image_shape[0] * image_shape[1]
        size_ratio = face_area / image_area if image_area > 0 else 0
        size_score = min(size_ratio * 10, 1.0)
        
        # ìœ„ì¹˜ ì ìˆ˜ (ì¤‘ì•™ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        center_x, center_y = x + w//2, y + h//2
        img_center_x, img_center_y = image_shape[1]//2, image_shape[0]//2
        distance = ((center_x - img_center_x)**2 + (center_y - img_center_y)**2)**0.5
        max_distance = (img_center_x**2 + img_center_y**2)**0.5
        position_score = 1.0 - (distance / max_distance) if max_distance > 0 else 1.0
        
        return (size_score * 0.7 + position_score * 0.3)

class UnifiedBatchProcessor:
    """í†µí•© ë°°ì¹˜ ì–¼êµ´ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = get_logger(__name__)
        
        # í™˜ê²½ ë¶„ì„ ë° ìµœì  ëª¨ë¸ ì„ íƒ
        self.env_analyzer = EnvironmentAnalyzer()
        self.env_info = self.env_analyzer.analyze_environment()
        
        # ìµœì  ëª¨ë¸ë¡œ ê²€ì¶œê¸° ì´ˆê¸°í™”
        optimal_model = self.env_info['recommended_model']
        if optimal_model:
            self.face_detector = FaceDetector(optimal_model)
            self.logger.info(f"ìµœì  ëª¨ë¸ ì„ íƒ: {optimal_model['name']}")
        else:
            self.logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ê²½ë¡œ ì„¤ì •
        self.domain_root = project_root / 'data' / 'domains' / 'face_recognition'
        
        self.paths = {
            # ì…ë ¥ ê²½ë¡œ
            'raw_captured': self.domain_root / 'raw_input' / 'captured',
            'raw_uploads': self.domain_root / 'raw_input' / 'uploads',
            
            # ì¶œë ¥ ê²½ë¡œ
            'detected_captured': self.domain_root / 'detected_faces' / 'from_captured',
            'detected_uploads': self.domain_root / 'detected_faces' / 'from_uploads'
        }
        
        # í´ë” ìƒì„±
        for path in self.paths.values():
            path.mkdir(parents=True, exist_ok=True)
        
        # í†µê³„
        self.stats = {
            'processed_files': 0,
            'detected_faces': 0,
            'saved_faces': 0,
            'errors': 0
        }
    
    def process_batch(self, source: str = 'both'):
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
        self.logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ - ì†ŒìŠ¤: {source}")
        self.logger.info(f"ì‚¬ìš© ëª¨ë¸: {self.face_detector.model_config['name']}")
        
        if source in ['captured', 'both']:
            self._process_folder('captured')
        
        if source in ['uploads', 'both']:
            self._process_folder('uploads')
        
        self._print_summary()
    
    def _process_folder(self, folder_type: str):
        """í´ë” ì²˜ë¦¬"""
        input_path = self.paths[f'raw_{folder_type}']
        output_path = self.paths[f'detected_{folder_type}']
        
        self.logger.info(f"ì²˜ë¦¬ ì¤‘: {input_path}")
        
        # ì§€ì›ë˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì (JSON ì œì™¸)
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv'}
        
        # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (JSON íŒŒì¼ ì œì™¸)
        files = []
        for file_path in input_path.iterdir():
            if file_path.is_file():
                ext = file_path.suffix.lower()
                # JSON íŒŒì¼ê³¼ README íŒŒì¼ ì œì™¸
                if ext in image_extensions or ext in video_extensions:
                    files.append(file_path)
        
        if not files:
            self.logger.warning(f"{input_path}ì— ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        self.logger.info(f"{len(files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€: ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ í™•ì¸
        processed_files = self._get_processed_files(output_path)
        new_files = []
        skipped_files = []
        
        for file_path in files:
            if self._is_already_processed(file_path, processed_files):
                skipped_files.append(file_path.name)
            else:
                new_files.append(file_path)
        
        if skipped_files:
            self.logger.info(f"ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ {len(skipped_files)}ê°œ ê±´ë„ˆëœ€: {', '.join(skipped_files[:5])}{'...' if len(skipped_files) > 5 else ''}")
        
        if not new_files:
            self.logger.info("ìƒˆë¡œ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        self.logger.info(f"ìƒˆë¡œ ì²˜ë¦¬í•  íŒŒì¼: {len(new_files)}ê°œ")
        
        for file_path in new_files:
            try:
                ext = file_path.suffix.lower()
                if ext in image_extensions:
                    self._process_image(file_path, output_path)
                elif ext in video_extensions:
                    self._process_video(file_path, output_path)
                    
                self.stats['processed_files'] += 1
                
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                self.stats['errors'] += 1
    
    def _get_processed_files(self, output_path: Path) -> Dict[str, Dict]:
        """ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ë“¤ì˜ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        processed_files = {}
        
        if not output_path.exists():
            return processed_files
        
        # JSON ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤ ìˆ˜ì§‘
        for json_file in output_path.glob('*.json'):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                source_file = metadata.get('source_file', '')
                if source_file:
                    # ì ˆëŒ€ ê²½ë¡œë¡œ ì •ê·œí™”
                    source_path = Path(source_file)
                    if source_path.exists():
                        # íŒŒì¼ í¬ê¸°ì™€ ìˆ˜ì • ì‹œê°„ìœ¼ë¡œ ë³€ê²½ ê°ì§€
                        stat = source_path.stat()
                        processed_files[str(source_path)] = {
                            'metadata': metadata,
                            'file_size': stat.st_size,
                            'mtime': stat.st_mtime
                        }
            except Exception as e:
                self.logger.debug(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ {json_file}: {e}")
        
        return processed_files
    
    def _is_already_processed(self, file_path: Path, processed_files: Dict[str, Dict]) -> bool:
        """íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        file_key = str(file_path)
        
        if file_key not in processed_files:
            return False
        
        # íŒŒì¼ í¬ê¸°ì™€ ìˆ˜ì • ì‹œê°„ í™•ì¸
        current_stat = file_path.stat()
        processed_info = processed_files[file_key]
        
        # íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸ (í¬ê¸°ë‚˜ ìˆ˜ì • ì‹œê°„ì´ ë‹¤ë¥´ë©´ ì¬ì²˜ë¦¬)
        if (current_stat.st_size != processed_info['file_size'] or 
            current_stat.st_mtime != processed_info['mtime']):
            return False
        
        return True
    
    def _process_image(self, image_path: Path, output_path: Path):
        """ì´ë¯¸ì§€ ì²˜ë¦¬"""
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(str(image_path))
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
        
        # ì–¼êµ´ ê²€ì¶œ
        detections = self.face_detector.detect_faces(image)
        
        if not detections:
            self.logger.debug(f"ì–¼êµ´ ì—†ìŒ: {image_path.name}")
            return
        
        self.stats['detected_faces'] += len(detections)
        
        # ì–¼êµ´ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]
        base_name = image_path.stem
        
        for i, detection in enumerate(detections):
            bbox = detection['bbox']
            x, y, w, h = bbox
            
            # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
            face_crop = image[y:y+h, x:x+w]
            
            # íŒŒì¼ëª… ìƒì„±
            face_filename = f"face_{base_name}_{timestamp}_{i:02d}_conf{detection['confidence']:.2f}.jpg"
            face_path = output_path / face_filename
            
            # ì–¼êµ´ ì €ì¥
            cv2.imwrite(str(face_path), face_crop)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                'timestamp': timestamp,
                'source_file': str(image_path),
                'source_type': 'image',
                'bbox': [int(x), int(y), int(w), int(h)],
                'confidence': float(detection['confidence']),
                'quality_score': float(detection['quality_score']),
                'face_path': str(face_path)
            }
            
            metadata_path = face_path.with_suffix('.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            self.stats['saved_faces'] += 1
        
        self.logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {image_path.name} ({len(detections)}ê°œ ì–¼êµ´)")
    
    def _process_video(self, video_path: Path, output_path: Path):
        """ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"ë¹„ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨: {video_path}")
        
        frame_count = 0
        detected_count = 0
        
        # ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ (1ì´ˆì— 1í”„ë ˆì„)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_interval = max(1, int(fps)) if fps > 0 else 30
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]
        base_name = video_path.stem
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # í”„ë ˆì„ ê°„ê²©ì— ë”°ë¼ ì²˜ë¦¬
            if frame_count % frame_interval == 0:
                detections = self.face_detector.detect_faces(frame)
                
                if detections:
                    self.stats['detected_faces'] += len(detections)
                    
                    for i, detection in enumerate(detections):
                        bbox = detection['bbox']
                        x, y, w, h = bbox
                        
                        # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
                        face_crop = frame[y:y+h, x:x+w]
                        
                        # íŒŒì¼ëª… ìƒì„±
                        face_filename = f"face_{base_name}_frame{frame_count:06d}_{timestamp}_{i:02d}_conf{detection['confidence']:.2f}.jpg"
                        face_path = output_path / face_filename
                        
                        # ì–¼êµ´ ì €ì¥
                        cv2.imwrite(str(face_path), face_crop)
                        
                        # ë©”íƒ€ë°ì´í„° ì €ì¥
                        metadata = {
                            'timestamp': timestamp,
                            'source_file': str(video_path),
                            'source_type': 'video',
                            'frame_number': frame_count,
                            'bbox': [int(x), int(y), int(w), int(h)],
                            'confidence': float(detection['confidence']),
                            'quality_score': float(detection['quality_score']),
                            'face_path': str(face_path)
                        }
                        
                        metadata_path = face_path.with_suffix('.json')
                        with open(metadata_path, 'w', encoding='utf-8') as f:
                            json.dump(metadata, f, indent=2, ensure_ascii=False)
                        
                        self.stats['saved_faces'] += 1
                        detected_count += 1
            
            frame_count += 1
        
        cap.release()
        self.logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {video_path.name} ({detected_count}ê°œ ì–¼êµ´, {frame_count}í”„ë ˆì„)")
    
    def _print_summary(self):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½"""
        print("\n" + "="*60)
        print("ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {self.face_detector.model_config['name']}")
        print(f"ğŸ“ ì²˜ë¦¬ëœ íŒŒì¼: {self.stats['processed_files']}ê°œ")
        print(f"ğŸ‘¤ ê²€ì¶œëœ ì–¼êµ´: {self.stats['detected_faces']}ê°œ")
        print(f"ğŸ’¾ ì €ì¥ëœ ì–¼êµ´: {self.stats['saved_faces']}ê°œ")
        print(f"âŒ ì˜¤ë¥˜: {self.stats['errors']}ê°œ")
        print("="*60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í†µí•© ë°°ì¹˜ ì–¼êµ´ ì²˜ë¦¬ê¸°")
    parser.add_argument(
        "--source", 
        choices=['captured', 'uploads', 'both'], 
        default='both',
        help="ì²˜ë¦¬í•  ì†ŒìŠ¤ í´ë” (ê¸°ë³¸ê°’: both)"
    )
    parser.add_argument(
        "--config", 
        type=str, 
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ"
    )
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    setup_logging()
    logger = get_logger(__name__)
    
    try:
        # ì„¤ì • ë¡œë“œ
        config = {}
        if args.config:
            config = load_config(args.config)
        
        # ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
        processor = UnifiedBatchProcessor(config)
        processor.process_batch(args.source)
        
        logger.info("ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        print(f"âŒ ì˜¤ë¥˜: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 