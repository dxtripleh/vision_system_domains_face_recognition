#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ìˆ˜ë™ ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ

ì¹´ë©”ë¼ë¡œ ì–¼êµ´ì„ ìº¡ì²˜í•˜ê³  ì´ë¦„ì„ ì§€ì •í•˜ì—¬ face_stagingì— ì €ì¥í•©ë‹ˆë‹¤.
í‚¤ë³´ë“œ ì¸í„°í˜ì´ìŠ¤:
- i: ì •ë³´ í‘œì‹œ í† ê¸€
- +/-: ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì ˆ
- s: í˜„ì¬ í”„ë ˆì„ ì €ì¥
- c: ì–¼êµ´ ìº¡ì²˜ + ì´ë¦„ ì§€ì •
- r: ë…¹í™” ì‹œì‘/ì¤‘ì§€
- p: ì¼ì‹œì •ì§€/ì¬ìƒ
- h: ë„ì›€ë§ í‘œì‹œ
- q: ì¢…ë£Œ
"""

import os
import sys
import cv2
import time
import uuid
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent.parent
sys.path.append(str(project_root))

from common.logging import setup_logging
from domains.face_recognition.core.services.environment_analyzer import EnvironmentAnalyzer
from domains.face_recognition.infrastructure.detection_engines.opencv_detection_engine import OpenCVDetectionEngine

class ManualFaceCaptureSystem:
    """ìˆ˜ë™ ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ"""
    
    def __init__(self, camera_id: int = 0):
        import logging
        setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # í™˜ê²½ ë¶„ì„ ë° ìµœì  ëª¨ë¸ ì„ íƒ
        self.env_analyzer = EnvironmentAnalyzer()
        self.analysis_result = self.env_analyzer.analyze_environment()
        
        # ìµœì  ê²€ì¶œ ì„¤ì •
        detection_config = self.analysis_result['optimal_models']['detection']['config']
        self.detector = OpenCVDetectionEngine(detection_config)
        
        # ì¹´ë©”ë¼ ì„¤ì •
        self.camera_id = camera_id
        self.cap = None
        
        # ê²½ë¡œ ì„¤ì •
        self.face_staging_dir = project_root / 'data' / 'temp' / 'face_staging'
        self.output_dir = project_root / 'data' / 'output'
        
        # í´ë” ìƒì„±
        for directory in [self.face_staging_dir, self.output_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # UI ìƒíƒœ
        self.show_info = True
        self.confidence_threshold = detection_config.get('confidence_threshold', 0.5)
        self.min_confidence = 0.1
        self.max_confidence = 0.9
        self.confidence_step = 0.05
        
        # ë…¹í™” ì„¤ì •
        self.is_recording = False
        self.video_writer = None
        self.recording_start_time = None
        
        # ì¼ì‹œì •ì§€ ì„¤ì •
        self.is_paused = False
        self.paused_frame = None
        
        # ìº¡ì²˜ëœ ì–¼êµ´ ì •ë³´
        self.captured_faces = []
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.fps_counter = 0
        self.fps_timer = time.time()
        self.current_fps = 0
        
        self.logger.info("ìˆ˜ë™ ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def start_capture(self):
        """ìº¡ì²˜ ì‹œì‘"""
        print("ğŸš€ ìˆ˜ë™ ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ ì‹œì‘")
        print("="*60)
        
        # í™˜ê²½ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        self._print_environment_info()
        
        # ì¹´ë©”ë¼ ì´ˆê¸°í™”
        if not self._initialize_camera():
            return False
        
        print("\nğŸ“¹ ì¹´ë©”ë¼ ì‹œì‘ë¨. í‚¤ë³´ë“œ ëª…ë ¹ì–´:")
        self._print_help()
        
        try:
            # ë©”ì¸ ìº¡ì²˜ ë£¨í”„
            self._capture_loop()
            
        except KeyboardInterrupt:
            print("\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ìº¡ì²˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            self._cleanup()
        
        return True
    
    def _print_environment_info(self):
        """í™˜ê²½ ì •ë³´ ì¶œë ¥"""
        tier_names = {
            'high_performance': 'ê³ ì„±ëŠ¥ ğŸš€',
            'balanced': 'ê· í˜•í˜• âš–ï¸',
            'lightweight': 'ê²½ëŸ‰í˜• ğŸª¶'
        }
        
        tier = self.analysis_result['performance_tier']
        detection_model = self.analysis_result['optimal_models']['detection']['model']
        
        print(f"ğŸ” í™˜ê²½ ë¶„ì„ ê²°ê³¼:")
        print(f"   ì„±ëŠ¥ ë“±ê¸‰: {tier_names.get(tier, tier)}")
        print(f"   ì„ íƒëœ ê²€ì¶œ ëª¨ë¸: {detection_model}")
        print(f"   ì‹ ë¢°ë„ ì„ê³„ê°’: {self.confidence_threshold:.2f}")
    
    def _initialize_camera(self) -> bool:
        """ì¹´ë©”ë¼ ì´ˆê¸°í™”"""
        try:
            self.cap = cv2.VideoCapture(self.camera_id)
            if not self.cap.isOpened():
                print(f"âŒ ì¹´ë©”ë¼ {self.camera_id} ì—°ê²° ì‹¤íŒ¨")
                return False
            
            # ì¹´ë©”ë¼ ì„¤ì •
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            
            # í…ŒìŠ¤íŠ¸ í”„ë ˆì„ ì½ê¸°
            ret, frame = self.cap.read()
            if not ret:
                print("âŒ ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            print(f"âœ… ì¹´ë©”ë¼ {self.camera_id} ì—°ê²° ì„±ê³µ ({frame.shape[1]}x{frame.shape[0]})")
            return True
            
        except Exception as e:
            self.logger.error(f"ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _capture_loop(self):
        """ë©”ì¸ ìº¡ì²˜ ë£¨í”„"""
        while True:
            if not self.is_paused:
                ret, frame = self.cap.read()
                if not ret:
                    print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                    break
                
                # í˜„ì¬ í”„ë ˆì„ ì €ì¥ (ì¼ì‹œì •ì§€ìš©)
                self.paused_frame = frame.copy()
            else:
                # ì¼ì‹œì •ì§€ ìƒíƒœì—ì„œëŠ” ì €ì¥ëœ í”„ë ˆì„ ì‚¬ìš©
                frame = self.paused_frame.copy()
            
            # ì–¼êµ´ ê²€ì¶œ
            detections = self.detector.detect(frame)
            
            # ê²€ì¶œ ê²°ê³¼ ì‹œê°í™”
            display_frame = self._visualize_detections(frame, detections)
            
            # ì •ë³´ ì˜¤ë²„ë ˆì´
            if self.show_info:
                display_frame = self._draw_info_overlay(display_frame, detections)
            
            # ë…¹í™” ì¤‘ì´ë©´ í”„ë ˆì„ ì €ì¥
            if self.is_recording and self.video_writer:
                self.video_writer.write(display_frame)
            
            # í™”ë©´ í‘œì‹œ
            cv2.imshow('Manual Face Capture - Press h for help', display_frame)
            
            # í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬
            action = self._handle_keyboard_input()
            if action == 'quit':
                break
            elif action == 'capture_face':
                self._capture_face_with_name(frame, detections)
            elif action == 'save_frame':
                self._save_current_frame(display_frame)
            elif action == 'toggle_record':
                self._toggle_recording(display_frame)
            elif action == 'toggle_pause':
                self._toggle_pause()
            elif action == 'show_help':
                self._print_help()
            elif action == 'toggle_info':
                self.show_info = not self.show_info
            elif action == 'increase_threshold':
                self._adjust_confidence_threshold(0.05)
            elif action == 'decrease_threshold':
                self._adjust_confidence_threshold(-0.05)
            
            # FPS ê³„ì‚°
            self._update_fps()
    
    def _visualize_detections(self, frame: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """ê²€ì¶œ ê²°ê³¼ ì‹œê°í™”"""
        display_frame = frame.copy()
        
        for detection in detections:
            bbox = detection['bbox']
            confidence = detection['confidence']
            
            # ì‹ ë¢°ë„ í•„í„°ë§
            if confidence < self.confidence_threshold:
                continue
            
            x, y, w, h = bbox
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            color = (0, 255, 0) if confidence >= 0.7 else (0, 255, 255)
            thickness = 2 if confidence >= 0.7 else 1
            
            cv2.rectangle(display_frame, (x, y), (x + w, y + h), color, thickness)
            
            # ì‹ ë¢°ë„ í…ìŠ¤íŠ¸
            text = f"{confidence:.2f}"
            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
            cv2.rectangle(display_frame, (x, y - text_size[1] - 5), 
                         (x + text_size[0], y), color, -1)
            cv2.putText(display_frame, text, (x, y - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)
        
        return display_frame
    
    def _draw_info_overlay(self, frame: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """ì •ë³´ ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸°"""
        # ë°˜íˆ¬ëª… ë°°ê²½
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (400, 120), (0, 0, 0), -1)
        frame = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)
        
        # ì •ë³´ í…ìŠ¤íŠ¸
        y_offset = 30
        line_height = 20
        
        # ê¸°ë³¸ ì •ë³´
        texts = [
            f"FPS: {self.current_fps:.1f}",
            f"Faces: {len([d for d in detections if d['confidence'] >= self.confidence_threshold])}",
            f"Confidence: {self.confidence_threshold:.2f}",
            f"Captured: {len(self.captured_faces)}",
        ]
        
        # ìƒíƒœ ì •ë³´
        if self.is_recording:
            elapsed = time.time() - self.recording_start_time
            texts.append(f"ğŸ”´ REC {elapsed:.0f}s")
        
        if self.is_paused:
            texts.append("â¸ï¸ PAUSED")
        
        for i, text in enumerate(texts):
            cv2.putText(frame, text, (20, y_offset + i * line_height), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return frame
    
    def _handle_keyboard_input(self) -> Optional[str]:
        """í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬"""
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q'):
            return 'quit'
        elif key == ord('i'):
            return 'toggle_info'
        elif key == ord('s'):
            return 'save_frame'
        elif key == ord('c'):
            return 'capture_face'
        elif key == ord('r'):
            return 'toggle_record'
        elif key == ord('p'):
            return 'toggle_pause'
        elif key == ord('h'):
            return 'show_help'
        elif key == ord('+') or key == ord('='):
            return 'increase_threshold'
        elif key == ord('-'):
            return 'decrease_threshold'
        
        return None
    
    def _capture_face_with_name(self, frame: np.ndarray, detections: List[Dict]):
        """ì–¼êµ´ ìº¡ì²˜ ë° ì´ë¦„ ì§€ì •"""
        # ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
        valid_detections = [d for d in detections if d['confidence'] >= self.confidence_threshold]
        
        if not valid_detections:
            print("âš ï¸  ìº¡ì²˜í•  ì–¼êµ´ì´ ì—†ìŠµë‹ˆë‹¤. (ì‹ ë¢°ë„ ê¸°ì¤€ ë¯¸ë‹¬)")
            return
        
        print(f"\nğŸ‘¤ {len(valid_detections)}ê°œ ì–¼êµ´ ë°œê²¬ë¨")
        
        # ì´ë¦„ ì…ë ¥ ì°½ í‘œì‹œ
        person_name = self._get_person_name_input(frame, valid_detections)
        
        if not person_name:
            print("â­ï¸  ìº¡ì²˜ ì·¨ì†Œë¨")
            return
        
        # ì–¼êµ´ë“¤ ì €ì¥
        saved_count = self._save_faces_to_staging(frame, valid_detections, person_name)
        
        if saved_count > 0:
            print(f"âœ… {person_name}: {saved_count}ê°œ ì–¼êµ´ ìº¡ì²˜ ì™„ë£Œ")
            self.captured_faces.extend(valid_detections)
        else:
            print("âŒ ì–¼êµ´ ì €ì¥ ì‹¤íŒ¨")
    
    def _get_person_name_input(self, frame: np.ndarray, detections: List[Dict]) -> Optional[str]:
        """ì´ë¦„ ì…ë ¥ ë°›ê¸°"""
        # ì–¼êµ´ ì˜ì—­ë“¤ì„ í•˜ì´ë¼ì´íŠ¸í•œ í”„ë ˆì„ ìƒì„±
        highlight_frame = frame.copy()
        for detection in detections:
            bbox = detection['bbox']
            x, y, w, h = bbox
            cv2.rectangle(highlight_frame, (x, y), (x + w, y + h), (0, 255, 0), 3)
            cv2.putText(highlight_frame, f"Conf: {detection['confidence']:.2f}", 
                       (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # ì…ë ¥ ì•ˆë‚´ í…ìŠ¤íŠ¸ ì¶”ê°€
        cv2.putText(highlight_frame, "Enter person name in console", 
                   (20, highlight_frame.shape[0] - 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        cv2.imshow('Face Capture - Enter name in console', highlight_frame)
        cv2.waitKey(1)
        
        # ì½˜ì†”ì—ì„œ ì´ë¦„ ì…ë ¥ ë°›ê¸°
        try:
            person_name = input(f"ğŸ‘¤ ì¸ë¬¼ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (Enter=ì·¨ì†Œ): ").strip()
            
            # ì…ë ¥ ì°½ ë‹«ê¸°
            cv2.destroyWindow('Face Capture - Enter name in console')
            
            return person_name if person_name else None
            
        except (KeyboardInterrupt, EOFError):
            cv2.destroyWindow('Face Capture - Enter name in console')
            return None
    
    def _save_faces_to_staging(self, frame: np.ndarray, detections: List[Dict], person_name: str) -> int:
        """ì–¼êµ´ë“¤ì„ face_stagingìœ¼ë¡œ ì €ì¥"""
        # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        safe_name = "".join(c for c in person_name if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_name = safe_name.replace(' ', '_')
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        folder_name = f"{safe_name}_{timestamp}_manual"
        staging_dir = self.face_staging_dir / folder_name
        staging_dir.mkdir(exist_ok=True)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            'person_name': person_name,
            'safe_name': safe_name,
            'created_at': timestamp,
            'source': 'manual_capture',
            'camera_id': self.camera_id,
            'face_count': len(detections),
            'confidence_threshold': self.confidence_threshold,
            'capture_session_id': str(uuid.uuid4())
        }
        
        metadata_file = staging_dir / 'metadata.json'
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # ì–¼êµ´ë“¤ ì €ì¥
        saved_count = 0
        for i, detection in enumerate(detections):
            bbox = detection['bbox']
            confidence = detection['confidence']
            
            # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
            face_crop = self._extract_face_crop(frame, bbox)
            if face_crop is None:
                continue
            
            # íŒŒì¼ëª… ìƒì„±
            filename = f"face_{safe_name}_{timestamp}_{i:02d}_conf{confidence:.2f}.jpg"
            dest_path = staging_dir / filename
            
            # ì´ë¯¸ì§€ ì €ì¥
            success = cv2.imwrite(str(dest_path), face_crop)
            if success:
                saved_count += 1
        
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {staging_dir}")
        return saved_count
    
    def _extract_face_crop(self, image: np.ndarray, bbox: Tuple[int, int, int, int]) -> Optional[np.ndarray]:
        """ì–¼êµ´ ì˜ì—­ ìë¥´ê¸°"""
        x, y, w, h = bbox
        
        # ì—¬ìœ ë¥¼ ë‘ê³  ìë¥´ê¸°
        margin = 20
        x1 = max(0, x - margin)
        y1 = max(0, y - margin)
        x2 = min(image.shape[1], x + w + margin)
        y2 = min(image.shape[0], y + h + margin)
        
        face_crop = image[y1:y2, x1:x2]
        
        if face_crop.size == 0 or face_crop.shape[0] < 50 or face_crop.shape[1] < 50:
            return None
        
        return face_crop
    
    def _save_current_frame(self, frame: np.ndarray):
        """í˜„ì¬ í”„ë ˆì„ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        filename = f"frame_{timestamp}.jpg"
        dest_path = self.output_dir / filename
        
        success = cv2.imwrite(str(dest_path), frame)
        if success:
            print(f"ğŸ“¸ í”„ë ˆì„ ì €ì¥: {dest_path}")
        else:
            print("âŒ í”„ë ˆì„ ì €ì¥ ì‹¤íŒ¨")
    
    def _toggle_recording(self, frame: np.ndarray):
        """ë…¹í™” í† ê¸€"""
        if not self.is_recording:
            # ë…¹í™” ì‹œì‘
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"recording_{timestamp}.mp4"
            dest_path = self.output_dir / filename
            
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            fps = 30
            frame_size = (frame.shape[1], frame.shape[0])
            
            self.video_writer = cv2.VideoWriter(str(dest_path), fourcc, fps, frame_size)
            
            if self.video_writer.isOpened():
                self.is_recording = True
                self.recording_start_time = time.time()
                print(f"ğŸ”´ ë…¹í™” ì‹œì‘: {dest_path}")
            else:
                print("âŒ ë…¹í™” ì‹œì‘ ì‹¤íŒ¨")
                self.video_writer = None
        else:
            # ë…¹í™” ì¤‘ì§€
            if self.video_writer:
                self.video_writer.release()
                self.video_writer = None
            
            elapsed = time.time() - self.recording_start_time
            self.is_recording = False
            print(f"â¹ï¸  ë…¹í™” ì¤‘ì§€ (ì´ {elapsed:.1f}ì´ˆ)")
    
    def _toggle_pause(self):
        """ì¼ì‹œì •ì§€ í† ê¸€"""
        self.is_paused = not self.is_paused
        status = "ì¼ì‹œì •ì§€" if self.is_paused else "ì¬ìƒ"
        print(f"â¸ï¸  {status}")
    
    def _adjust_confidence_threshold(self, delta: float):
        """ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì ˆ"""
        new_threshold = self.confidence_threshold + delta
        self.confidence_threshold = max(self.min_confidence, 
                                      min(self.max_confidence, new_threshold))
        print(f"ğŸšï¸  ì‹ ë¢°ë„ ì„ê³„ê°’: {self.confidence_threshold:.2f}")
    
    def _update_fps(self):
        """FPS ì—…ë°ì´íŠ¸"""
        self.fps_counter += 1
        current_time = time.time()
        
        if current_time - self.fps_timer >= 1.0:
            self.current_fps = self.fps_counter / (current_time - self.fps_timer)
            self.fps_counter = 0
            self.fps_timer = current_time
    
    def _print_help(self):
        """ë„ì›€ë§ ì¶œë ¥"""
        help_text = """
ğŸ“‹ í‚¤ë³´ë“œ ëª…ë ¹ì–´:
   i  - ì •ë³´ í‘œì‹œ í† ê¸€
   +  - ì‹ ë¢°ë„ ì„ê³„ê°’ ì¦ê°€
   -  - ì‹ ë¢°ë„ ì„ê³„ê°’ ê°ì†Œ
   s  - í˜„ì¬ í”„ë ˆì„ ì €ì¥
   c  - ì–¼êµ´ ìº¡ì²˜ + ì´ë¦„ ì§€ì •
   r  - ë…¹í™” ì‹œì‘/ì¤‘ì§€
   p  - ì¼ì‹œì •ì§€/ì¬ìƒ
   h  - ë„ì›€ë§ í‘œì‹œ
   q  - ì¢…ë£Œ
        """
        print(help_text)
    
    def _cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.cap:
            self.cap.release()
        
        if self.video_writer:
            self.video_writer.release()
        
        cv2.destroyAllWindows()
        
        print(f"\nğŸ“Š ì„¸ì…˜ ìš”ì•½:")
        print(f"   ìº¡ì²˜ëœ ì–¼êµ´: {len(self.captured_faces)}ê°œ")
        print(f"   í‰ê·  FPS: {self.current_fps:.1f}")
        print("âœ… ì‹œìŠ¤í…œ ì¢…ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ìˆ˜ë™ ì–¼êµ´ ìº¡ì²˜ ì‹œìŠ¤í…œ")
    parser.add_argument("--camera", type=int, default=0, help="ì¹´ë©”ë¼ ID (ê¸°ë³¸ê°’: 0)")
    args = parser.parse_args()
    
    # í•˜ë“œì›¨ì–´ ì—°ê²° ê²€ì¦
    print("ğŸ” í•˜ë“œì›¨ì–´ ì—°ê²° ìƒíƒœ í™•ì¸ ì¤‘...")
    
    # ì¹´ë©”ë¼ ì—°ê²° í…ŒìŠ¤íŠ¸
    test_cap = cv2.VideoCapture(args.camera)
    if not test_cap.isOpened():
        print(f"âŒ ì¹´ë©”ë¼ {args.camera}ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ë¥¸ ì¹´ë©”ë¼ IDë¥¼ ì‹œë„í•´ë³´ì„¸ìš”: --camera 1")
        return
    
    ret, frame = test_cap.read()
    test_cap.release()
    
    if not ret:
        print(f"âŒ ì¹´ë©”ë¼ {args.camera}ì—ì„œ ì˜ìƒì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ì¹´ë©”ë¼ {args.camera} ì—°ê²° í™•ì¸ë¨")
    
    # ì‹œìŠ¤í…œ ì‹œì‘
    capture_system = ManualFaceCaptureSystem(camera_id=args.camera)
    capture_system.start_capture()

if __name__ == "__main__":
    main() 