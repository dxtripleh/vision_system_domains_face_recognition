---
description: 
globs: 
alwaysApply: false
---
# í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­ ë° í™˜ê²½ ê·œì¹™ (Hardware Requirements & Environment Rules)

ì´ ê·œì¹™ì€ í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­, í™˜ê²½ ìë™ ê°ì§€, í”Œë«í¼ë³„ ìµœì í™”ì— ê´€í•œ í‘œì¤€ì…ë‹ˆë‹¤.

## ğŸ–¥ï¸ í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­

### ì§€ì› í”Œë«í¼
```python
SUPPORTED_PLATFORMS = {
    'windows': {
        'versions': ['10', '11'],
        'architectures': ['x64'],
        'python_versions': ['3.8', '3.9', '3.10', '3.11'],
        'special_requirements': {
            'visual_cpp_redistributable': '2019 ì´ìƒ',
            'direct_x': '12 ì´ìƒ'
        }
    },
    'linux': {
        'distributions': ['Ubuntu 18.04+', 'CentOS 7+', 'Debian 10+'],
        'architectures': ['x64', 'arm64'],
        'python_versions': ['3.8', '3.9', '3.10', '3.11'],
        'special_requirements': {
            'glibc': '2.27 ì´ìƒ',
            'gtk': '3.0 ì´ìƒ'
        }
    },
    'jetson': {
        'models': ['Nano', 'Xavier NX', 'AGX Xavier', 'TX2', 'Orin'],
        'jetpack_versions': ['4.6+', '5.0+'],
        'python_versions': ['3.6', '3.8', '3.9'],
        'special_requirements': {
            'cuda': '10.2 ì´ìƒ',
            'tensorrt': '8.0 ì´ìƒ',
            'opencv': '4.5 ì´ìƒ (CUDA ì§€ì›)'
        }
    },
    'raspberry_pi': {
        'models': ['4B', '400', 'CM4'],
        'os_versions': ['Bullseye', 'Bookworm'],
        'python_versions': ['3.9', '3.10', '3.11'],
        'special_requirements': {
            'gpu_memory_split': '128MB ì´ìƒ',
            'camera_module': 'v2 ì´ìƒ ê¶Œì¥'
        }
    }
}
```

### í•˜ë“œì›¨ì–´ êµ¬ì„± ìš”ì†Œ
```python
HARDWARE_COMPONENTS = {
    'cameras': {
        'usb_cameras': {
            'supported_formats': ['MJPG', 'YUYV', 'H264'],
            'min_resolution': (640, 480),
            'max_resolution': (1920, 1080),
            'min_fps': 15,
            'max_fps': 60,
            'usb_version': '2.0 ì´ìƒ'
        },
        'ip_cameras': {
            'protocols': ['RTSP', 'HTTP', 'ONVIF'],
            'encodings': ['H264', 'H265', 'MJPEG'],
            'authentication': ['None', 'Basic', 'Digest'],
            'network_requirements': {
                'bandwidth': '10Mbps ì´ìƒ',
                'latency': '100ms ì´í•˜'
            }
        },
        'csi_cameras': {
            'platforms': ['Jetson', 'Raspberry Pi'],
            'supported_sensors': ['IMX219', 'IMX477', 'OV5647'],
            'interface': 'CSI-2',
            'lanes': [2, 4]
        }
    },
    'compute': {
        'cpu': {
            'min_cores': 4,
            'min_frequency': '2.0GHz',
            'architectures': ['x64', 'arm64'],
            'instruction_sets': ['SSE4.2', 'AVX2']
        },
        'memory': {
            'min_ram': '8GB',
            'recommended_ram': '16GB',
            'swap_requirement': '2GB'
        },
        'gpu': {
            'nvidia': {
                'min_compute_capability': '6.1',
                'min_vram': '4GB',
                'cuda_versions': ['11.0', '11.2', '11.8', '12.0']
            },
            'amd': {
                'rocm_versions': ['5.0', '5.2', '5.4'],
                'min_vram': '4GB'
            },
            'intel': {
                'integrated_graphics': True,
                'arc_support': True,
                'opencl_version': '2.0 ì´ìƒ'
            }
        }
    },
    'storage': {
        'min_free_space': '50GB',
        'recommended_type': 'SSD',
        'io_speed': {
            'read': '100MB/s ì´ìƒ',
            'write': '50MB/s ì´ìƒ'
        }
    }
}
```

## ğŸ” í™˜ê²½ ìë™ ê°ì§€

### í•˜ë“œì›¨ì–´ ê°ì§€ í•¨ìˆ˜
```python
import platform
import psutil
import subprocess
import cv2
import torch

def detect_hardware_environment():
    """
    ì‹¤í–‰ í™˜ê²½ì˜ í•˜ë“œì›¨ì–´ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤.
    """
    env_info = {
        'platform': {
            'system': platform.system(),
            'release': platform.release(),
            'version': platform.version(),
            'machine': platform.machine(),
            'processor': platform.processor()
        },
        'compute': {
            'cpu_count': psutil.cpu_count(logical=False),
            'cpu_logical_count': psutil.cpu_count(logical=True),
            'cpu_frequency': psutil.cpu_freq().max if psutil.cpu_freq() else None,
            'memory_total': psutil.virtual_memory().total,
            'memory_available': psutil.virtual_memory().available
        },
        'gpu': {
            'cuda_available': torch.cuda.is_available(),
            'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'cuda_version': torch.version.cuda if torch.cuda.is_available() else None
        },
        'cameras': detect_cameras(),
        'storage': detect_storage_info()
    }
    
    return env_info

def detect_cameras():
    """ì—°ê²°ëœ ì¹´ë©”ë¼ ì¥ì¹˜ë¥¼ ê°ì§€í•©ë‹ˆë‹¤."""
    cameras = []
    
    # USB ì¹´ë©”ë¼ ê°ì§€
    for i in range(10):  # ìµœëŒ€ 10ê°œ ì¹´ë©”ë¼ í™•ì¸
        cap = cv2.VideoCapture(i)
        if cap.isOpened():
            ret, frame = cap.read()
            if ret:
                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                fps = cap.get(cv2.CAP_PROP_FPS)
                
                cameras.append({
                    'id': i,
                    'type': 'USB',
                    'resolution': (width, height),
                    'fps': fps,
                    'status': 'available'
                })
            cap.release()
    
    return cameras

def select_optimal_configuration(env_info):
    """í™˜ê²½ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì  ì„¤ì •ì„ ì„ íƒí•©ë‹ˆë‹¤."""
    config = {
        'device': 'cpu',
        'batch_size': 1,
        'precision': 'fp32',
        'num_workers': 1,
        'memory_fraction': 0.5
    }
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì„¤ì •
    if env_info['gpu']['cuda_available']:
        config.update({
            'device': 'cuda',
            'batch_size': min(4, env_info['gpu']['cuda_device_count'] * 2),
            'precision': 'fp16',
            'memory_fraction': 0.8
        })
    
    # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ì›Œì»¤ ì¡°ì •
    cpu_cores = env_info['compute']['cpu_count']
    config['num_workers'] = min(cpu_cores, 8)
    
    # ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
    memory_gb = env_info['compute']['memory_total'] / (1024**3)
    if memory_gb < 8:
        config['batch_size'] = max(1, config['batch_size'] // 2)
    
    return config
```

### í”Œë«í¼ë³„ ìµœì í™”
```python
PLATFORM_OPTIMIZATIONS = {
    'windows': {
        'opencv_backend': 'DSHOW',
        'threading_backend': 'TBB',
        'memory_management': 'automatic',
        'gpu_scheduling': 'hardware_accelerated'
    },
    'linux': {
        'opencv_backend': 'V4L2',
        'threading_backend': 'OpenMP',
        'memory_management': 'manual',
        'cpu_affinity': True
    },
    'jetson': {
        'opencv_backend': 'GSTREAMER',
        'threading_backend': 'CUDA',
        'memory_management': 'unified',
        'gpu_scheduling': 'cuda_context',
        'power_mode': 'MAXN',
        'jetson_clocks': True
    },
    'raspberry_pi': {
        'opencv_backend': 'V4L2',
        'threading_backend': 'OpenMP',
        'memory_management': 'conservative',
        'gpu_memory_split': 128,
        'camera_interface': 'MMAL'
    }
}
```

## âš™ï¸ ì„¤ì • ìë™ ì¡°ì •

### ëª¨ë¸ ì„ íƒ ì „ëµ
```python
def select_model_by_hardware(task, env_info):
    """í•˜ë“œì›¨ì–´ í™˜ê²½ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ"""
    
    model_configs = {
        'detection': {
            'high_performance': {
                'model': 'yolov8l',
                'requirements': {
                    'gpu_vram': 6,  # GB
                    'cpu_cores': 8,
                    'memory': 16  # GB
                }
            },
            'balanced': {
                'model': 'yolov8m',
                'requirements': {
                    'gpu_vram': 4,
                    'cpu_cores': 4,
                    'memory': 8
                }
            },
            'lightweight': {
                'model': 'yolov8n',
                'requirements': {
                    'gpu_vram': 2,
                    'cpu_cores': 2,
                    'memory': 4
                }
            }
        }
    }
    
    # í•˜ë“œì›¨ì–´ ì„±ëŠ¥ í‰ê°€
    gpu_vram = get_gpu_memory_gb(env_info)
    cpu_cores = env_info['compute']['cpu_count']
    memory_gb = env_info['compute']['memory_total'] / (1024**3)
    
    # ëª¨ë¸ ì„ íƒ
    task_models = model_configs.get(task, {})
    
    for performance_level in ['high_performance', 'balanced', 'lightweight']:
        requirements = task_models[performance_level]['requirements']
        
        if (gpu_vram >= requirements['gpu_vram'] and
            cpu_cores >= requirements['cpu_cores'] and
            memory_gb >= requirements['memory']):
            return task_models[performance_level]['model']
    
    # ìµœì†Œ ì‚¬ì–‘ ëª¨ë¸ ë°˜í™˜
    return task_models['lightweight']['model']

def get_gpu_memory_gb(env_info):
    """GPU ë©”ëª¨ë¦¬ í¬ê¸° ë°˜í™˜ (GB)"""
    if env_info['gpu']['cuda_available']:
        try:
            import torch
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            return gpu_memory / (1024**3)
        except:
            return 0
    return 0
```

## ğŸ—ï¸ í”Œë«í¼ë³„ ì„¤ì¹˜ ìš”êµ¬ì‚¬í•­

### Windows ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
```batch
@echo off
REM Windows í™˜ê²½ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

echo "ë¹„ì „ ì‹œìŠ¤í…œ Windows ì„¤ì¹˜ ì‹œì‘..."

REM Python ë²„ì „ í™•ì¸
python --version > nul 2>&1
if %errorlevel% neq 0 (
    echo "Pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Python 3.8 ì´ìƒì„ ì„¤ì¹˜í•˜ì„¸ìš”."
    exit /b 1
)

REM Visual C++ Redistributable í™•ì¸
reg query "HKLM\SOFTWARE\Microsoft\VisualStudio\14.0\VC\Runtimes\x64" > nul 2>&1
if %errorlevel% neq 0 (
    echo "Visual C++ Redistributableê°€ í•„ìš”í•©ë‹ˆë‹¤."
    echo "https://aka.ms/vs/17/release/vc_redist.x64.exe ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
)

REM CUDA ì„¤ì¹˜ í™•ì¸
nvcc --version > nul 2>&1
if %errorlevel% equ 0 (
    echo "CUDAê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
) else (
    echo "CUDAê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ë²„ì „ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
)

REM ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r config/requirements/requirements_windows.txt

echo "ì„¤ì¹˜ ì™„ë£Œ!"
```

### Linux ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# Linux í™˜ê²½ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

echo "ë¹„ì „ ì‹œìŠ¤í…œ Linux ì„¤ì¹˜ ì‹œì‘..."

# Python ë²„ì „ í™•ì¸
python3 --version >/dev/null 2>&1
if [ $? -ne 0 ]; then
    echo "Python3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "Ubuntu: sudo apt-get install python3 python3-pip"
    echo "CentOS: sudo yum install python3 python3-pip"
    exit 1
fi

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
if command -v apt-get >/dev/null 2>&1; then
    # Ubuntu/Debian
    sudo apt-get update
    sudo apt-get install -y python3-dev libgl1-mesa-glx libglib2.0-0
    sudo apt-get install -y v4l-utils  # ì¹´ë©”ë¼ ì§€ì›
elif command -v yum >/dev/null 2>&1; then
    # CentOS/RHEL
    sudo yum install -y python3-devel mesa-libGL glib2
    sudo yum install -y v4l-utils
fi

# CUDA í™•ì¸
if command -v nvcc >/dev/null 2>&1; then
    echo "CUDAê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
else
    echo "CUDAê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ë²„ì „ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."
    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
fi

# ì˜ì¡´ì„± ì„¤ì¹˜
pip3 install -r config/requirements/requirements_linux.txt

echo "ì„¤ì¹˜ ì™„ë£Œ!"
```

### Jetson ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# Jetson í™˜ê²½ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

echo "ë¹„ì „ ì‹œìŠ¤í…œ Jetson ì„¤ì¹˜ ì‹œì‘..."

# JetPack ë²„ì „ í™•ì¸
if [ ! -f /etc/nv_tegra_release ]; then
    echo "Jetson í”Œë«í¼ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    exit 1
fi

JETPACK_VERSION=$(cat /etc/nv_tegra_release | grep -o 'R[0-9]*')
echo "JetPack ë²„ì „: $JETPACK_VERSION"

# ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
sudo apt-get update

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
sudo apt-get install -y python3-pip python3-dev
sudo apt-get install -y libhdf5-serial-dev hdf5-tools
sudo apt-get install -y libhdf5-dev zlib1g-dev zip libjpeg8-dev

# Jetsonì— ìµœì í™”ëœ PyTorch ì„¤ì¹˜
if [ "$JETPACK_VERSION" = "R35" ] || [ "$JETPACK_VERSION" = "R34" ]; then
    # JetPack 5.x
    pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
elif [ "$JETPACK_VERSION" = "R32" ]; then
    # JetPack 4.x
    wget https://nvidia.box.com/shared/static/p57jwntv436lfrd78inwl7iml6p13fzh.whl -O torch-1.8.0-cp36-cp36m-linux_aarch64.whl
    pip3 install torch-1.8.0-cp36-cp36m-linux_aarch64.whl
fi

# Jetson ì „ìš© ì˜ì¡´ì„± ì„¤ì¹˜
pip3 install -r config/requirements/requirements_jetson.txt

# Jetson Clocks í™œì„±í™” (ì„±ëŠ¥ ìµœì í™”)
sudo jetson_clocks

echo "Jetson ì„¤ì¹˜ ì™„ë£Œ!"
```

## ğŸ”§ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ì‹¤ì‹œê°„ ì„±ëŠ¥ ë©”íŠ¸ë¦­
```python
import time
import psutil
import threading

class PerformanceMonitor:
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, interval=1.0):
        self.interval = interval
        self.monitoring = False
        self.metrics = {
            'cpu_percent': [],
            'memory_percent': [],
            'gpu_memory_used': [],
            'fps': [],
            'latency': []
        }
        
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join()
            
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring:
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=None)
            self.metrics['cpu_percent'].append(cpu_percent)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory_percent = psutil.virtual_memory().percent
            self.metrics['memory_percent'].append(memory_percent)
            
            # GPU ë©”ëª¨ë¦¬ (CUDA ì‚¬ìš© ì‹œ)
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()
                self.metrics['gpu_memory_used'].append(gpu_memory * 100)
            
            time.sleep(self.interval)
            
    def get_current_metrics(self):
        """í˜„ì¬ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        if not self.metrics['cpu_percent']:
            return None
            
        return {
            'cpu_percent': self.metrics['cpu_percent'][-1],
            'memory_percent': self.metrics['memory_percent'][-1],
            'gpu_memory_used': self.metrics['gpu_memory_used'][-1] if self.metrics['gpu_memory_used'] else 0
        }
```

## âš ï¸ í•˜ë“œì›¨ì–´ ê´€ë ¨ ê¸ˆì§€ ì‚¬í•­

1. **ì ˆëŒ€ ê¸ˆì§€**: í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì—†ì´ í”„ë¡œê·¸ë¨ ì‹¤í–‰
2. **ì ˆëŒ€ ê¸ˆì§€**: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì ì ˆí•œ ì˜¤ë¥˜ ì²˜ë¦¬ ëˆ„ë½
3. **ì ˆëŒ€ ê¸ˆì§€**: í”Œë«í¼ë³„ ìµœì í™” ì„¤ì • ë¬´ì‹œ
4. **ì ˆëŒ€ ê¸ˆì§€**: ì¹´ë©”ë¼ ì¥ì¹˜ ì ìœ  í›„ í•´ì œ ëˆ„ë½
5. **ì ˆëŒ€ ê¸ˆì§€**: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì—†ëŠ” ì¥ì‹œê°„ ì‹¤í–‰
6. **ì ˆëŒ€ ê¸ˆì§€**: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ ì¥ì¹˜ ì—†ëŠ” ë°˜ë³µ ì²˜ë¦¬
7. **ì ˆëŒ€ ê¸ˆì§€**: í•˜ë“œì›¨ì–´ í˜¸í™˜ì„± í™•ì¸ ì—†ëŠ” ëª¨ë¸ ë¡œë“œ
8. **ì ˆëŒ€ ê¸ˆì§€**: í™˜ê²½ë³„ ì˜ì¡´ì„± íŒ¨í‚¤ì§€ í˜¼ìš© ì„¤ì¹˜

ì´ ëª¨ë“  í•˜ë“œì›¨ì–´ ê´€ë ¨ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ì—¬ ì•ˆì •ì ì´ê³  ìµœì í™”ëœ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì„¸ìš”.

## ğŸ¤– ì•™ìƒë¸” ëª¨ë¸ ê´€ë¦¬ ê·œì¹™

### ì•™ìƒë¸” ì „ëµ ì •ì˜
```python
# common/models/ensemble_manager.py
from typing import Dict, List, Any, Optional
import torch
import numpy as np

class EnsembleManager:
    """ì•™ìƒë¸” ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    ENSEMBLE_STRATEGIES = {
        'voting': {
            'description': 'ë‹¤ìˆ˜ê²° íˆ¬í‘œ ë°©ì‹',
            'weight_by_confidence': True,
            'min_models': 3
        },
        'averaging': {
            'description': 'í™•ë¥  í‰ê·  ë°©ì‹',
            'weighted': True,
            'softmax': True
        },
        'stacking': {
            'description': 'ë©”íƒ€ ëª¨ë¸ í•™ìŠµ',
            'meta_model': 'lightgbm',
            'cross_validation': 5
        },
        'cascade': {
            'description': 'ê³„ì¸µì  ì•™ìƒë¸”',
            'confidence_threshold': 0.7
        }
    }
    
    def __init__(self, strategy: str = 'voting'):
        self.strategy = strategy
        self.models = []
        self.weights = []
        self.strategy_config = self.ENSEMBLE_STRATEGIES[strategy]
    
    def add_model(self, model, weight: float = 1.0, conf_threshold: float = 0.5):
        """ëª¨ë¸ ì¶”ê°€"""
        self.models.append({
            'model': model,
            'weight': weight,
            'conf_threshold': conf_threshold
        })
        self.weights.append(weight)
    
    def predict(self, input_data):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        if self.strategy == 'voting':
            return self._voting_predict(input_data)
        elif self.strategy == 'averaging':
            return self._averaging_predict(input_data)
        elif self.strategy == 'cascade':
            return self._cascade_predict(input_data)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì „ëµ: {self.strategy}")
    
    def _voting_predict(self, input_data):
        """ë‹¤ìˆ˜ê²° íˆ¬í‘œ ì˜ˆì¸¡"""
        predictions = []
        confidences = []
        
        for model_info in self.models:
            pred = model_info['model'].predict(input_data)
            predictions.append(pred)
            confidences.append(pred.get('confidence', 0.5))
        
        # ê°€ì¤‘ì¹˜ ì ìš© íˆ¬í‘œ
        if self.strategy_config['weight_by_confidence']:
            weights = [conf * weight for conf, weight in zip(confidences, self.weights)]
        else:
            weights = self.weights
        
        # ìµœì¢… ì˜ˆì¸¡ ê²°í•©
        return self._combine_predictions(predictions, weights)
    
    def _averaging_predict(self, input_data):
        """í‰ê·  ì˜ˆì¸¡"""
        predictions = []
        
        for model_info in self.models:
            pred = model_info['model'].predict(input_data)
            predictions.append(pred)
        
        # ê°€ì¤‘ í‰ê· 
        if self.strategy_config['weighted']:
            return self._weighted_average(predictions, self.weights)
        else:
            return self._simple_average(predictions)
    
    def _cascade_predict(self, input_data):
        """ê³„ì¸µì  ì˜ˆì¸¡"""
        for i, model_info in enumerate(self.models):
            pred = model_info['model'].predict(input_data)
            
            # ì‹ ë¢°ë„ê°€ ì„ê³„ê°’ ì´ìƒì´ë©´ ê²°ê³¼ ë°˜í™˜
            if pred.get('confidence', 0) >= self.strategy_config['confidence_threshold']:
                pred['used_models'] = i + 1
                return pred
        
        # ëª¨ë“  ëª¨ë¸ì„ ì‚¬ìš©í•´ë„ ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ë§ˆì§€ë§‰ ê²°ê³¼ ë°˜í™˜
        pred['used_models'] = len(self.models)
        pred['low_confidence_warning'] = True
        return pred

# ê°ì²´ ê°ì§€ ì•™ìƒë¸” ì„¤ì • ì˜ˆì‹œ
OBJECT_DETECTION_ENSEMBLE = {
    'models': [
        {'model_id': 'yolov8x', 'weight': 0.4, 'conf_threshold': 0.45},
        {'model_id': 'rt_detr_r50vd', 'weight': 0.3, 'conf_threshold': 0.4},
        {'model_id': 'faster_rcnn', 'weight': 0.2, 'conf_threshold': 0.5},
        {'model_id': 'ssd_mobilenet', 'weight': 0.1, 'conf_threshold': 0.5}
    ],
    'strategy': 'voting',
    'iou_threshold': 0.5,
    'weighted_box_fusion': True
}
```

### í•˜ë“œì›¨ì–´ ê¸°ë°˜ ì•™ìƒë¸” ìµœì í™”
```python
def optimize_ensemble_for_hardware():
    """í•˜ë“œì›¨ì–´ í™˜ê²½ì— ë”°ë¥¸ ì•™ìƒë¸” ìµœì í™”"""
    
    hardware_info = analyze_environment()
    
    if hardware_info['hardware'].get('gpu', {}).get('available', False):
        # GPU í™˜ê²½: ë” ë§ì€ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥
        gpu_memory = hardware_info['hardware']['gpu']['memory_gb']
        
        if gpu_memory >= 8:
            # ê³ ì„±ëŠ¥ GPU: 4ê°œ ëª¨ë¸ ì•™ìƒë¸”
            return {
                'models': ['yolov8x', 'rt_detr_r50vd', 'faster_rcnn', 'efficientdet_d7'],
                'strategy': 'voting',
                'batch_size': 4
            }
        elif gpu_memory >= 4:
            # ì¤‘ê°„ ì„±ëŠ¥ GPU: 3ê°œ ëª¨ë¸ ì•™ìƒë¸”
            return {
                'models': ['yolov8l', 'rt_detr_r18', 'faster_rcnn'],
                'strategy': 'averaging',
                'batch_size': 2
            }
        else:
            # ì €ì„±ëŠ¥ GPU: 2ê°œ ëª¨ë¸ ì•™ìƒë¸”
            return {
                'models': ['yolov8m', 'ssd_mobilenet'],
                'strategy': 'cascade',
                'batch_size': 1
            }
    else:
        # CPU í™˜ê²½: ê²½ëŸ‰ ëª¨ë¸ë§Œ ì‚¬ìš©
        cpu_cores = hardware_info['hardware']['cpu']['cores']
        
        if cpu_cores >= 8:
            return {
                'models': ['yolov8n', 'ssd_mobilenet'],
                'strategy': 'cascade',
                'batch_size': 1
            }
        else:
            # ë‹¨ì¼ ëª¨ë¸ ì‚¬ìš© ê¶Œì¥
            return {
                'models': ['yolov8n'],
                'strategy': 'single',
                'batch_size': 1
            }
```

## ğŸ’¾ í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ì—°ë™ ê·œì¹™

### í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ì„¤ì •
```python
# common/storage/cloud_storage.py
import os
import boto3
from azure.storage.blob import BlobServiceClient
from google.cloud import storage as gcs
from abc import ABC, abstractmethod

class CloudStorageInterface(ABC):
    """í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def upload_file(self, local_path: str, remote_path: str) -> bool:
        pass
    
    @abstractmethod
    def download_file(self, remote_path: str, local_path: str) -> bool:
        pass
    
    @abstractmethod
    def list_files(self, prefix: str = "") -> List[str]:
        pass

class AWSStorage(CloudStorageInterface):
    """AWS S3 ìŠ¤í† ë¦¬ì§€"""
    
    def __init__(self, bucket_name: str, region: str = 'us-east-1'):
        self.bucket_name = bucket_name
        self.s3_client = boto3.client(
            's3',
            region_name=region,
            aws_access_key_id=os.getenv('AWS_ACCESS_KEY'),
            aws_secret_access_key=os.getenv('AWS_SECRET_KEY')
        )
    
    def upload_file(self, local_path: str, remote_path: str) -> bool:
        try:
            self.s3_client.upload_file(local_path, self.bucket_name, remote_path)
            return True
        except Exception as e:
            print(f"AWS ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def download_file(self, remote_path: str, local_path: str) -> bool:
        try:
            self.s3_client.download_file(self.bucket_name, remote_path, local_path)
            return True
        except Exception as e:
            print(f"AWS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

class AzureStorage(CloudStorageInterface):
    """Azure Blob Storage"""
    
    def __init__(self, container_name: str):
        self.container_name = container_name
        connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
        self.blob_service = BlobServiceClient.from_connection_string(connection_string)
    
    def upload_file(self, local_path: str, remote_path: str) -> bool:
        try:
            blob_client = self.blob_service.get_blob_client(
                container=self.container_name, 
                blob=remote_path
            )
            with open(local_path, 'rb') as data:
                blob_client.upload_blob(data, overwrite=True)
            return True
        except Exception as e:
            print(f"Azure ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

class GCPStorage(CloudStorageInterface):
    """Google Cloud Storage"""
    
    def __init__(self, bucket_name: str):
        self.bucket_name = bucket_name
        credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
        self.client = gcs.Client.from_service_account_json(credentials_path)
        self.bucket = self.client.bucket(bucket_name)
    
    def upload_file(self, local_path: str, remote_path: str) -> bool:
        try:
            blob = self.bucket.blob(remote_path)
            blob.upload_from_filename(local_path)
            return True
        except Exception as e:
            print(f"GCP ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

# í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ íŒ©í† ë¦¬
def create_cloud_storage(provider: str, **config) -> CloudStorageInterface:
    """í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ìƒì„±"""
    if provider == 'aws':
        return AWSStorage(config['bucket'], config.get('region', 'us-east-1'))
    elif provider == 'azure':
        return AzureStorage(config['container'])
    elif provider == 'gcp':
        return GCPStorage(config['bucket'])
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œë°”ì´ë”: {provider}")

# ì‚¬ìš© ì˜ˆì‹œ
CLOUD_STORAGE_CONFIG = {
    'aws': {
        'bucket': 'vision-system-data',
        'region': 'us-east-1',
        'path_prefix': 'data/'
    },
    'azure': {
        'container': 'vision-system-data',
        'path_prefix': 'data/'
    },
    'gcp': {
        'bucket': 'vision-system-data',
        'path_prefix': 'data/'
    }
}
```

### ìë™ ë°±ì—… ì‹œìŠ¤í…œ
```python
# common/storage/backup_manager.py
import os
import shutil
import gzip
import schedule
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path

class BackupManager:
    """ìë™ ë°±ì—… ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: dict):
        self.config = config
        self.enabled = config.get('enabled', True)
        self.retention_days = config.get('retention_days', 30)
        self.compression = config.get('compression', True)
        self.include_patterns = config.get('include_patterns', [])
        self.exclude_patterns = config.get('exclude_patterns', [])
        self.backup_dir = Path(config.get('backup_dir', 'data/backups'))
        self.logger = logging.getLogger(__name__)
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        self.backup_dir.mkdir(parents=True, exist_ok=True)
    
    def setup_schedule(self):
        """ë°±ì—… ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        if not self.enabled:
            return
        
        schedule_str = self.config.get('schedule', '0 2 * * *')  # ë§¤ì¼ 02:00
        
        # Cron í˜•ì‹ì„ schedule ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if schedule_str == '0 2 * * *':
            schedule.every().day.at("02:00").do(self.run_backup)
        elif schedule_str == '0 */6 * * *':
            schedule.every(6).hours.do(self.run_backup)
        
        self.logger.info(f"ë°±ì—… ìŠ¤ì¼€ì¤„ ì„¤ì •: {schedule_str}")
    
    def run_backup(self):
        """ë°±ì—… ì‹¤í–‰"""
        if not self.enabled:
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f"backup_{timestamp}"
        backup_path = self.backup_dir / backup_name
        
        self.logger.info(f"ë°±ì—… ì‹œì‘: {backup_path}")
        
        try:
            # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
            backup_path.mkdir(exist_ok=True)
            
            # íŒŒì¼ ë°±ì—…
            backed_up_files = 0
            for pattern in self.include_patterns:
                for file_path in Path('.').rglob(pattern):
                    if self._should_backup(file_path):
                        self._backup_file(file_path, backup_path)
                        backed_up_files += 1
            
            # ì••ì¶•
            if self.compression:
                self._compress_backup(backup_path)
            
            # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
            self._cleanup_old_backups()
            
            self.logger.info(f"ë°±ì—… ì™„ë£Œ: {backed_up_files}ê°œ íŒŒì¼")
            
        except Exception as e:
            self.logger.error(f"ë°±ì—… ì‹¤íŒ¨: {e}")
    
    def _should_backup(self, file_path: Path) -> bool:
        """ë°±ì—… ëŒ€ìƒ í™•ì¸"""
        # ì œì™¸ íŒ¨í„´ í™•ì¸
        for exclude_pattern in self.exclude_patterns:
            if file_path.match(exclude_pattern):
                return False
        return True
    
    def _backup_file(self, source: Path, backup_dir: Path):
        """íŒŒì¼ ë°±ì—…"""
        relative_path = source.relative_to('.')
        target_path = backup_dir / relative_path
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        target_path.parent.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ ë³µì‚¬
        shutil.copy2(source, target_path)
    
    def _compress_backup(self, backup_path: Path):
        """ë°±ì—… ì••ì¶•"""
        archive_path = backup_path.with_suffix('.tar.gz')
        
        import tarfile
        with tarfile.open(archive_path, 'w:gz') as tar:
            tar.add(backup_path, arcname=backup_path.name)
        
        # ì›ë³¸ ë””ë ‰í† ë¦¬ ì‚­ì œ
        shutil.rmtree(backup_path)
        
        self.logger.info(f"ë°±ì—… ì••ì¶• ì™„ë£Œ: {archive_path}")
    
    def _cleanup_old_backups(self):
        """ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=self.retention_days)
        
        for backup_file in self.backup_dir.iterdir():
            if backup_file.is_file() and backup_file.name.startswith('backup_'):
                file_time = datetime.fromtimestamp(backup_file.stat().st_mtime)
                if file_time < cutoff_date:
                    backup_file.unlink()
                    self.logger.info(f"ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {backup_file}")

# ë°±ì—… ì„¤ì • ì˜ˆì‹œ
BACKUP_CONFIG = {
    'enabled': True,
    'schedule': '0 2 * * *',  # ë§¤ì¼ 02:00
    'retention_days': 30,
    'compression': True,
    'include_patterns': ['*.db', 'results/*.json', 'config/*.yaml'],
    'exclude_patterns': ['*.tmp', 'cache/*', 'temp/*'],
    'backup_dir': 'data/backups'
}
```

### í´ë¼ìš°ë“œ ë™ê¸°í™”
```python
def sync_to_cloud(cloud_provider: str, local_path: str, remote_path: str):
    """ë¡œì»¬ ë°ì´í„°ë¥¼ í´ë¼ìš°ë“œì™€ ë™ê¸°í™”"""
    
    storage = create_cloud_storage(
        cloud_provider, 
        **CLOUD_STORAGE_CONFIG[cloud_provider]
    )
    
    # íŒŒì¼ ì—…ë¡œë“œ
    success = storage.upload_file(local_path, remote_path)
    
    if success:
        logging.info(f"í´ë¼ìš°ë“œ ë™ê¸°í™” ì„±ê³µ: {local_path} -> {remote_path}")
    else:
        logging.error(f"í´ë¼ìš°ë“œ ë™ê¸°í™” ì‹¤íŒ¨: {local_path}")
    
    return success
```

## ğŸ¤– ëª¨ë¸ ë„¤ì´ë° ë° ë²„ì „ ê´€ë¦¬

### ëª¨ë¸ íŒŒì¼ ëª…ëª… ê·œì¹™
ì²´ê³„ì ì¸ ëª¨ë¸ íŒŒì¼ ëª…ëª… ê·œì¹™ì„ í†µí•´ ëª¨ë¸ì˜ ìš©ë„, ì•„í‚¤í…ì²˜, ë²„ì „ì„ ëª…í™•íˆ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ëª¨ë¸ ë„¤ì´ë° ë° ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ

ëª¨ë¸ íŒŒì¼ì˜ ì²´ê³„ì ì¸ ëª…ëª…ê³¼ ë²„ì „ ê´€ë¦¬ë¥¼ ìœ„í•œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
"""

import os
import re
import json
import hashlib
import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum


class ModelType(Enum):
    """ëª¨ë¸ íƒ€ì…"""
    DETECTION = "detection"
    CLASSIFICATION = "classification"
    SEGMENTATION = "segmentation"
    FACE_RECOGNITION = "face_recognition"
    EMOTION_RECOGNITION = "emotion_recognition"
    POSE_ESTIMATION = "pose_estimation"
    ANOMALY_DETECTION = "anomaly_detection"


class ModelFramework(Enum):
    """ëª¨ë¸ í”„ë ˆì„ì›Œí¬"""
    PYTORCH = "pytorch"
    ONNX = "onnx"
    TENSORRT = "tensorrt"
    TENSORFLOW = "tensorflow"
    OPENVINO = "openvino"


class ModelArchitecture(Enum):
    """ëª¨ë¸ ì•„í‚¤í…ì²˜"""
    YOLOV8 = "yolov8"
    YOLOV5 = "yolov5"
    EFFICIENTDET = "efficientdet"
    RESNET = "resnet"
    MOBILENET = "mobilenet"
    TRANSFORMER = "transformer"
    CUSTOM = "custom"


@dataclass
class ModelMetadata:
    """ëª¨ë¸ ë©”íƒ€ë°ì´í„°"""
    # ê¸°ë³¸ ì •ë³´
    name: str
    version: str
    model_type: ModelType
    architecture: ModelArchitecture
    framework: ModelFramework
    
    # íŒŒì¼ ì •ë³´
    file_path: str
    file_size: int
    file_hash: str
    created_date: str
    modified_date: str
    
    # ì„±ëŠ¥ ì •ë³´
    accuracy: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None
    f1_score: Optional[float] = None
    inference_time_ms: Optional[float] = None
    model_size_mb: Optional[float] = None
    
    # í›ˆë ¨ ì •ë³´
    dataset_name: Optional[str] = None
    dataset_version: Optional[str] = None
    training_epochs: Optional[int] = None
    batch_size: Optional[int] = None
    learning_rate: Optional[float] = None
    
    # í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
    min_gpu_memory_gb: Optional[float] = None
    min_cpu_cores: Optional[int] = None
    min_ram_gb: Optional[float] = None
    
    # ì „ì²˜ë¦¬ ì •ë³´
    input_size: Optional[Tuple[int, int]] = None
    mean: Optional[Tuple[float, float, float]] = None
    std: Optional[Tuple[float, float, float]] = None
    
    # ê¸°íƒ€
    description: Optional[str] = None
    tags: Optional[List[str]] = None
    author: Optional[str] = None
    license: Optional[str] = None


class ModelNamingManager:
    """ëª¨ë¸ ë„¤ì´ë° ê´€ë¦¬ì"""
    
    # ë„¤ì´ë° íŒ¨í„´: {task}_{architecture}_{dataset}_{version}_{date}.{ext}
    NAMING_PATTERN = r"^(?P<task>\w+)_(?P<architecture>\w+)_(?P<dataset>\w+)_(?P<version>v\d+\.\d+\.\d+)_(?P<date>\d{8})\.(?P<ext>\w+)$"
    
    def __init__(self, models_dir: str = "models/weights"):
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        self.metadata_dir = self.models_dir.parent / "metadata"
        self.metadata_dir.mkdir(exist_ok=True)
    
    def generate_model_name(self, 
                          task: str,
                          architecture: str, 
                          dataset: str,
                          version: str = "v1.0.0",
                          extension: str = "pt") -> str:
        """ëª¨ë¸ íŒŒì¼ëª… ìƒì„±"""
        date_str = datetime.datetime.now().strftime("%Y%m%d")
        return f"{task}_{architecture}_{dataset}_{version}_{date_str}.{extension}"
    
    def parse_model_name(self, filename: str) -> Optional[Dict[str, str]]:
        """ëª¨ë¸ íŒŒì¼ëª… íŒŒì‹±"""
        match = re.match(self.NAMING_PATTERN, filename)
        if match:
            return match.groupdict()
        return None
    
    def validate_model_name(self, filename: str) -> bool:
        """ëª¨ë¸ íŒŒì¼ëª… ìœ íš¨ì„± ê²€ì¦"""
        return self.parse_model_name(filename) is not None
    
    def increment_version(self, version: str, increment_type: str = "patch") -> str:
        """ë²„ì „ ì¦ê°€"""
        # v1.2.3 í˜•ì‹ì—ì„œ ìˆ«ì ì¶”ì¶œ
        match = re.match(r"v(\d+)\.(\d+)\.(\d+)", version)
        if not match:
            return "v1.0.0"
        
        major, minor, patch = map(int, match.groups())
        
        if increment_type == "major":
            major += 1
            minor = 0
            patch = 0
        elif increment_type == "minor":
            minor += 1
            patch = 0
        else:  # patch
            patch += 1
        
        return f"v{major}.{minor}.{patch}"
    
    def get_latest_version(self, task: str, architecture: str, dataset: str) -> str:
        """ìµœì‹  ë²„ì „ ì¡°íšŒ"""
        pattern = f"{task}_{architecture}_{dataset}_v*"
        matching_files = list(self.models_dir.glob(pattern))
        
        if not matching_files:
            return "v1.0.0"
        
        versions = []
        for file_path in matching_files:
            parsed = self.parse_model_name(file_path.name)
            if parsed:
                versions.append(parsed['version'])
        
        if not versions:
            return "v1.0.0"
        
        # ë²„ì „ ì •ë ¬ (v1.0.0 í˜•ì‹)
        def version_key(v):
            match = re.match(r"v(\d+)\.(\d+)\.(\d+)", v)
            return tuple(map(int, match.groups())) if match else (0, 0, 0)
        
        latest = max(versions, key=version_key)
        return self.increment_version(latest, "patch")
```

## ğŸ¯ ì•™ìƒë¸” ëª¨ë¸ ê´€ë¦¬

### ì•™ìƒë¸” ì „ëµ êµ¬í˜„
ë‹¤ì–‘í•œ ì•™ìƒë¸” ì „ëµì„ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì•™ìƒë¸” ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ

ì—¬ëŸ¬ ëª¨ë¸ì„ ì¡°í•©í•˜ì—¬ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì•™ìƒë¸” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
"""

import numpy as np
import torch
from typing import List, Dict, Any, Optional, Callable
from enum import Enum
from abc import ABC, abstractmethod


class EnsembleStrategy(Enum):
    """ì•™ìƒë¸” ì „ëµ"""
    VOTING = "voting"
    AVERAGING = "averaging"
    STACKING = "stacking"
    CASCADE = "cascade"


class BaseEnsemble(ABC):
    """ì•™ìƒë¸” ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, models: List[Any], weights: Optional[List[float]] = None):
        self.models = models
        self.weights = weights or [1.0 / len(models)] * len(models)
        self.num_models = len(models)
    
    @abstractmethod
    def predict(self, inputs: Any) -> Any:
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        pass
    
    def validate_inputs(self, inputs: Any) -> bool:
        """ì…ë ¥ ê²€ì¦"""
        return inputs is not None


class VotingEnsemble(BaseEnsemble):
    """íˆ¬í‘œ ê¸°ë°˜ ì•™ìƒë¸”"""
    
    def __init__(self, models: List[Any], voting_type: str = "hard", weights: Optional[List[float]] = None):
        super().__init__(models, weights)
        self.voting_type = voting_type  # "hard" or "soft"
    
    def predict(self, inputs: Any) -> Any:
        """íˆ¬í‘œ ê¸°ë°˜ ì˜ˆì¸¡"""
        predictions = []
        
        for model in self.models:
            pred = model(inputs)
            predictions.append(pred)
        
        if self.voting_type == "hard":
            return self._hard_voting(predictions)
        else:
            return self._soft_voting(predictions)
    
    def _hard_voting(self, predictions: List[Any]) -> Any:
        """í•˜ë“œ íˆ¬í‘œ (ë‹¤ìˆ˜ê²°)"""
        # ê° ì˜ˆì¸¡ì„ í´ë˜ìŠ¤ë¡œ ë³€í™˜ í›„ íˆ¬í‘œ
        class_predictions = []
        for pred in predictions:
            if isinstance(pred, torch.Tensor):
                class_pred = torch.argmax(pred, dim=-1)
            else:
                class_pred = np.argmax(pred, axis=-1)
            class_predictions.append(class_pred)
        
        # ë‹¤ìˆ˜ê²° íˆ¬í‘œ
        if isinstance(class_predictions[0], torch.Tensor):
            stacked = torch.stack(class_predictions, dim=0)
            return torch.mode(stacked, dim=0)[0]
        else:
            stacked = np.stack(class_predictions, axis=0)
            return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=stacked)
    
    def _soft_voting(self, predictions: List[Any]) -> Any:
        """ì†Œí”„íŠ¸ íˆ¬í‘œ (í™•ë¥  í‰ê· )"""
        weighted_preds = []
        
        for pred, weight in zip(predictions, self.weights):
            if isinstance(pred, torch.Tensor):
                weighted_pred = pred * weight
            else:
                weighted_pred = pred * weight
            weighted_preds.append(weighted_pred)
        
        if isinstance(weighted_preds[0], torch.Tensor):
            return torch.sum(torch.stack(weighted_preds, dim=0), dim=0)
        else:
            return np.sum(np.stack(weighted_preds, axis=0), axis=0)


class AveragingEnsemble(BaseEnsemble):
    """í‰ê·  ê¸°ë°˜ ì•™ìƒë¸”"""
    
    def __init__(self, models: List[Any], averaging_type: str = "arithmetic", weights: Optional[List[float]] = None):
        super().__init__(models, weights)
        self.averaging_type = averaging_type  # "arithmetic", "geometric", "harmonic"
    
    def predict(self, inputs: Any) -> Any:
        """í‰ê·  ê¸°ë°˜ ì˜ˆì¸¡"""
        predictions = []
        
        for model in self.models:
            pred = model(inputs)
            predictions.append(pred)
        
        if self.averaging_type == "arithmetic":
            return self._arithmetic_mean(predictions)
        elif self.averaging_type == "geometric":
            return self._geometric_mean(predictions)
        elif self.averaging_type == "harmonic":
            return self._harmonic_mean(predictions)
    
    def _arithmetic_mean(self, predictions: List[Any]) -> Any:
        """ì‚°ìˆ  í‰ê· """
        weighted_preds = []
        
        for pred, weight in zip(predictions, self.weights):
            weighted_preds.append(pred * weight)
        
        if isinstance(weighted_preds[0], torch.Tensor):
            return torch.sum(torch.stack(weighted_preds, dim=0), dim=0)
        else:
            return np.sum(np.stack(weighted_preds, axis=0), axis=0)
    
    def _geometric_mean(self, predictions: List[Any]) -> Any:
        """ê¸°í•˜ í‰ê· """
        if isinstance(predictions[0], torch.Tensor):
            product = torch.ones_like(predictions[0])
            for pred, weight in zip(predictions, self.weights):
                product *= torch.pow(pred, weight)
            return product
        else:
            product = np.ones_like(predictions[0])
            for pred, weight in zip(predictions, self.weights):
                product *= np.power(pred, weight)
            return product
    
    def _harmonic_mean(self, predictions: List[Any]) -> Any:
        """ì¡°í™” í‰ê· """
        if isinstance(predictions[0], torch.Tensor):
            reciprocal_sum = torch.zeros_like(predictions[0])
            for pred, weight in zip(predictions, self.weights):
                reciprocal_sum += weight / pred
            return len(predictions) / reciprocal_sum
        else:
            reciprocal_sum = np.zeros_like(predictions[0])
            for pred, weight in zip(predictions, self.weights):
                reciprocal_sum += weight / pred
            return len(predictions) / reciprocal_sum


class StackingEnsemble(BaseEnsemble):
    """ìŠ¤íƒœí‚¹ ì•™ìƒë¸”"""
    
    def __init__(self, base_models: List[Any], meta_model: Any, use_original_features: bool = True):
        super().__init__(base_models)
        self.meta_model = meta_model
        self.use_original_features = use_original_features
        self.is_trained = False
    
    def fit(self, X_train: Any, y_train: Any, X_val: Any, y_val: Any):
        """ë©”íƒ€ ëª¨ë¸ í›ˆë ¨"""
        # ë² ì´ìŠ¤ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘
        base_predictions_train = self._get_base_predictions(X_train)
        base_predictions_val = self._get_base_predictions(X_val)
        
        # ë©”íƒ€ í”¼ì²˜ êµ¬ì„±
        meta_features_train = self._construct_meta_features(base_predictions_train, X_train)
        meta_features_val = self._construct_meta_features(base_predictions_val, X_val)
        
        # ë©”íƒ€ ëª¨ë¸ í›ˆë ¨
        self.meta_model.fit(meta_features_train, y_train)
        self.is_trained = True
        
        # ê²€ì¦ ì„±ëŠ¥ ë°˜í™˜
        val_pred = self.meta_model.predict(meta_features_val)
        return val_pred
    
    def predict(self, inputs: Any) -> Any:
        """ìŠ¤íƒœí‚¹ ì˜ˆì¸¡"""
        if not self.is_trained:
            raise ValueError("Meta model is not trained. Call fit() first.")
        
        base_predictions = self._get_base_predictions(inputs)
        meta_features = self._construct_meta_features(base_predictions, inputs)
        
        return self.meta_model.predict(meta_features)
    
    def _get_base_predictions(self, inputs: Any) -> List[Any]:
        """ë² ì´ìŠ¤ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘"""
        predictions = []
        for model in self.models:
            pred = model(inputs)
            predictions.append(pred)
        return predictions
    
    def _construct_meta_features(self, base_predictions: List[Any], original_features: Any) -> Any:
        """ë©”íƒ€ í”¼ì²˜ êµ¬ì„±"""
        if isinstance(base_predictions[0], torch.Tensor):
            meta_features = torch.cat(base_predictions, dim=-1)
            if self.use_original_features:
                meta_features = torch.cat([meta_features, original_features], dim=-1)
        else:
            meta_features = np.concatenate(base_predictions, axis=-1)
            if self.use_original_features:
                meta_features = np.concatenate([meta_features, original_features], axis=-1)
        
        return meta_features


class CascadeEnsemble(BaseEnsemble):
    """ìºìŠ¤ì¼€ì´ë“œ ì•™ìƒë¸”"""
    
    def __init__(self, models: List[Any], thresholds: List[float]):
        super().__init__(models)
        self.thresholds = thresholds
        if len(models) != len(thresholds) + 1:
            raise ValueError("Number of thresholds should be one less than number of models")
    
    def predict(self, inputs: Any) -> Any:
        """ìºìŠ¤ì¼€ì´ë“œ ì˜ˆì¸¡"""
        current_inputs = inputs
        
        for i, (model, threshold) in enumerate(zip(self.models[:-1], self.thresholds)):
            predictions = model(current_inputs)
            
            # ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§
            confident_mask = self._get_confident_predictions(predictions, threshold)
            
            if confident_mask.sum() == len(predictions):
                # ëª¨ë“  ì˜ˆì¸¡ì´ ì‹ ë¢°ë„ë¥¼ ë§Œì¡±í•˜ë©´ ë°˜í™˜
                return predictions
            
            # ì‹ ë¢°ë„ê°€ ë‚®ì€ ìƒ˜í”Œë“¤ë§Œ ë‹¤ìŒ ëª¨ë¸ë¡œ ì „ë‹¬
            current_inputs = self._filter_inputs(current_inputs, ~confident_mask)
        
        # ë§ˆì§€ë§‰ ëª¨ë¸ë¡œ ë‚˜ë¨¸ì§€ ì˜ˆì¸¡
        final_predictions = self.models[-1](mdc:current_inputs)
        return final_predictions
    
    def _get_confident_predictions(self, predictions: Any, threshold: float) -> Any:
        """ì‹ ë¢°ë„ê°€ ë†’ì€ ì˜ˆì¸¡ ì‹ë³„"""
        if isinstance(predictions, torch.Tensor):
            max_probs = torch.max(torch.softmax(predictions, dim=-1), dim=-1)[0]
            return max_probs > threshold
        else:
            max_probs = np.max(predictions, axis=-1)
            return max_probs > threshold
    
    def _filter_inputs(self, inputs: Any, mask: Any) -> Any:
        """ë§ˆìŠ¤í¬ì— ë”°ë¼ ì…ë ¥ í•„í„°ë§"""
        if isinstance(inputs, torch.Tensor):
            return inputs[mask]
        else:
            return inputs[mask]


class EnsembleManager:
    """ì•™ìƒë¸” ê´€ë¦¬ì"""
    
    def __init__(self):
        self.ensembles: Dict[str, BaseEnsemble] = {}
        self.performance_history: Dict[str, List[float]] = {}
    
    def create_ensemble(self, 
                       name: str,
                       strategy: EnsembleStrategy,
                       models: List[Any],
                       **kwargs) -> BaseEnsemble:
        """ì•™ìƒë¸” ìƒì„±"""
        if strategy == EnsembleStrategy.VOTING:
            ensemble = VotingEnsemble(models, **kwargs)
        elif strategy == EnsembleStrategy.AVERAGING:
            ensemble = AveragingEnsemble(models, **kwargs)
        elif strategy == EnsembleStrategy.STACKING:
            ensemble = StackingEnsemble(models, **kwargs)
        elif strategy == EnsembleStrategy.CASCADE:
            ensemble = CascadeEnsemble(models, **kwargs)
        else:
            raise ValueError(f"Unknown ensemble strategy: {strategy}")
        
        self.ensembles[name] = ensemble
        self.performance_history[name] = []
        
        return ensemble
    
    def get_ensemble(self, name: str) -> Optional[BaseEnsemble]:
        """ì•™ìƒë¸” ì¡°íšŒ"""
        return self.ensembles.get(name)
    
    def evaluate_ensemble(self, name: str, test_data: Any, test_labels: Any) -> float:
        """ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€"""
        ensemble = self.get_ensemble(name)
        if not ensemble:
            raise ValueError(f"Ensemble '{name}' not found")
        
        predictions = ensemble.predict(test_data)
        accuracy = self._calculate_accuracy(predictions, test_labels)
        
        self.performance_history[name].append(accuracy)
        return accuracy
    
    def _calculate_accuracy(self, predictions: Any, labels: Any) -> float:
        """ì •í™•ë„ ê³„ì‚°"""
        if isinstance(predictions, torch.Tensor):
            pred_classes = torch.argmax(predictions, dim=-1)
            correct = (pred_classes == labels).float().mean().item()
        else:
            pred_classes = np.argmax(predictions, axis=-1)
            correct = np.mean(pred_classes == labels)
        
        return correct
    
    def get_best_ensemble(self) -> Tuple[str, float]:
        """ìµœê³  ì„±ëŠ¥ ì•™ìƒë¸” ì¡°íšŒ"""
        best_name = None
        best_performance = 0.0
        
        for name, history in self.performance_history.items():
            if history:
                avg_performance = np.mean(history)
                if avg_performance > best_performance:
                    best_performance = avg_performance
                    best_name = name
        
        return best_name, best_performance
    
    def optimize_ensemble_weights(self, name: str, validation_data: Any, validation_labels: Any):
        """ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”"""
        ensemble = self.get_ensemble(name)
        if not ensemble or not hasattr(ensemble, 'weights'):
            return
        
        from scipy.optimize import minimize
        
        def objective(weights):
            ensemble.weights = weights.tolist()
            predictions = ensemble.predict(validation_data)
            accuracy = self._calculate_accuracy(predictions, validation_labels)
            return -accuracy  # ìµœì†Œí™”ë¥¼ ìœ„í•´ ìŒìˆ˜ ë°˜í™˜
        
        # ì œì•½ ì¡°ê±´: ê°€ì¤‘ì¹˜ í•©ì´ 1
        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
        bounds = [(0, 1) for _ in range(ensemble.num_models)]
        
        initial_weights = np.array(ensemble.weights)
        result = minimize(objective, initial_weights, bounds=bounds, constraints=constraints)
        
        if result.success:
            ensemble.weights = result.x.tolist()
            print(f"Optimized weights for {name}: {ensemble.weights}")
        else:
            print(f"Weight optimization failed for {name}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê°€ìƒì˜ ëª¨ë¸ë“¤
    models = [lambda x: np.random.rand(10, 3) for _ in range(3)]
    
    # ì•™ìƒë¸” ê´€ë¦¬ì ìƒì„±
    ensemble_manager = EnsembleManager()
    
    # íˆ¬í‘œ ì•™ìƒë¸” ìƒì„±
    voting_ensemble = ensemble_manager.create_ensemble(
        name="voting_ensemble",
        strategy=EnsembleStrategy.VOTING,
        models=models,
        voting_type="soft",
        weights=[0.4, 0.3, 0.3]
    )
    
    # í‰ê·  ì•™ìƒë¸” ìƒì„±
    averaging_ensemble = ensemble_manager.create_ensemble(
        name="averaging_ensemble",
        strategy=EnsembleStrategy.AVERAGING,
        models=models,
        averaging_type="arithmetic"
    )
    
    print("Ensembles created successfully!")
```

## â˜ï¸ í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ì—°ë™

### í´ë¼ìš°ë“œ ë°±ì—… ì‹œìŠ¤í…œ
ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ í´ë¼ìš°ë“œì— ìë™ìœ¼ë¡œ ë°±ì—…í•˜ê³  ë™ê¸°í™”í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ì—°ë™ ì‹œìŠ¤í…œ

AWS S3, Azure Blob Storage, Google Cloud Storageì™€ì˜ ì—°ë™ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import os
import boto3
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Any
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime, timedelta


@dataclass
class CloudFile:
    """í´ë¼ìš°ë“œ íŒŒì¼ ì •ë³´"""
    local_path: str
    cloud_path: str
    size: int
    last_modified: datetime
    etag: str
    metadata: Dict[str, str]


class CloudStorageInterface(ABC):
    """í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def upload_file(self, local_path: str, cloud_path: str, metadata: Optional[Dict] = None) -> bool:
        """íŒŒì¼ ì—…ë¡œë“œ"""
        pass
    
    @abstractmethod
    def download_file(self, cloud_path: str, local_path: str) -> bool:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        pass
    
    @abstractmethod
    def delete_file(self, cloud_path: str) -> bool:
        """íŒŒì¼ ì‚­ì œ"""
        pass
    
    @abstractmethod
    def list_files(self, prefix: str = "") -> List[CloudFile]:
        """íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        pass
    
    @abstractmethod
    def file_exists(self, cloud_path: str) -> bool:
        """íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        pass


class S3Storage(CloudStorageInterface):
    """AWS S3 ìŠ¤í† ë¦¬ì§€"""
    
    def __init__(self, bucket_name: str, aws_access_key: str = None, aws_secret_key: str = None, region: str = "us-east-1"):
        self.bucket_name = bucket_name
        self.region = region
        
        # AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        session = boto3.Session(
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key,
            region_name=region
        )
        self.s3_client = session.client('s3')
        self.s3_resource = session.resource('s3')
        
        # ë²„í‚· ì¡´ì¬ í™•ì¸ ë° ìƒì„±
        self._ensure_bucket_exists()
    
    def _ensure_bucket_exists(self):
        """ë²„í‚· ì¡´ì¬ í™•ì¸ ë° ìƒì„±"""
        try:
            self.s3_client.head_bucket(Bucket=self.bucket_name)
        except:
            try:
                if self.region == 'us-east-1':
                    self.s3_client.create_bucket(Bucket=self.bucket_name)
                else:
                    self.s3_client.create_bucket(
                        Bucket=self.bucket_name,
                        CreateBucketConfiguration={'LocationConstraint': self.region}
                    )
                print(f"Created S3 bucket: {self.bucket_name}")
            except Exception as e:
                print(f"Error creating bucket: {e}")
    
    def upload_file(self, local_path: str, cloud_path: str, metadata: Optional[Dict] = None) -> bool:
        """íŒŒì¼ ì—…ë¡œë“œ"""
        try:
            extra_args = {}
            if metadata:
                extra_args['Metadata'] = metadata
            
            self.s3_client.upload_file(local_path, self.bucket_name, cloud_path, ExtraArgs=extra_args)
            print(f"Uploaded {local_path} to s3://{self.bucket_name}/{cloud_path}")
            return True
        except Exception as e:
            print(f"Error uploading file: {e}")
            return False
    
    def download_file(self, cloud_path: str, local_path: str) -> bool:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            # ë¡œì»¬ ë””ë ‰í† ë¦¬ ìƒì„±
            Path(local_path).parent.mkdir(parents=True, exist_ok=True)
            
            self.s3_client.download_file(self.bucket_name, cloud_path, local_path)
            print(f"Downloaded s3://{self.bucket_name}/{cloud_path} to {local_path}")
            return True
        except Exception as e:
            print(f"Error downloading file: {e}")
            return False
    
    def delete_file(self, cloud_path: str) -> bool:
        """íŒŒì¼ ì‚­ì œ"""
        try:
            self.s3_client.delete_object(Bucket=self.bucket_name, Key=cloud_path)
            print(f"Deleted s3://{self.bucket_name}/{cloud_path}")
            return True
        except Exception as e:
            print(f"Error deleting file: {e}")
            return False
    
    def list_files(self, prefix: str = "") -> List[CloudFile]:
        """íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            response = self.s3_client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)
            files = []
            
            for obj in response.get('Contents', []):
                # ë©”íƒ€ë°ì´í„° ì¡°íšŒ
                head_response = self.s3_client.head_object(Bucket=self.bucket_name, Key=obj['Key'])
                
                cloud_file = CloudFile(
                    local_path="",  # ë¡œì»¬ ê²½ë¡œëŠ” ì•Œ ìˆ˜ ì—†ìŒ
                    cloud_path=obj['Key'],
                    size=obj['Size'],
                    last_modified=obj['LastModified'],
                    etag=obj['ETag'].strip('"'),
                    metadata=head_response.get('Metadata', {})
                )
                files.append(cloud_file)
            
            return files
        except Exception as e:
            print(f"Error listing files: {e}")
            return []
    
    def file_exists(self, cloud_path: str) -> bool:
        """íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            self.s3_client.head_object(Bucket=self.bucket_name, Key=cloud_path)
            return True
        except:
            return False


class AzureBlobStorage(CloudStorageInterface):
    """Azure Blob Storage"""
    
    def __init__(self, account_name: str, account_key: str, container_name: str):
        try:
            from azure.storage.blob import BlobServiceClient
        except ImportError:
            raise ImportError("azure-storage-blob package is required for Azure Blob Storage")
        
        self.account_name = account_name
        self.container_name = container_name
        
        # Azure í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        account_url = f"https://{account_name}.blob.core.windows.net"
        self.blob_service_client = BlobServiceClient(account_url=account_url, credential=account_key)
        
        # ì»¨í…Œì´ë„ˆ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
        self._ensure_container_exists()
    
    def _ensure_container_exists(self):
        """ì»¨í…Œì´ë„ˆ ì¡´ì¬ í™•ì¸ ë° ìƒì„±"""
        try:
            container_client = self.blob_service_client.get_container_client(self.container_name)
            container_client.get_container_properties()
        except:
            try:
                self.blob_service_client.create_container(self.container_name)
                print(f"Created Azure container: {self.container_name}")
            except Exception as e:
                print(f"Error creating container: {e}")
    
    def upload_file(self, local_path: str, cloud_path: str, metadata: Optional[Dict] = None) -> bool:
        """íŒŒì¼ ì—…ë¡œë“œ"""
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=self.container_name, 
                blob=cloud_path
            )
            
            with open(local_path, "rb") as data:
                blob_client.upload_blob(data, overwrite=True, metadata=metadata)
            
            print(f"Uploaded {local_path} to Azure Blob: {cloud_path}")
            return True
        except Exception as e:
            print(f"Error uploading file: {e}")
            return False
    
    def download_file(self, cloud_path: str, local_path: str) -> bool:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            # ë¡œì»¬ ë””ë ‰í† ë¦¬ ìƒì„±
            Path(local_path).parent.mkdir(parents=True, exist_ok=True)
            
            blob_client = self.blob_service_client.get_blob_client(
                container=self.container_name, 
                blob=cloud_path
            )
            
            with open(local_path, "wb") as download_file:
                download_file.write(blob_client.download_blob().readall())
            
            print(f"Downloaded Azure Blob {cloud_path} to {local_path}")
            return True
        except Exception as e:
            print(f"Error downloading file: {e}")
            return False
    
    def delete_file(self, cloud_path: str) -> bool:
        """íŒŒì¼ ì‚­ì œ"""
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=self.container_name, 
                blob=cloud_path
            )
            blob_client.delete_blob()
            
            print(f"Deleted Azure Blob: {cloud_path}")
            return True
        except Exception as e:
            print(f"Error deleting file: {e}")
            return False
    
    def list_files(self, prefix: str = "") -> List[CloudFile]:
        """íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            container_client = self.blob_service_client.get_container_client(self.container_name)
            blobs = container_client.list_blobs(name_starts_with=prefix)
            
            files = []
            for blob in blobs:
                cloud_file = CloudFile(
                    local_path="",
                    cloud_path=blob.name,
                    size=blob.size,
                    last_modified=blob.last_modified,
                    etag=blob.etag.strip('"'),
                    metadata=blob.metadata or {}
                )
                files.append(cloud_file)
            
            return files
        except Exception as e:
            print(f"Error listing files: {e}")
            return []
    
    def file_exists(self, cloud_path: str) -> bool:
        """íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=self.container_name, 
                blob=cloud_path
            )
            blob_client.get_blob_properties()
            return True
        except:
            return False


class BackupManager:
    """ë°±ì—… ê´€ë¦¬ì"""
    
    def __init__(self, storage: CloudStorageInterface, local_base_path: str = "."):
        self.storage = storage
        self.local_base_path = Path(local_base_path)
        self.sync_log_file = self.local_base_path / "sync_log.json"
        
        # ë™ê¸°í™” ë¡œê·¸ ë¡œë“œ
        self.sync_log = self._load_sync_log()
    
    def _load_sync_log(self) -> Dict:
        """ë™ê¸°í™” ë¡œê·¸ ë¡œë“œ"""
        if self.sync_log_file.exists():
            try:
                import json
                with open(self.sync_log_file, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {}
    
    def _save_sync_log(self):
        """ë™ê¸°í™” ë¡œê·¸ ì €ì¥"""
        import json
        with open(self.sync_log_file, 'w') as f:
            json.dump(self.sync_log, f, indent=2, default=str)
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    
    def backup_file(self, local_path: str, cloud_path: Optional[str] = None, force: bool = False) -> bool:
        """ë‹¨ì¼ íŒŒì¼ ë°±ì—…"""
        local_path = Path(local_path)
        if not local_path.exists():
            print(f"Local file not found: {local_path}")
            return False
        
        if cloud_path is None:
            cloud_path = str(local_path.relative_to(self.local_base_path))
        
        # íŒŒì¼ í•´ì‹œ ê³„ì‚°
        current_hash = self._calculate_file_hash(str(local_path))
        
        # ì´ì „ ë™ê¸°í™” ì •ë³´ í™•ì¸
        sync_key = str(local_path)
        last_sync = self.sync_log.get(sync_key, {})
        
        if not force and last_sync.get('hash') == current_hash:
            print(f"File unchanged, skipping: {local_path}")
            return True
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            'local_path': str(local_path),
            'upload_time': datetime.now().isoformat(),
            'file_hash': current_hash,
            'file_size': str(local_path.stat().st_size)
        }
        
        # ì—…ë¡œë“œ ì‹¤í–‰
        success = self.storage.upload_file(str(local_path), cloud_path, metadata)
        
        if success:
            # ë™ê¸°í™” ë¡œê·¸ ì—…ë°ì´íŠ¸
            self.sync_log[sync_key] = {
                'cloud_path': cloud_path,
                'hash': current_hash,
                'last_sync': datetime.now().isoformat()
            }
            self._save_sync_log()
        
        return success
    
    def backup_directory(self, local_dir: str, cloud_prefix: str = "", exclude_patterns: List[str] = None) -> Dict[str, bool]:
        """ë””ë ‰í† ë¦¬ ë°±ì—…"""
        local_dir = Path(local_dir)
        exclude_patterns = exclude_patterns or ['*.tmp', '*.log', '__pycache__']
        
        results = {}
        
        for file_path in local_dir.rglob("*"):
            if file_path.is_file():
                # ì œì™¸ íŒ¨í„´ í™•ì¸
                skip = False
                for pattern in exclude_patterns:
                    if file_path.match(pattern):
                        skip = True
                        break
                
                if skip:
                    continue
                
                # í´ë¼ìš°ë“œ ê²½ë¡œ ìƒì„±
                relative_path = file_path.relative_to(local_dir)
                cloud_path = f"{cloud_prefix}/{relative_path}" if cloud_prefix else str(relative_path)
                cloud_path = cloud_path.replace("\\", "/")  # Windows ê²½ë¡œ ì²˜ë¦¬
                
                # ë°±ì—… ì‹¤í–‰
                success = self.backup_file(str(file_path), cloud_path)
                results[str(file_path)] = success
        
        return results
    
    def restore_file(self, cloud_path: str, local_path: Optional[str] = None) -> bool:
        """ë‹¨ì¼ íŒŒì¼ ë³µì›"""
        if local_path is None:
            local_path = self.local_base_path / cloud_path
        
        return self.storage.download_file(cloud_path, str(local_path))
    
    def sync_directory(self, local_dir: str, cloud_prefix: str = "", bidirectional: bool = False) -> Dict[str, Any]:
        """ë””ë ‰í† ë¦¬ ë™ê¸°í™”"""
        results = {
            'uploaded': [],
            'downloaded': [],
            'conflicts': [],
            'errors': []
        }
        
        local_dir = Path(local_dir)
        
        # ë¡œì»¬ íŒŒì¼ ëª©ë¡
        local_files = {}
        for file_path in local_dir.rglob("*"):
            if file_path.is_file():
                relative_path = file_path.relative_to(local_dir)
                cloud_path = f"{cloud_prefix}/{relative_path}" if cloud_prefix else str(relative_path)
                cloud_path = cloud_path.replace("\\", "/")
                local_files[cloud_path] = file_path
        
        # í´ë¼ìš°ë“œ íŒŒì¼ ëª©ë¡
        cloud_files = {cf.cloud_path: cf for cf in self.storage.list_files(cloud_prefix)}
        
        # ì—…ë¡œë“œí•  íŒŒì¼ (ë¡œì»¬ì—ë§Œ ìˆê±°ë‚˜ ë” ìƒˆë¡œìš´ íŒŒì¼)
        for cloud_path, local_path in local_files.items():
            if cloud_path not in cloud_files:
                # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ
                if self.backup_file(str(local_path), cloud_path):
                    results['uploaded'].append(cloud_path)
                else:
                    results['errors'].append(f"Failed to upload {cloud_path}")
            else:
                # íŒŒì¼ ë¹„êµ ë° ì—…ë°ì´íŠ¸
                cloud_file = cloud_files[cloud_path]
                local_mtime = datetime.fromtimestamp(local_path.stat().st_mtime)
                
                if local_mtime > cloud_file.last_modified:
                    if self.backup_file(str(local_path), cloud_path):
                        results['uploaded'].append(cloud_path)
                    else:
                        results['errors'].append(f"Failed to upload {cloud_path}")
        
        # ì–‘ë°©í–¥ ë™ê¸°í™”ì¸ ê²½ìš° ë‹¤ìš´ë¡œë“œ
        if bidirectional:
            for cloud_path, cloud_file in cloud_files.items():
                if cloud_path not in local_files:
                    # ìƒˆ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                    local_path = local_dir / cloud_path
                    if self.restore_file(cloud_path, str(local_path)):
                        results['downloaded'].append(cloud_path)
                    else:
                        results['errors'].append(f"Failed to download {cloud_path}")
        
        return results
    
    def cleanup_old_backups(self, days_to_keep: int = 30) -> List[str]:
        """ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        deleted_files = []
        
        cloud_files = self.storage.list_files()
        
        for cloud_file in cloud_files:
            if cloud_file.last_modified < cutoff_date:
                if self.storage.delete_file(cloud_file.cloud_path):
                    deleted_files.append(cloud_file.cloud_path)
        
        return deleted_files


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # S3 ìŠ¤í† ë¦¬ì§€ ì„¤ì •
    s3_storage = S3Storage(
        bucket_name="my-vision-system-backup",
        aws_access_key="your-access-key",
        aws_secret_key="your-secret-key",
        region="us-west-2"
    )
    
    # ë°±ì—… ê´€ë¦¬ì ìƒì„±
    backup_manager = BackupManager(s3_storage, local_base_path=".")
    
    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ë°±ì—…
    results = backup_manager.backup_directory(
        local_dir="models",
        cloud_prefix="vision-system/models",
        exclude_patterns=['*.tmp', '*.log']
    )
    
    print(f"Backup completed. Results: {results}")
    
    # ë””ë ‰í† ë¦¬ ë™ê¸°í™”
    sync_results = backup_manager.sync_directory(
        local_dir="models",
        cloud_prefix="vision-system/models",
        bidirectional=True
    )
    
    print(f"Sync completed. Results: {sync_results}")
```


