---
description: 
globs: 
alwaysApply: true
---
# ê³µí†µ ê°œë°œ ê·œì¹™ (Common Development Rules)

ì´ ê·œì¹™ì€ í”„ë¡œì íŠ¸ ìœ í˜•ì— ê´€ê³„ì—†ì´ ëª¨ë“  Python í”„ë¡œì íŠ¸ì— ì ìš©í•  ìˆ˜ ìˆëŠ” ì¼ë°˜ì ì¸ ê°œë°œ í‘œì¤€ì…ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ ë„¤ì´ë° ê·œì¹™

### Python íŒŒì¼ ë„¤ì´ë° ì»¨ë²¤ì…˜
- **ì‹¤í–‰ íŒŒì¼**: `run_*.py` (ì˜ˆ: `run_main_process.py`)
- **í…ŒìŠ¤íŠ¸ íŒŒì¼**: `test_*.py` (ì˜ˆ: `test_main_module.py`)
- **ì¸í„°í˜ì´ìŠ¤ íŒŒì¼**: `*_interface.py` (ì˜ˆ: `api_interface.py`)
- **ì„¤ì • íŒŒì¼**: `*_config.py` (ì˜ˆ: `database_config.py`)
- **ìœ í‹¸ë¦¬í‹° íŒŒì¼**: `*_utils.py` (ì˜ˆ: `string_utils.py`)
- **ëª¨ë¸ íŒŒì¼**: `*_model.py` (ì˜ˆ: `data_model.py`)

### ë³€ìˆ˜ ë° í•¨ìˆ˜ ë„¤ì´ë°
- **ë³€ìˆ˜/í•¨ìˆ˜**: snake_case (ì˜ˆ: `process_data`, `user_count`)
- **í´ë˜ìŠ¤**: PascalCase (ì˜ˆ: `DataProcessor`, `UserManager`)
- **ìƒìˆ˜**: UPPER_SNAKE_CASE (ì˜ˆ: `MAX_RETRY_COUNT`, `DEFAULT_TIMEOUT`)
- **ëª¨ë“ˆ**: snake_case (ì˜ˆ: `data_processing`, `user_management`)
- **íŒ¨í‚¤ì§€**: lowercase (ì˜ˆ: `utils`, `models`)

## ğŸ Python ì½”ë”© í‘œì¤€

### íŒŒì¼ í—¤ë” (ëª¨ë“  .py íŒŒì¼ í•„ìˆ˜)
```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ëª¨ë“ˆ ì„¤ëª….

ì´ ëª¨ë“ˆì€ [ê¸°ëŠ¥ ì„¤ëª…]ì„ ì œê³µí•©ë‹ˆë‹¤.

Example:
    ê¸°ë³¸ì ì¸ ì‚¬ìš© ì˜ˆì‹œë¥¼ ì—¬ê¸°ì— ì‘ì„±í•©ë‹ˆë‹¤.
    
    $ python run_main_process.py --input data.csv
"""

import os
import sys
import logging
from typing import Dict, List, Optional, Tuple, Union
```

### Import ìˆœì„œ (ì—„ê²©íˆ ì¤€ìˆ˜)
1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
2. ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬  
3. ë¡œì»¬ ì• í”Œë¦¬ì¼€ì´ì…˜/ë¼ì´ë¸ŒëŸ¬ë¦¬
4. ìƒëŒ€ import (ê°™ì€ í´ë” ë‚´)

```python
# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import sys
import logging
from pathlib import Path

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import pandas as pd
import yaml

# ë¡œì»¬ ì• í”Œë¦¬ì¼€ì´ì…˜
from common.logging import get_logger
from common.config import load_config
from core.base import BaseProcessor

# ìƒëŒ€ import
from .utils import preprocess_data
from .models import DataModel
```

### Type Hints í•„ìˆ˜ ì‚¬ìš©
```python
def process_data(
    data: List[Dict[str, Union[str, int]]],
    config: Dict[str, Any],
    output_path: Optional[str] = None
) -> Tuple[bool, str]:
    """ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜"""
    pass
```

### Docstring í•„ìˆ˜ ì‘ì„± (Google Style)
```python
def process_data(data: List[Dict], config: Dict) -> Tuple[bool, str]:
    """ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜.
    
    Args:
        data: ì²˜ë¦¬í•  ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        (ì„±ê³µ ì—¬ë¶€, ê²°ê³¼ ë©”ì‹œì§€) íŠœí”Œ
        
    Raises:
        ValueError: ì…ë ¥ ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
        
    Example:
        >>> data = [{'id': 1, 'value': 'test'}]
        >>> config = {'mode': 'strict'}
        >>> success, message = process_data(data, config)
    """
    pass
```

### ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
```python
import logging
logger = logging.getLogger(__name__)

def safe_process():
    """ì•ˆì „í•œ í•¨ìˆ˜ ì‹¤í–‰ íŒ¨í„´"""
    try:
        # ë©”ì¸ ë¡œì§
        result = process_data()
        logger.info("Processing completed successfully")
        return result
    except Exception as e:
        logger.error(f"Error in {__name__}: {str(e)}")
        raise
```

## ğŸ”§ ê°œë°œ ë„êµ¬ ìë™í™” ê·œì¹™

### ì½”ë”© ë„êµ¬ ì„¤ì • ë° ì„¤ì¹˜
```python
# scripts/setup_coding_tools.py ì˜ˆì‹œ
REQUIRED_TOOLS = {
    'black': {
        'version': '23.3.0',
        'purpose': 'Python ì½”ë“œ ìë™ í¬ë§·íŒ…',
        'config_file': 'pyproject.toml',
        'installation': 'pip install black==23.3.0'
    },
    'isort': {
        'version': '5.12.0',
        'purpose': 'Python import ë¬¸ ìë™ ì •ë ¬',
        'config_file': 'pyproject.toml',
        'installation': 'pip install isort==5.12.0'
    },
    'flake8': {
        'version': '6.0.0',
        'purpose': 'Python ì½”ë“œ ë¦°íŒ…',
        'config_file': '.flake8',
        'installation': 'pip install flake8==6.0.0'
    },
    'pylint': {
        'version': '2.17.0',
        'purpose': 'Python ì½”ë“œ ì •ì  ë¶„ì„',
        'config_file': '.pylintrc',
        'installation': 'pip install pylint==2.17.0'
    },
    'mypy': {
        'version': '1.3.0',
        'purpose': 'Python íƒ€ì… ì²´í‚¹',
        'config_file': 'pyproject.toml',
        'installation': 'pip install mypy==1.3.0'
    }
}

def setup_development_tools():
    """ê°œë°œ ë„êµ¬ ìë™ ì„¤ì¹˜ ë° ì„¤ì •"""
    for tool_name, tool_config in REQUIRED_TOOLS.items():
        install_tool(tool_name, tool_config)
        generate_config_file(tool_name, tool_config)
```

### Git í›… ìë™ ì„¤ì •
```python
# scripts/setup_git_hooks.py ì˜ˆì‹œ
GIT_HOOKS = {
    'pre-commit': {
        'checks': ['validate_rules', 'code_style_check', 'docstring_check'],
        'path': 'scripts/git_hooks/pre-commit'
    },
    'pre-push': {
        'checks': ['validate_rules', 'security_check'],
        'path': 'scripts/git_hooks/pre-push'
    },
    'commit-msg': {
        'checks': ['check_commit_message'],
        'path': 'scripts/git_hooks/commit-msg'
    }
}

def setup_git_hooks():
    """Git í›… ìë™ ì„¤ì¹˜"""
    for hook_name, hook_config in GIT_HOOKS.items():
        install_git_hook(hook_name, hook_config)
```

### ì½”ë“œ ìŠ¤íƒ€ì¼ ìë™ ê²€ì‚¬
```python
# scripts/check_code_style.py ì˜ˆì‹œ
def check_code_style(files=None, auto_fix=False):
    """
    ì½”ë“œ ìŠ¤íƒ€ì¼ ê²€ì‚¬ ìŠ¤í¬ë¦½íŠ¸
    
    Args:
        files: ê²€ì‚¬í•  íŒŒì¼ ëª©ë¡ (Noneì´ë©´ ì „ì²´)
        auto_fix: ìë™ ìˆ˜ì • ì—¬ë¶€
    
    Returns:
        ê²€ì‚¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    results = {}
    
    # Black ì½”ë“œ í¬ë§·íŒ… ê²€ì‚¬
    if auto_fix:
        subprocess.run(['black', '.'], check=True)
    else:
        result = subprocess.run(['black', '--check', '.'], capture_output=True)
        results['black'] = {'passed': result.returncode == 0, 'output': result.stdout.decode()}
    
    # isort import ì •ë ¬ ê²€ì‚¬
    if auto_fix:
        subprocess.run(['isort', '.'], check=True)
    else:
        result = subprocess.run(['isort', '--check-only', '.'], capture_output=True)
        results['isort'] = {'passed': result.returncode == 0, 'output': result.stdout.decode()}
    
    # flake8 ë¦°íŒ… ê²€ì‚¬
    result = subprocess.run(['flake8', '.'], capture_output=True)
    results['flake8'] = {'passed': result.returncode == 0, 'output': result.stdout.decode()}
    
    return results
```

## ğŸ“Š ë°ì´í„°ì…‹ ê´€ë¦¬ ê·œì¹™

### ë°ì´í„°ì…‹ êµ¬ì¡° í‘œì¤€í™”
```python
# í‘œì¤€ ë°ì´í„°ì…‹ êµ¬ì¡°
DATASET_STRUCTURE = {
    'raw': 'ì›ë³¸ ë°ì´í„° (ìˆ˜ì • ê¸ˆì§€)',
    'processed': {
        'aligned': 'ì •ë ¬ëœ ì´ë¯¸ì§€',
        'resized': 'í¬ê¸° ì¡°ì •ëœ ì´ë¯¸ì§€',
        'ready': 'í•™ìŠµ ì¤€ë¹„ ì™„ë£Œëœ ì´ë¯¸ì§€'
    },
    'augmented': {
        'flip': 'ì¢Œìš° ë°˜ì „ëœ ì´ë¯¸ì§€',
        'bright': 'ë°ê¸° ì¡°ì •ëœ ì´ë¯¸ì§€',
        'rot': 'íšŒì „ëœ ì´ë¯¸ì§€',
        'zoom': 'í™•ëŒ€/ì¶•ì†Œëœ ì´ë¯¸ì§€'
    },
    'annotations': {
        'bounding_box': 'ë°”ìš´ë”© ë°•ìŠ¤ ì£¼ì„',
        'landmarks': 'ëœë“œë§ˆí¬ ì£¼ì„',
        'segmentation': 'ì„¸ê·¸ë©˜í…Œì´ì…˜ ì£¼ì„'
    },
    'metadata': {
        'class_map.json': 'í´ë˜ìŠ¤ ID-ì´ë¦„ ë§¤í•‘',
        'train.txt': 'í•™ìŠµ ë°ì´í„°ì…‹ íŒŒì¼ ëª©ë¡',
        'val.txt': 'ê²€ì¦ ë°ì´í„°ì…‹ íŒŒì¼ ëª©ë¡',
        'test.txt': 'í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ íŒŒì¼ ëª©ë¡',
        'stats.json': 'ë°ì´í„°ì…‹ í†µê³„ ì •ë³´'
    }
}

# ë°ì´í„°ì…‹ íŒŒì¼ ë„¤ì´ë° ê·œì¹™
DATASET_NAMING_RULES = {
    'pattern': '{domain}_{class}_{date}_{info}_{index}.{ext}',
    'date_format': '%Y%m%d',
    'augmented_pattern': '{original_name}_{augment_type}[_{param}].{ext}',
    'examples': [
        'face_happy_20240624_indoor_001.jpg',
        'object_detection_person_20240624_outdoor_001.jpg',
        'face_happy_20240624_indoor_001_flip.jpg',
        'face_happy_20240624_indoor_001_bright_120.jpg'
    ]
}
```

### ëª¨ë“ˆ êµ¬ì¡° í‘œì¤€í™”
```python
# í‘œì¤€ ëª¨ë“ˆ êµ¬ì¡°
MODULE_STRUCTURE = {
    'required_files': [
        '{module_name}_pipeline.py',    # í•µì‹¬ íŒŒì´í”„ë¼ì¸
        '{module_name}_interface.py',   # ì™¸ë¶€ ì—°ë™ ì¸í„°í˜ì´ìŠ¤
        'run_{module_name}.py',         # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
        'README.md'                     # ë¬¸ì„œ
    ],
    'optional_files': [
        '{module_name}_config.yaml',    # ì„¤ì • íŒŒì¼
        'STRUCTURE.md',                 # êµ¬ì¡° ì„¤ëª…
        'CHANGELOG.md'                  # ë³€ê²½ ì´ë ¥
    ],
    'optional_dirs': [
        'processors/',                  # ì²˜ë¦¬ê¸°
        'utils/',                      # ìœ í‹¸ë¦¬í‹°
        'tests/'                       # í…ŒìŠ¤íŠ¸
    ]
}

# ëª¨ë“ˆ ê°œë°œ ê°€ì´ë“œë¼ì¸
MODULE_GUIDELINES = {
    'minimum_features': 2,  # ìµœì†Œ 2ê°œ ì´ìƒì˜ features ê²°í•©
    'independence': True,   # ë…ë¦½ì  ì‚¬ìš© ê°€ëŠ¥
    'interface_clarity': True,  # ëª…í™•í•œ ì¸í„°í˜ì´ìŠ¤ ì •ì˜
    'documentation_required': ['README.md', 'docstrings'],
    'testing_required': ['integration_tests']
}
```

## ğŸ”§ ìŠ¤í¬ë¦½íŠ¸ ê´€ë¦¬ ê·œì¹™

### ìŠ¤í¬ë¦½íŠ¸ ë¶„ë¥˜ ë° ë„¤ì´ë°
```python
# ìŠ¤í¬ë¦½íŠ¸ ì¹´í…Œê³ ë¦¬ë³„ ë„¤ì´ë° ê·œì¹™
SCRIPT_CATEGORIES = {
    'setup': {
        'prefix': 'setup_',
        'purpose': 'ê°œë°œ í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”',
        'location': 'scripts/',
        'examples': ['setup_coding_tools.py', 'setup_git_hooks.py']
    },
    'validate': {
        'prefix': 'validate_',
        'purpose': 'ë°ì´í„° ë° ì„¤ì • ê²€ì¦',
        'location': 'scripts/validation/',
        'examples': ['validate_structure.py', 'validate_config.py']
    },
    'download': {
        'prefix': 'download_',
        'purpose': 'ëª¨ë¸ ë° ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ',
        'location': 'scripts/model_management/',
        'examples': ['download_models.py', 'download_opencv_cascades.py']
    },
    'cleanup': {
        'prefix': 'cleanup_',
        'purpose': 'ì •ë¦¬ ë° ìµœì í™”',
        'location': 'scripts/',
        'examples': ['cleanup_models.py', 'cleanup_temp_files.py']
    }
}

def validate_script_naming(script_path):
    """ìŠ¤í¬ë¦½íŠ¸ ë„¤ì´ë° ê·œì¹™ ê²€ì¦"""
    script_name = os.path.basename(script_path)
    
    for category, config in SCRIPT_CATEGORIES.items():
        if script_name.startswith(config['prefix']):
            expected_location = config['location']
            actual_location = os.path.dirname(script_path) + '/'
            
            if not actual_location.endswith(expected_location):
                return {
                    'valid': False,
                    'message': f"Script {script_name} should be in {expected_location}"
                }
    
    return {'valid': True}
```

### ì½”ë“œ ìŠ¹ê²© ê·œì¹™
```python
# ì½”ë“œ ìŠ¹ê²© ê¸°ì¤€
CODE_PROMOTION_CRITERIA = {
    'reusability': {
        'description': 'ì—¬ëŸ¬ ëª¨ë“ˆì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥ì„±',
        'weight': 0.3,
        'threshold': 0.7
    },
    'stability': {
        'description': 'ì½”ë“œì˜ ì•ˆì •ì„± ë° í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€',
        'weight': 0.25,
        'threshold': 0.8
    },
    'generality': {
        'description': 'ì½”ë“œì˜ ë²”ìš©ì„±',
        'weight': 0.25,
        'threshold': 0.7
    },
    'duplication': {
        'description': 'ì½”ë“œ ì¤‘ë³µ ì •ë„',
        'weight': 0.2,
        'threshold': 0.6
    }
}

def analyze_promotion_candidate(file_path):
    """ì½”ë“œ ìŠ¹ê²© í›„ë³´ ë¶„ì„"""
    scores = {}
    
    # ì¬ì‚¬ìš©ì„± ë¶„ì„
    scores['reusability'] = analyze_reusability(file_path)
    
    # ì•ˆì •ì„± ë¶„ì„
    scores['stability'] = analyze_stability(file_path)
    
    # ë²”ìš©ì„± ë¶„ì„
    scores['generality'] = analyze_generality(file_path)
    
    # ì¤‘ë³µ ë¶„ì„
    scores['duplication'] = analyze_duplication(file_path)
    
    # ì´ì  ê³„ì‚°
    total_score = sum(
        scores[criterion] * config['weight']
        for criterion, config in CODE_PROMOTION_CRITERIA.items()
    )
    
    # ìŠ¹ê²© ìê²© íŒì •
    eligible = all(
        scores[criterion] >= config['threshold']
        for criterion, config in CODE_PROMOTION_CRITERIA.items()
    )
    
    return {
        'file_path': file_path,
        'total_score': total_score,
        'scores': scores,
        'eligible': eligible,
        'recommendation': 'promote' if eligible and total_score >= 0.7 else 'keep'
    }
```

## ğŸ¯ í’ˆì§ˆ ë³´ì¦ ê·œì¹™

### ìë™í™”ëœ í’ˆì§ˆ ê²€ì‚¬
```python
# í’ˆì§ˆ ê²€ì‚¬ í•­ëª©
QUALITY_CHECKS = {
    'code_style': {
        'tools': ['black', 'isort', 'flake8'],
        'auto_fix': ['black', 'isort'],
        'manual_fix': ['flake8']
    },
    'type_checking': {
        'tools': ['mypy'],
        'auto_fix': [],
        'manual_fix': ['mypy']
    },
    'security': {
        'tools': ['bandit', 'safety'],
        'auto_fix': [],
        'manual_fix': ['bandit', 'safety']
    },
    'documentation': {
        'tools': ['pydocstyle'],
        'auto_fix': [],
        'manual_fix': ['pydocstyle']
    }
}

def run_quality_checks(check_types=None, auto_fix=False):
    """í’ˆì§ˆ ê²€ì‚¬ ì‹¤í–‰"""
    if check_types is None:
        check_types = list(QUALITY_CHECKS.keys())
    
    results = {}
    
    for check_type in check_types:
        if check_type not in QUALITY_CHECKS:
            results[check_type] = {'status': 'error', 'message': f'Unknown check: {check_type}'}
            continue
        
        check_config = QUALITY_CHECKS[check_type]
        check_results = {}
        
        for tool in check_config['tools']:
            if auto_fix and tool in check_config['auto_fix']:
                # ìë™ ìˆ˜ì • ì‹¤í–‰
                result = run_tool_with_fix(tool)
            else:
                # ê²€ì‚¬ë§Œ ì‹¤í–‰
                result = run_tool_check(tool)
            
            check_results[tool] = result
        
        results[check_type] = {
            'status': 'success' if all(r['passed'] for r in check_results.values()) else 'failed',
            'tools': check_results
        }
    
    return results
```

ì´ëŸ¬í•œ ê·œì¹™ë“¤ì€ í”„ë¡œì íŠ¸ì˜ ì¼ê´€ì„±ê³¼ í’ˆì§ˆì„ ë³´ì¥í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.
        files: ê²€ì‚¬í•  íŒŒì¼ ëª©ë¡ (Noneì´ë©´ ì „ì²´)
        auto_fix: ìë™ ìˆ˜ì • ì—¬ë¶€
    
    Returns:
        ê²€ì‚¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    results = {
        'black': run_black_check(files, auto_fix),
        'isort': run_isort_check(files, auto_fix),
        'flake8': run_flake8_check(files),
        'pylint': run_pylint_check(files),
        'mypy': run_mypy_check(files)
    }
    
    # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
    generate_style_report(results)
    
    return results
```

### ê·œì¹™ ê²€ì¦ ìë™í™”
```python
# scripts/validate_rules.py ì˜ˆì‹œ
def validate_project_rules():
    """í”„ë¡œì íŠ¸ ê·œì¹™ ì¤€ìˆ˜ ê²€ì¦"""
    validations = {
        'file_naming': validate_file_naming_rules(),
        'import_order': validate_import_order(),
        'docstring': validate_docstring_rules(),
        'type_hints': validate_type_hints(),
        'structure': validate_project_structure()
    }
    
    # CI/CDì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ exit code ì„¤ì •
    if all(validations.values()):
        print("âœ“ ëª¨ë“  ê·œì¹™ ê²€ì¦ í†µê³¼")
        return 0
    else:
        print("âœ— ê·œì¹™ ìœ„ë°˜ ë°œê²¬")
        return 1
```

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡° í‘œì¤€

### ìµœì†Œ í•„ìˆ˜ í´ë” êµ¬ì¡°
```
project_root/
â”œâ”€â”€ src/                    # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ models/
â”œâ”€â”€ datasets/               # ğŸ¯ í•™ìŠµ ì „ìš© ë°ì´í„° (ML ë°ì´í„°ì…‹)
â”‚   â”œâ”€â”€ raw/                # ì›ë³¸ ë°ì´í„°
â”‚   â”œâ”€â”€ processed/          # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â”œâ”€â”€ annotations/        # ë¼ë²¨ë§ ë°ì´í„°
â”‚   â””â”€â”€ splits/             # train/val/test ë¶„í• 
â”œâ”€â”€ data/                   # ğŸ¯ ëŸ°íƒ€ì„ ì „ìš© ë°ì´í„°
â”‚   â”œâ”€â”€ temp/               # ì„ì‹œ íŒŒì¼ (ìë™ ì •ë¦¬)
â”‚   â”œâ”€â”€ logs/               # ë¡œê·¸ íŒŒì¼
â”‚   â””â”€â”€ output/             # ê²°ê³¼ë¬¼ ì €ì¥
â”œâ”€â”€ tests/                  # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ docs/                   # ë¬¸ì„œ
â”œâ”€â”€ config/                 # ì„¤ì • íŒŒì¼
â”œâ”€â”€ models/                 # ëª¨ë¸ ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ weights/            # ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â”œâ”€â”€ metadata/           # ëª¨ë¸ ë©”íƒ€ë°ì´í„°
â”‚   â””â”€â”€ configs/            # ëª¨ë¸ ì„¤ì •
â”œâ”€â”€ scripts/                # ê°œë°œ ë„êµ¬ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ setup_coding_tools.py
â”‚   â”œâ”€â”€ setup_git_hooks.py
â”‚   â”œâ”€â”€ check_code_style.py
â”‚   â””â”€â”€ validate_rules.py
â”œâ”€â”€ requirements.txt        # ì˜ì¡´ì„±
â”œâ”€â”€ pyproject.toml         # í”„ë¡œì íŠ¸ ì„¤ì •
â”œâ”€â”€ .flake8               # Flake8 ì„¤ì •
â”œâ”€â”€ .pylintrc             # Pylint ì„¤ì •
â”œâ”€â”€ README.md              # í”„ë¡œì íŠ¸ ì„¤ëª…
â””â”€â”€ .gitignore             # Git ë¬´ì‹œ íŒŒì¼
```

## âš ï¸ **ì¤‘ìš”í•œ íŒŒì¼ ê´€ë¦¬ ê·œì¹™**

### **ğŸš« ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­**
```python
# âŒ ì ˆëŒ€ ê¸ˆì§€: ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì— ì„ì‹œ íŒŒì¼, ë¡œê·¸ íŒŒì¼ ìƒì„±
FORBIDDEN_ROOT_FILES = [
    "*.log",           # ë¡œê·¸ íŒŒì¼ â†’ data/logs/ ì— ì €ì¥
    "*.tmp",           # ì„ì‹œ íŒŒì¼ â†’ data/temp/ ì— ì €ì¥
    "output_*",        # ê²°ê³¼ë¬¼ â†’ data/output/ ì— ì €ì¥
    "temp_*",          # ì„ì‹œ íŒŒì¼ â†’ data/temp/ ì— ì €ì¥
    "cache_*",         # ìºì‹œ íŒŒì¼ â†’ data/temp/ ì— ì €ì¥
    "debug_*",         # ë””ë²„ê·¸ íŒŒì¼ â†’ data/logs/ ì— ì €ì¥
    "result_*",        # ê²°ê³¼ íŒŒì¼ â†’ data/output/ ì— ì €ì¥
    "test_output_*",   # í…ŒìŠ¤íŠ¸ ê²°ê³¼ â†’ data/output/ ì— ì €ì¥
]

# âœ… ì˜¬ë°”ë¥¸ íŒŒì¼ ì €ì¥ ìœ„ì¹˜
CORRECT_FILE_LOCATIONS = {
    'logs': 'data/logs/',           # ëª¨ë“  ë¡œê·¸ íŒŒì¼
    'temp': 'data/temp/',           # ëª¨ë“  ì„ì‹œ íŒŒì¼
    'output': 'data/output/',       # ëª¨ë“  ê²°ê³¼ë¬¼
    'models': 'models/weights/',    # ëª¨ë¸ ê°€ì¤‘ì¹˜
    'datasets': 'datasets/',        # í•™ìŠµ ë°ì´í„°
    'config': 'config/',            # ì„¤ì • íŒŒì¼
}
```

### **ğŸ“‚ ë°ì´í„° í´ë” êµ¬ë¶„ ì›ì¹™**
```python
# ğŸ¯ datasets/ = í•™ìŠµ ì „ìš© (ML ë°ì´í„°ì…‹)
DATASETS_PURPOSE = {
    'raw/': 'ì›ë³¸ í•™ìŠµ ë°ì´í„° (ì´ë¯¸ì§€, ë¹„ë””ì˜¤, í…ìŠ¤íŠ¸)',
    'processed/': 'ì „ì²˜ë¦¬ëœ í•™ìŠµ ë°ì´í„°',
    'annotations/': 'ë¼ë²¨ë§ ë°ì´í„° (YOLO, COCO, JSON, CSV)',
    'splits/': 'train/validation/test ë¶„í•  ì •ë³´'
}

# ğŸ¯ data/ = ëŸ°íƒ€ì„ ì „ìš© (ì‹¤í–‰ ì¤‘ ìƒì„±ë˜ëŠ” ë°ì´í„°)
DATA_PURPOSE = {
    'temp/': 'ì„ì‹œ íŒŒì¼ (ìë™ ì •ë¦¬, 24ì‹œê°„ ë³´ê´€)',
    'logs/': 'ì‹œìŠ¤í…œ ë¡œê·¸, ì—ëŸ¬ ë¡œê·¸ (30ì¼ ë³´ê´€)',
    'output/': 'ì¶”ë¡  ê²°ê³¼, ì²˜ë¦¬ ê²°ê³¼ë¬¼ (ì‚¬ìš©ì ì •ì˜ ë³´ê´€)'
}

# ğŸ¯ models/ = ëª¨ë¸ ì €ì¥ì†Œ
MODELS_PURPOSE = {
    'weights/': 'í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ (.pt, .onnx, .h5)',
    'metadata/': 'ëª¨ë¸ ë©”íƒ€ë°ì´í„° (ì„±ëŠ¥, ë²„ì „ ì •ë³´)',
    'configs/': 'ëª¨ë¸ ì„¤ì • íŒŒì¼ (.yaml, .json)'
}
```

### **íŒŒì¼ ë„¤ì´ë° ë° ì €ì¥ ê·œì¹™**
```python
# íŒŒì¼ ë„¤ì´ë° ê·œì¹™
FILE_NAMING_RULES = {
    'logs': 'data/logs/{component}_{date}.log',
    'temp': 'data/temp/temp_{purpose}_{timestamp}.{ext}',
    'output': 'data/output/{purpose}_{date}.{ext}',
    'models': 'models/weights/{task}_{architecture}_{dataset}_{date}.{ext}',
    'datasets': 'datasets/{domain}/{category}/{filename}.{ext}',
}

# ì˜ˆì‹œ
EXAMPLES = {
    'training_log': 'data/logs/face_recognition_20250628.log',
    'temp_image': 'data/temp/temp_aligned_face_20250628_143022.jpg',
    'inference_result': 'data/output/recognition_results_20250628.json',
    'model_weight': 'models/weights/face_detection_yolov8n_wider_20250628.pt',
    'dataset_image': 'datasets/face_recognition/raw/person_001/face_001.jpg',
}
```

## ğŸ§ª ë¹„ì „ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì „ëµ ê·œì¹™

### ì‹¤ì‹œê°„ ë¹„ì „ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
```python
import pytest
import time
import threading
import numpy as np
from unittest.mock import Mock, patch
from typing import List, Dict, Tuple

class VisionSystemTestFramework:
    """ë¹„ì „ ì‹œìŠ¤í…œ ì „ìš© í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""
    
    def __init__(self):
        self.test_data_path = "tests/data"
        self.benchmark_thresholds = {
            'fps_min': 15.0,
            'latency_max_ms': 100.0,
            'memory_max_mb': 512.0,
            'accuracy_min': 0.85
        }
    
    def setup_test_environment(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
        # í…ŒìŠ¤íŠ¸ìš© ê°€ì§œ ì¹´ë©”ë¼ ì„¤ì •
        self.mock_camera = self._create_mock_camera()
        
        # í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ ë¡œë”©
        self.test_model = self._load_test_model()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •
        self.performance_monitor = PerformanceMonitor()

class RealTimeProcessingTest:
    """ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def sample_frames(self):
        """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ í”„ë ˆì„ ìƒì„±"""
        frames = []
        for i in range(100):
            # ë‹¤ì–‘í•œ í•´ìƒë„ì˜ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ ìƒì„±
            frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            frames.append(frame)
        return frames
    
    @pytest.fixture
    def real_time_processor(self):
        """ì‹¤ì‹œê°„ ì²˜ë¦¬ê¸° í”½ìŠ¤ì²˜"""
        mock_model = Mock()
        processor = RealTimeProcessor(mock_model, device='cpu')
        yield processor
        processor.stop()
    
    def test_frame_drop_handling(self, real_time_processor, sample_frames):
        """í”„ë ˆì„ ë“œë¡­ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        dropped_frames = 0
        processed_frames = 0
        
        # ë²„í¼ í¬ê¸°ë³´ë‹¤ ë§ì€ í”„ë ˆì„ ë¹ ë¥´ê²Œ íˆ¬ì…
        for frame in sample_frames:
            success = real_time_processor.frame_buffer.put_frame(frame)
            if not success:
                dropped_frames += 1
            else:
                processed_frames += 1
        
        # í”„ë ˆì„ ë“œë¡­ì´ ì ì ˆíˆ ë°œìƒí–ˆëŠ”ì§€ í™•ì¸
        assert dropped_frames > 0, "í”„ë ˆì„ ë“œë¡­ì´ ë°œìƒí•˜ì§€ ì•ŠìŒ"
        assert processed_frames > 0, "ì²˜ë¦¬ëœ í”„ë ˆì„ì´ ì—†ìŒ"
        
        # ë“œë¡­ë¥ ì´ ì„ê³„ê°’ ì´í•˜ì¸ì§€ í™•ì¸
        drop_rate = dropped_frames / len(sample_frames)
        assert drop_rate < 0.3, f"í”„ë ˆì„ ë“œë¡­ë¥ ì´ ë„ˆë¬´ ë†’ìŒ: {drop_rate:.2%}"
    
    def test_latency_measurement(self, real_time_processor, sample_frames):
        """ì§€ì—°ì‹œê°„ ì¸¡ì • í…ŒìŠ¤íŠ¸"""
        latencies = []
        
        for frame in sample_frames[:10]:  # 10ê°œ í”„ë ˆì„ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            start_time = time.time()
            
            # í”„ë ˆì„ ì²˜ë¦¬
            real_time_processor.frame_buffer.put_frame(frame)
            result = real_time_processor.frame_buffer.get_frame()
            
            if result:
                latency = (time.time() - start_time) * 1000  # ms ë‹¨ìœ„
                latencies.append(latency)
        
        # í‰ê·  ì§€ì—°ì‹œê°„ ê²€ì¦
        avg_latency = np.mean(latencies)
        assert avg_latency < 100, f"í‰ê·  ì§€ì—°ì‹œê°„ì´ ë„ˆë¬´ ë†’ìŒ: {avg_latency:.1f}ms"
        
        # P95 ì§€ì—°ì‹œê°„ ê²€ì¦
        p95_latency = np.percentile(latencies, 95)
        assert p95_latency < 150, f"P95 ì§€ì—°ì‹œê°„ì´ ë„ˆë¬´ ë†’ìŒ: {p95_latency:.1f}ms"
    
    @pytest.mark.performance
    def test_fps_benchmark(self, real_time_processor):
        """FPS ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        fps_counter = FPSCounter()
        test_duration = 5.0  # 5ì´ˆê°„ í…ŒìŠ¤íŠ¸
        
        start_time = time.time()
        frame_count = 0
        
        while time.time() - start_time < test_duration:
            # ê°€ì§œ í”„ë ˆì„ ìƒì„± ë° ì²˜ë¦¬
            frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            real_time_processor.frame_buffer.put_frame(frame)
            
            current_fps = fps_counter.tick()
            frame_count += 1
            
            time.sleep(0.01)  # 10ms ëŒ€ê¸°
        
        final_fps = fps_counter.tick()
        
        # FPS ì„ê³„ê°’ ê²€ì¦
        assert final_fps >= 15.0, f"FPSê°€ ë„ˆë¬´ ë‚®ìŒ: {final_fps:.1f}"
        assert frame_count > 0, "ì²˜ë¦¬ëœ í”„ë ˆì„ì´ ì—†ìŒ"

class ModelPerformanceTest:
    """ëª¨ë¸ ì„±ëŠ¥ íšŒê·€ í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def benchmark_dataset(self):
        """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹"""
        return {
            'images': self._load_test_images(),
            'annotations': self._load_test_annotations(),
            'expected_metrics': {
                'mAP': 0.85,
                'mAP50': 0.92,
                'precision': 0.88,
                'recall': 0.83
            }
        }
    
    def test_model_accuracy_regression(self, benchmark_dataset):
        """ëª¨ë¸ ì •í™•ë„ íšŒê·€ í…ŒìŠ¤íŠ¸"""
        model = self._load_production_model()
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        results = self._run_benchmark(model, benchmark_dataset)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê²€ì¦
        for metric, expected_value in benchmark_dataset['expected_metrics'].items():
            actual_value = results[metric]
            tolerance = 0.02  # 2% í—ˆìš© ì˜¤ì°¨
            
            assert actual_value >= expected_value - tolerance, \
                f"{metric} ì„±ëŠ¥ ì €í•˜: {actual_value:.3f} < {expected_value:.3f}"
    
    def test_inference_speed_benchmark(self):
        """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        model = self._load_production_model()
        test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # ì›Œë°ì—…
        for _ in range(10):
            _ = model(test_image)
        
        # ì‹¤ì œ ì¸¡ì •
        inference_times = []
        for _ in range(100):
            start_time = time.time()
            _ = model(test_image)
            inference_time = (time.time() - start_time) * 1000  # ms
            inference_times.append(inference_time)
        
        # ì„±ëŠ¥ ê²€ì¦
        avg_time = np.mean(inference_times)
        p95_time = np.percentile(inference_times, 95)
        
        assert avg_time < 50, f"í‰ê·  ì¶”ë¡  ì‹œê°„ì´ ë„ˆë¬´ ëŠë¦¼: {avg_time:.1f}ms"
        assert p95_time < 80, f"P95 ì¶”ë¡  ì‹œê°„ì´ ë„ˆë¬´ ëŠë¦¼: {p95_time:.1f}ms"

class HardwareCompatibilityTest:
    """í•˜ë“œì›¨ì–´ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.gpu
    def test_gpu_compatibility(self):
        """GPU í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        if not torch.cuda.is_available():
            pytest.skip("GPUê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥")
        
        # GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
        device = torch.device('cuda')
        test_tensor = torch.randn(1000, 1000, device=device)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        memory_used = torch.cuda.memory_allocated() / 1024**2  # MB
        assert memory_used > 0, "GPU ë©”ëª¨ë¦¬ê°€ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ"
        
        # GPU ì—°ì‚° í…ŒìŠ¤íŠ¸
        result = torch.matmul(test_tensor, test_tensor.T)
        assert result.device == device, "GPUì—ì„œ ì—°ì‚°ë˜ì§€ ì•ŠìŒ"
    
    @pytest.mark.cpu
    def test_cpu_fallback(self):
        """CPU í´ë°± í…ŒìŠ¤íŠ¸"""
        # GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜
        with patch('torch.cuda.is_available', return_value=False):
            processor = RealTimeProcessor(Mock(), device='cpu')
            
            # CPUì—ì„œ ì •ìƒ ë™ì‘ í™•ì¸
            test_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            success = processor.frame_buffer.put_frame(test_frame)
            assert success, "CPU í™˜ê²½ì—ì„œ í”„ë ˆì„ ì²˜ë¦¬ ì‹¤íŒ¨"
    
    @pytest.mark.jetson
    def test_jetson_optimization(self):
        """Jetson í”Œë«í¼ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        # Jetson í™˜ê²½ ê°ì§€
        is_jetson = self._detect_jetson_platform()
        
        if not is_jetson:
            pytest.skip("Jetson í”Œë«í¼ì´ ì•„ë‹˜")
        
        # Jetson íŠ¹í™” ìµœì í™” í™•ì¸
        processor = RealTimeProcessor(Mock(), device='cuda')
        
        # TensorRT ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        assert hasattr(processor.model, 'trt_engine'), "TensorRT ì—”ì§„ì´ ì—†ìŒ"

class CameraStreamStabilityTest:
    """ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
    
    def test_camera_connection_stability(self):
        """ì¹´ë©”ë¼ ì—°ê²° ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
        camera_manager = MockCameraManager()
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        success = camera_manager.connect()
        assert success, "ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨"
        
        # ìŠ¤íŠ¸ë¦¼ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (30ì´ˆê°„)
        stable_duration = 30.0
        start_time = time.time()
        frame_count = 0
        error_count = 0
        
        while time.time() - start_time < stable_duration:
            try:
                frame = camera_manager.get_frame()
                if frame is not None:
                    frame_count += 1
                else:
                    error_count += 1
            except Exception:
                error_count += 1
            
            time.sleep(0.033)  # 30 FPS
        
        # ì•ˆì •ì„± ê²€ì¦
        error_rate = error_count / (frame_count + error_count)
        assert error_rate < 0.05, f"ì—ëŸ¬ìœ¨ì´ ë„ˆë¬´ ë†’ìŒ: {error_rate:.2%}"
        assert frame_count > 0, "í”„ë ˆì„ì„ ë°›ì§€ ëª»í•¨"
    
    def test_multiple_camera_sync(self):
        """ë‹¤ì¤‘ ì¹´ë©”ë¼ ë™ê¸°í™” í…ŒìŠ¤íŠ¸"""
        camera_count = 3
        cameras = [MockCameraManager(f"cam_{i}") for i in range(camera_count)]
        
        # ëª¨ë“  ì¹´ë©”ë¼ ì—°ê²°
        for cam in cameras:
            assert cam.connect(), f"ì¹´ë©”ë¼ {cam.id} ì—°ê²° ì‹¤íŒ¨"
        
        # ë™ê¸°í™” í…ŒìŠ¤íŠ¸
        sync_tolerance_ms = 50  # 50ms í—ˆìš© ì˜¤ì°¨
        
        for _ in range(100):  # 100 í”„ë ˆì„ í…ŒìŠ¤íŠ¸
            timestamps = []
            
            for cam in cameras:
                frame_data = cam.get_frame_with_timestamp()
                timestamps.append(frame_data['timestamp'])
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ë™ê¸°í™” í™•ì¸
            max_diff = (max(timestamps) - min(timestamps)) * 1000  # ms
            assert max_diff < sync_tolerance_ms, \
                f"ì¹´ë©”ë¼ ë™ê¸°í™” ì˜¤ì°¨ê°€ ë„ˆë¬´ í¼: {max_diff:.1f}ms"

class MemoryLeakTest:
    """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸"""
    
    def test_long_running_memory_stability(self):
        """ì¥ì‹œê°„ ì‹¤í–‰ ë©”ëª¨ë¦¬ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
        import psutil
        import gc
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024**2  # MB
        
        processor = RealTimeProcessor(Mock(), device='cpu')
        processor.start_processing()
        
        # 1ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” 10ì´ˆë¡œ ì¶•ì†Œ)
        test_duration = 10.0
        start_time = time.time()
        
        memory_samples = []
        
        while time.time() - start_time < test_duration:
            # í”„ë ˆì„ ì²˜ë¦¬
            frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            processor.frame_buffer.put_frame(frame)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            current_memory = process.memory_info().rss / 1024**2
            memory_samples.append(current_memory)
            
            time.sleep(0.1)
        
        processor.stop()
        
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê²€ì¦
        final_memory = process.memory_info().rss / 1024**2
        memory_increase = final_memory - initial_memory
        
        # 100MB ì´ìƒ ì¦ê°€í•˜ë©´ ëˆ„ìˆ˜ë¡œ íŒë‹¨
        assert memory_increase < 100, \
            f"ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬: {memory_increase:.1f}MB ì¦ê°€"
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ”ì§€ í™•ì¸
        if len(memory_samples) > 10:
            trend = np.polyfit(range(len(memory_samples)), memory_samples, 1)[0]
            assert trend < 1.0, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€: {trend:.2f}MB/s"

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì„¤ì •
pytest_plugins = [
    "pytest_benchmark",
    "pytest_mock", 
    "pytest_timeout",
    "pytest_xdist"  # ë³‘ë ¬ í…ŒìŠ¤íŠ¸
]

# í…ŒìŠ¤íŠ¸ ë§ˆì»¤ ì •ì˜
pytestmark = [
    pytest.mark.vision_system,
    pytest.mark.performance,
    pytest.mark.integration
]
```

### í…ŒìŠ¤íŠ¸ ìë™í™” ë° CI/CD í†µí•©
```python
# conftest.py - í…ŒìŠ¤íŠ¸ ì„¤ì •
import pytest
import numpy as np
from unittest.mock import Mock

@pytest.fixture(scope="session")
def test_config():
    """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
    return {
        'test_data_path': 'tests/data',
        'model_path': 'tests/models/test_model.pt',
        'benchmark_thresholds': {
            'fps_min': 15.0,
            'latency_max_ms': 100.0,
            'accuracy_min': 0.85
        }
    }

@pytest.fixture
def mock_camera():
    """ê°€ì§œ ì¹´ë©”ë¼ í”½ìŠ¤ì²˜"""
    camera = Mock()
    camera.get_frame.return_value = np.random.randint(
        0, 255, (480, 640, 3), dtype=np.uint8
    )
    camera.is_connected.return_value = True
    return camera

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
BENCHMARK_CONFIG = {
    'min_rounds': 10,
    'max_time': 30.0,
    'warmup_rounds': 3
}

# GitHub Actions ì›Œí¬í”Œë¡œìš° í†µí•©
GITHUB_ACTIONS_CONFIG = """
name: Vision System Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test-cpu:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-benchmark pytest-timeout
    - name: Run CPU tests
      run: |
        pytest tests/ -m "not gpu and not jetson" --benchmark-only
    
  test-gpu:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python with GPU
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    - name: Install GPU dependencies
      run: |
        pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
        pip install -r requirements.txt
    - name: Run GPU tests
      run: |
        pytest tests/ -m gpu --benchmark-only
"""
```


