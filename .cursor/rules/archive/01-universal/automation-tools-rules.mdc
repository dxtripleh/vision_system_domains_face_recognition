---
description: 
globs: 
alwaysApply: false
---
# ìë™í™” ë„êµ¬ ê·œì¹™ (Automation Tools Rules)

ì´ ê·œì¹™ì€ ê°œë°œ í”„ë¡œì„¸ìŠ¤ ìë™í™”, CI/CD, ìŠ¤í¬ë¦½íŠ¸ ê´€ë¦¬, ì½”ë“œ í’ˆì§ˆ ë„êµ¬ì— ê´€í•œ í‘œì¤€ì…ë‹ˆë‹¤.

## ğŸ”§ ìŠ¤í¬ë¦½íŠ¸ ìë™í™” ê·œì¹™

### ìŠ¤í¬ë¦½íŠ¸ ë„¤ì´ë° ë° ë¶„ë¥˜
```python
SCRIPT_CATEGORIES = {
    'setup': {
        'prefix': 'setup_',
        'purpose': 'ê°œë°œ í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”',
        'examples': ['setup_coding_tools.py', 'setup_git_hooks.py', 'setup_environment.py']
    },
    'check': {
        'prefix': 'check_',
        'purpose': 'ì½”ë“œ í’ˆì§ˆ ë° ê·œì¹™ ê²€ì¦',
        'examples': ['check_code_style.py', 'check_dependencies.py', 'check_security.py']
    },
    'validate': {
        'prefix': 'validate_',
        'purpose': 'ë°ì´í„° ë° ì„¤ì • ê²€ì¦',
        'examples': ['validate_rules.py', 'validate_config.py', 'validate_data.py']
    },
    'generate': {
        'prefix': 'generate_',
        'purpose': 'íŒŒì¼ ë° ì½”ë“œ ìƒì„±',
        'examples': ['generate_docs.py', 'generate_config.py', 'generate_templates.py']
    },
    'deploy': {
        'prefix': 'deploy_',
        'purpose': 'ë°°í¬ ë° ë¦´ë¦¬ì¦ˆ',
        'examples': ['deploy_staging.py', 'deploy_production.py', 'deploy_docs.py']
    },
    'download': {
        'prefix': 'download_',
        'purpose': 'ëª¨ë¸ ë° ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ',
        'examples': ['download_models.py', 'download_opencv_cascades.py', 'download_datasets.py']
    },
    'cleanup': {
        'prefix': 'cleanup_',
        'purpose': 'ì •ë¦¬ ë° ìµœì í™”',
        'examples': ['cleanup_models.py', 'cleanup_weights.py', 'cleanup_temp_files.py']
    }
}
```

### ëª¨ë¸ ê´€ë¦¬ ìë™í™”
```python
# scripts/model_management/ êµ¬í˜„ í‘œì¤€
MODEL_MANAGEMENT_TASKS = {
    'download_models': {
        'description': 'ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ',
        'script': 'download_models.py',
        'config_key': 'model_urls'
    },
    'download_opencv_cascades': {
        'description': 'OpenCV Haar Cascade íŒŒì¼ ë‹¤ìš´ë¡œë“œ',
        'script': 'download_opencv_cascades.py',
        'config_key': 'cascade_urls'
    },
    'generate_metadata': {
        'description': 'ëª¨ë¸ ë©”íƒ€ë°ì´í„° ìƒì„±',
        'script': 'generate_model_metadata.py',
        'config_key': 'metadata_config'
    },
    'validate_metadata': {
        'description': 'ëª¨ë¸ ë©”íƒ€ë°ì´í„° ê²€ì¦',
        'script': 'validate_model_metadata.py',
        'config_key': 'validation_config'
    },
    'cleanup_models': {
        'description': 'ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì •ë¦¬',
        'script': 'cleanup_models.py',
        'config_key': 'cleanup_config'
    },
    'restructure_models': {
        'description': 'ëª¨ë¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì¬ì •ë¦¬',
        'script': 'restructure_models.py',
        'config_key': 'restructure_config'
    }
}

def run_model_management_task(task_name, config_file=None):
    """ëª¨ë¸ ê´€ë¦¬ ì‘ì—… ì‹¤í–‰"""
    if task_name not in MODEL_MANAGEMENT_TASKS:
        raise ValueError(f"Unknown task: {task_name}")
    
    task_config = MODEL_MANAGEMENT_TASKS[task_name]
    script_path = f"scripts/model_management/{task_config['script']}"
    
    if not os.path.exists(script_path):
        raise FileNotFoundError(f"Script not found: {script_path}")
    
    # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
    cmd = [sys.executable, script_path]
    if config_file:
        cmd.extend(['--config', config_file])
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    return {
        'returncode': result.returncode,
        'stdout': result.stdout,
        'stderr': result.stderr,
        'success': result.returncode == 0
    }
```

## ğŸ“Š ë°ì´í„°ì…‹ ê´€ë¦¬ ìë™í™”

### ë°ì´í„°ì…‹ íŒŒì´í”„ë¼ì¸ ìë™í™”
```python
# scripts/dataset_pipeline.py êµ¬í˜„ í‘œì¤€
DATASET_PIPELINE_STAGES = {
    'setup': {
        'description': 'ë„ë©”ì¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ë° í´ë˜ìŠ¤ ë§µ ì„¤ì •',
        'function': 'setup_domain',
        'dependencies': []
    },
    'split': {
        'description': 'ë°ì´í„°ì…‹ ë¶„í•  (train/validation/test)',
        'function': 'split_dataset',
        'dependencies': ['setup']
    },
    'augment': {
        'description': 'ë°ì´í„° ì¦ê°•',
        'function': 'augment_dataset',
        'dependencies': ['split']
    },
    'preprocess': {
        'description': 'ë°ì´í„° ì „ì²˜ë¦¬',
        'function': 'preprocess_dataset',
        'dependencies': ['augment']
    },
    'validate': {
        'description': 'ë°ì´í„°ì…‹ ê²€ì¦',
        'function': 'validate_dataset',
        'dependencies': ['preprocess']
    }
}

def run_dataset_pipeline(domain, config, stages=None):
    """ë°ì´í„°ì…‹ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    if stages is None:
        stages = list(DATASET_PIPELINE_STAGES.keys())
    
    results = {}
    
    for stage in stages:
        if stage not in DATASET_PIPELINE_STAGES:
            results[stage] = {'status': 'error', 'message': f'Unknown stage: {stage}'}
            continue
        
        stage_config = DATASET_PIPELINE_STAGES[stage]
        
        # ì˜ì¡´ì„± í™•ì¸
        for dependency in stage_config['dependencies']:
            if dependency not in results or results[dependency]['status'] != 'success':
                results[stage] = {'status': 'skipped', 'message': f'Dependency {dependency} failed'}
                continue
        
        try:
            function = globals()[stage_config['function']]
            result = function(domain, config)
            results[stage] = {'status': 'success', 'result': result}
            
        except Exception as e:
            results[stage] = {'status': 'error', 'message': str(e)}
    
    return results
```

## ğŸ” ì½”ë“œ í’ˆì§ˆ ìë™í™”

### ì½”ë“œ ìŠ¹ê²© ìë™í™”
```python
# scripts/code_promotion/ êµ¬í˜„ í‘œì¤€
CODE_PROMOTION_CRITERIA = {
    'reusability': {
        'description': 'ì—¬ëŸ¬ ëª¨ë“ˆì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥ì„±',
        'weight': 0.3,
        'threshold': 0.7,
        'analyzer': 'analyze_reusability'
    },
    'stability': {
        'description': 'ì½”ë“œì˜ ì•ˆì •ì„± ë° í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€',
        'weight': 0.25,
        'threshold': 0.8,
        'analyzer': 'analyze_stability'
    },
    'generality': {
        'description': 'ì½”ë“œì˜ ë²”ìš©ì„±',
        'weight': 0.25,
        'threshold': 0.7,
        'analyzer': 'analyze_generality'
    },
    'duplication': {
        'description': 'ì½”ë“œ ì¤‘ë³µ ì •ë„',
        'weight': 0.2,
        'threshold': 0.6,
        'analyzer': 'analyze_duplication'
    }
}

def analyze_promotion_candidates(source_path):
    """ì½”ë“œ ìŠ¹ê²© í›„ë³´ ë¶„ì„"""
    candidates = []
    
    for root, dirs, files in os.walk(source_path):
        for file in files:
            if file.endswith('.py'):
                file_path = os.path.join(root, file)
                
                scores = {}
                total_score = 0
                
                for criterion, config in CODE_PROMOTION_CRITERIA.items():
                    analyzer = globals()[config['analyzer']]
                    score = analyzer(file_path)
                    scores[criterion] = score
                    total_score += score * config['weight']
                
                if total_score >= 0.7:  # ì „ì²´ ì ìˆ˜ ì„ê³„ê°’
                    candidates.append({
                        'file_path': file_path,
                        'total_score': total_score,
                        'scores': scores,
                        'eligible': all(
                            scores[criterion] >= config['threshold']
                            for criterion, config in CODE_PROMOTION_CRITERIA.items()
                        )
                    })
    
    return candidates
```

### êµ¬ì¡° ê²€ì¦ ìë™í™”
```python
# scripts/validate_structure.py êµ¬í˜„ í‘œì¤€
STRUCTURE_VALIDATION_RULES = {
    'root_structure': {
        'description': 'ë£¨íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ê²€ì¦',
        'validator': 'validate_root_structure',
        'severity': 'error'
    },
    'common_structure': {
        'description': 'common ë””ë ‰í† ë¦¬ êµ¬ì¡° ê²€ì¦',
        'validator': 'validate_common_structure',
        'severity': 'error'
    },
    'layer_dependencies': {
        'description': 'ê³„ì¸µ ê°„ ì˜ì¡´ì„± ê²€ì¦',
        'validator': 'validate_layer_dependencies',
        'severity': 'error'
    },
    'file_naming': {
        'description': 'íŒŒì¼ ë„¤ì´ë° ê·œì¹™ ê²€ì¦',
        'validator': 'validate_file_naming',
        'severity': 'warning'
    },
    'import_rules': {
        'description': 'Import ê·œì¹™ ê²€ì¦',
        'validator': 'validate_import_rules',
        'severity': 'warning'
    }
}

def run_structure_validation(rules=None, fail_on_warning=False):
    """êµ¬ì¡° ê²€ì¦ ì‹¤í–‰"""
    if rules is None:
        rules = list(STRUCTURE_VALIDATION_RULES.keys())
    
    results = {}
    has_errors = False
    has_warnings = False
    
    for rule_name in rules:
        if rule_name not in STRUCTURE_VALIDATION_RULES:
            results[rule_name] = {
                'status': 'error',
                'message': f'Unknown rule: {rule_name}'
            }
            has_errors = True
            continue
        
        rule_config = STRUCTURE_VALIDATION_RULES[rule_name]
        
        try:
            validator = globals()[rule_config['validator']]
            result = validator()
            
            results[rule_name] = {
                'status': 'success' if result['valid'] else rule_config['severity'],
                'valid': result['valid'],
                'errors': result.get('errors', []),
                'warnings': result.get('warnings', []),
                'info': result.get('info', {})
            }
            
            if not result['valid']:
                if rule_config['severity'] == 'error':
                    has_errors = True
                else:
                    has_warnings = True
                    
        except Exception as e:
            results[rule_name] = {
                'status': 'error',
                'message': str(e)
            }
            has_errors = True
    
    # ì „ì²´ ê²°ê³¼ íŒì •
    overall_status = 'success'
    if has_errors:
        overall_status = 'error'
    elif has_warnings and fail_on_warning:
        overall_status = 'error'
    elif has_warnings:
        overall_status = 'warning'
    
    return {
        'overall_status': overall_status,
        'results': results,
        'summary': {
            'total_rules': len(rules),
            'passed': sum(1 for r in results.values() if r['status'] == 'success'),
            'warnings': sum(1 for r in results.values() if r['status'] == 'warning'),
            'errors': sum(1 for r in results.values() if r['status'] == 'error')
        }
    }
```

## ğŸš€ CI/CD í†µí•©

### GitHub Actions ì›Œí¬í”Œë¡œìš°
```yaml
# .github/workflows/structure_validation.yml
name: Structure Validation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  validate-structure:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Validate project structure
      run: |
        python scripts/validate_structure.py --fail-on-warning
    
    - name: Validate code style
      run: |
        python scripts/check_code_style.py
    
    - name: Run tests
      run: |
        python -m pytest tests/ --cov=src/ --cov-report=xml
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

### Pre-commit ì„¤ì •
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black
        language_version: python3.9

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
        args: ["--profile", "black"]

  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        args: [--max-line-length=88, --extend-ignore=E203,W503]

  - repo: local
    hooks:
      - id: validate-structure
        name: Validate project structure
        entry: python scripts/validate_structure.py
        language: system
        pass_filenames: false
        always_run: true

      - id: check-naming-conventions
        name: Check file naming conventions
        entry: python scripts/check_naming_conventions.py
        language: system
        files: \.py$
```

## ğŸ“ˆ ìë™í™” ëª¨ë‹ˆí„°ë§ ë° ë³´ê³ 

### ìë™í™” ë³´ê³ ì„œ ìƒì„±
```python
# scripts/generate_automation_report.py êµ¬í˜„ í‘œì¤€
REPORT_SECTIONS = {
    'structure_validation': {
        'title': 'í”„ë¡œì íŠ¸ êµ¬ì¡° ê²€ì¦',
        'generator': 'generate_structure_report',
        'output_file': 'structure_validation_report.md'
    },
    'code_quality': {
        'title': 'ì½”ë“œ í’ˆì§ˆ ë¶„ì„',
        'generator': 'generate_quality_report',
        'output_file': 'code_quality_report.md'
    },
    'test_coverage': {
        'title': 'í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€',
        'generator': 'generate_coverage_report',
        'output_file': 'test_coverage_report.md'
    },
    'dependency_analysis': {
        'title': 'ì˜ì¡´ì„± ë¶„ì„',
        'generator': 'generate_dependency_report',
        'output_file': 'dependency_analysis_report.md'
    }
}

def generate_comprehensive_report(output_dir="data/reports/automation"):
    """ì¢…í•© ìë™í™” ë³´ê³ ì„œ ìƒì„±"""
    os.makedirs(output_dir, exist_ok=True)
    
    reports = {}
    
    for section_name, config in REPORT_SECTIONS.items():
        try:
            generator = globals()[config['generator']]
            report_content = generator()
            
            output_path = os.path.join(output_dir, config['output_file'])
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            reports[section_name] = {
                'status': 'success',
                'output_file': output_path,
                'title': config['title']
            }
            
        except Exception as e:
            reports[section_name] = {
                'status': 'error',
                'message': str(e),
                'title': config['title']
            }
    
    # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
    summary_report = generate_summary_report(reports)
    summary_path = os.path.join(output_dir, 'automation_summary.md')
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(summary_report)
    
    return {
        'summary_report': summary_path,
        'section_reports': reports
    }
```

ì´ ìë™í™” ë„êµ¬ ê·œì¹™ì€ ê°œë°œ í”„ë¡œì„¸ìŠ¤ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ê³  ì½”ë“œ í’ˆì§ˆì„ ë³´ì¥í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.

## ğŸ”„ ìŠ¤í¬ë¦½íŠ¸ ê°œë°œ ê·œì¹™ (ë©±ë“±ì„±, ë°±ì—…, ë¡¤ë°±)

### ë©±ë“±ì„± ë³´ì¥ ê·œì¹™
ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ëŠ” ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•´ë„ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë³´ì¥í•´ì•¼ í•©ë‹ˆë‹¤:

```python
# scripts/idempotent_script_template.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ë©±ë“±ì„±ì„ ë³´ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ í…œí”Œë¦¿

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•´ë„ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import logging
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional

class IdempotentScript:
    """ë©±ë“±ì„±ì„ ë³´ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, script_name: str, state_dir: str = "data/script_states"):
        self.script_name = script_name
        self.state_dir = Path(state_dir)
        self.state_dir.mkdir(parents=True, exist_ok=True)
        self.state_file = self.state_dir / f"{script_name}_state.json"
        self.logger = logging.getLogger(script_name)
        
        # ì´ì „ ìƒíƒœ ë¡œë“œ
        self.previous_state = self._load_state()
        self.current_state = {}
    
    def _load_state(self) -> Dict[str, Any]:
        """ì´ì „ ì‹¤í–‰ ìƒíƒœ ë¡œë“œ"""
        if not self.state_file.exists():
            return {}
        
        try:
            with open(self.state_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.warning(f"ìƒíƒœ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _save_state(self):
        """í˜„ì¬ ìƒíƒœ ì €ì¥"""
        try:
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.current_state, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"ìƒíƒœ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _calculate_checksum(self, file_path: str) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°"""
        if not os.path.exists(file_path):
            return ""
        
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def should_execute(self, operation_id: str, **params) -> bool:
        """ì‘ì—… ì‹¤í–‰ ì—¬ë¶€ ê²°ì •"""
        # ë§¤ê°œë³€ìˆ˜ ê¸°ë°˜ í•´ì‹œ ìƒì„±
        param_hash = hashlib.md5(
            json.dumps(params, sort_keys=True).encode()
        ).hexdigest()
        
        # ì´ì „ ì‹¤í–‰ ì •ë³´ í™•ì¸
        prev_execution = self.previous_state.get(operation_id, {})
        
        if prev_execution.get('param_hash') == param_hash:
            self.logger.info(f"ì‘ì—… '{operation_id}' ì´ë¯¸ ì‹¤í–‰ë¨ (ìŠ¤í‚µ)")
            return False
        
        return True
    
    def mark_completed(self, operation_id: str, **params):
        """ì‘ì—… ì™„ë£Œ í‘œì‹œ"""
        param_hash = hashlib.md5(
            json.dumps(params, sort_keys=True).encode()
        ).hexdigest()
        
        self.current_state[operation_id] = {
            'param_hash': param_hash,
            'completed_at': str(datetime.now()),
            'status': 'completed'
        }
    
    def execute(self):
        """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError("execute ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    def run(self):
        """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë° ìƒíƒœ ê´€ë¦¬"""
        try:
            self.execute()
            self._save_state()
            self.logger.info("ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise

# ì‚¬ìš© ì˜ˆì‹œ
class ModelDownloadScript(IdempotentScript):
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì‹œ"""
    
    def execute(self):
        models_to_download = [
            {'name': 'yolov8n', 'url': 'https://...', 'path': 'models/weights/yolov8n.pt'},
            {'name': 'yolov8s', 'url': 'https://...', 'path': 'models/weights/yolov8s.pt'}
        ]
        
        for model in models_to_download:
            operation_id = f"download_{model['name']}"
            
            if self.should_execute(operation_id, **model):
                self._download_model(model)
                self.mark_completed(operation_id, **model)
    
    def _download_model(self, model: dict):
        """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì‹¤ì œ êµ¬í˜„)"""
        # ë‹¤ìš´ë¡œë“œ ë¡œì§
        self.logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {model['name']}")
        # download_file(model['url'], model['path'])
```

### ìë™ ë°±ì—… ì‹œìŠ¤í…œ
```python
# common/utils/backup_system.py
import os
import shutil
import datetime
import tarfile
from pathlib import Path
from typing import List, Optional

class AutoBackupSystem:
    """ìë™ ë°±ì—… ì‹œìŠ¤í…œ"""
    
    def __init__(self, backup_dir: str = "data/backups"):
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        self.max_backups = 10  # ìµœëŒ€ ë°±ì—… ê°œìˆ˜
    
    def create_backup(self, source_paths: List[str], backup_name: str = None) -> str:
        """ë°±ì—… ìƒì„±"""
        if backup_name is None:
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_name = f"backup_{timestamp}"
        
        backup_path = self.backup_dir / f"{backup_name}.tar.gz"
        
        with tarfile.open(backup_path, 'w:gz') as tar:
            for source_path in source_paths:
                if os.path.exists(source_path):
                    tar.add(source_path, arcname=os.path.basename(source_path))
        
        # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
        self._cleanup_old_backups()
        
        return str(backup_path)
    
    def restore_backup(self, backup_path: str, restore_dir: str = "."):
        """ë°±ì—… ë³µì›"""
        if not os.path.exists(backup_path):
            raise FileNotFoundError(f"ë°±ì—… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {backup_path}")
        
        with tarfile.open(backup_path, 'r:gz') as tar:
            tar.extractall(path=restore_dir)
    
    def list_backups(self) -> List[str]:
        """ë°±ì—… ëª©ë¡ ì¡°íšŒ"""
        backups = []
        for backup_file in self.backup_dir.glob("backup_*.tar.gz"):
            backups.append(str(backup_file))
        return sorted(backups, reverse=True)  # ìµœì‹ ìˆœ ì •ë ¬
    
    def _cleanup_old_backups(self):
        """ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬"""
        backups = self.list_backups()
        if len(backups) > self.max_backups:
            for old_backup in backups[self.max_backups:]:
                os.remove(old_backup)

# ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë°±ì—… ì‹œìŠ¤í…œ ì‚¬ìš©
def with_backup(backup_paths: List[str]):
    """ë°±ì—…ì„ í¬í•¨í•œ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            backup_system = AutoBackupSystem()
            
            # ì‹¤í–‰ ì „ ë°±ì—…
            backup_path = backup_system.create_backup(
                backup_paths, 
                f"pre_{func.__name__}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            
            try:
                result = func(*args, **kwargs)
                return result
            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ ë°±ì—… ë³µì› ì˜µì…˜ ì œê³µ
                print(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                print(f"ë°±ì—… íŒŒì¼: {backup_path}")
                print("ë°±ì—…ì„ ë³µì›í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:")
                print(f"python -c \"from common.utils.backup_system import AutoBackupSystem; AutoBackupSystem().restore_backup('{backup_path}')\"")
                raise
        return wrapper
    return decorator
```

### ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜
```python
# common/utils/rollback_manager.py
import json
import logging
from typing import Dict, List, Any, Callable
from dataclasses import dataclass, asdict
from datetime import datetime

@dataclass
class RollbackAction:
    """ë¡¤ë°± ì•¡ì…˜ ì •ì˜"""
    action_type: str  # 'file_restore', 'command_execute', 'config_restore'
    description: str
    data: Dict[str, Any]
    timestamp: str

class RollbackManager:
    """ë¡¤ë°± ê´€ë¦¬ì"""
    
    def __init__(self, transaction_id: str = None):
        self.transaction_id = transaction_id or f"txn_{int(datetime.now().timestamp())}"
        self.rollback_actions: List[RollbackAction] = []
        self.logger = logging.getLogger(__name__)
    
    def add_file_backup(self, file_path: str):
        """íŒŒì¼ ë°±ì—… ì¶”ê°€"""
        if os.path.exists(file_path):
            backup_content = self._read_file_content(file_path)
            action = RollbackAction(
                action_type='file_restore',
                description=f'íŒŒì¼ ë³µì›: {file_path}',
                data={
                    'file_path': file_path,
                    'original_content': backup_content
                },
                timestamp=datetime.now().isoformat()
            )
            self.rollback_actions.append(action)
    
    def add_command_rollback(self, rollback_command: str, description: str = ""):
        """ëª…ë ¹ì–´ ë¡¤ë°± ì¶”ê°€"""
        action = RollbackAction(
            action_type='command_execute',
            description=description or f'ëª…ë ¹ì–´ ì‹¤í–‰: {rollback_command}',
            data={'command': rollback_command},
            timestamp=datetime.now().isoformat()
        )
        self.rollback_actions.append(action)
    
    def add_config_backup(self, config_key: str, config_value: Any):
        """ì„¤ì • ë°±ì—… ì¶”ê°€"""
        action = RollbackAction(
            action_type='config_restore',
            description=f'ì„¤ì • ë³µì›: {config_key}',
            data={
                'config_key': config_key,
                'original_value': config_value
            },
            timestamp=datetime.now().isoformat()
        )
        self.rollback_actions.append(action)
    
    def execute_rollback(self):
        """ë¡¤ë°± ì‹¤í–‰"""
        self.logger.info(f"ë¡¤ë°± ì‹œì‘: {self.transaction_id}")
        
        # ì—­ìˆœìœ¼ë¡œ ë¡¤ë°± ì‹¤í–‰
        for action in reversed(self.rollback_actions):
            try:
                self._execute_action(action)
                self.logger.info(f"ë¡¤ë°± ì™„ë£Œ: {action.description}")
            except Exception as e:
                self.logger.error(f"ë¡¤ë°± ì‹¤íŒ¨: {action.description} - {e}")
    
    def _execute_action(self, action: RollbackAction):
        """ê°œë³„ ì•¡ì…˜ ì‹¤í–‰"""
        if action.action_type == 'file_restore':
            file_path = action.data['file_path']
            content = action.data['original_content']
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
        
        elif action.action_type == 'command_execute':
            command = action.data['command']
            os.system(command)
        
        elif action.action_type == 'config_restore':
            # ì„¤ì • ë³µì› ë¡œì§ (êµ¬í˜„ í•„ìš”)
            pass
    
    def _read_file_content(self, file_path: str) -> str:
        """íŒŒì¼ ë‚´ìš© ì½ê¸°"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception:
            return ""
    
    def save_rollback_plan(self, file_path: str = None):
        """ë¡¤ë°± ê³„íš ì €ì¥"""
        if file_path is None:
            file_path = f"data/rollback_plans/rollback_{self.transaction_id}.json"
        
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        rollback_data = {
            'transaction_id': self.transaction_id,
            'created_at': datetime.now().isoformat(),
            'actions': [asdict(action) for action in self.rollback_actions]
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(rollback_data, f, indent=2, ensure_ascii=False)

# ì‚¬ìš© ì˜ˆì‹œ: ë¡¤ë°± ì§€ì› ìŠ¤í¬ë¦½íŠ¸
def safe_script_execution(func):
    """ì•ˆì „í•œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë°ì½”ë ˆì´í„°"""
    def wrapper(*args, **kwargs):
        rollback_manager = RollbackManager()
        
        try:
            # ì‹¤í–‰ ì „ ì¤‘ìš” íŒŒì¼ë“¤ ë°±ì—…
            important_files = [
                'config/main_config.yaml',
                'models/current_model.pt'
            ]
            
            for file_path in important_files:
                if os.path.exists(file_path):
                    rollback_manager.add_file_backup(file_path)
            
            # ë¡¤ë°± ê³„íš ì €ì¥
            rollback_manager.save_rollback_plan()
            
            # í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)
            
            return result
            
        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ìë™ ë¡¤ë°±
            print(f"ì‹¤í–‰ ì‹¤íŒ¨, ë¡¤ë°± ì‹œì‘: {e}")
            rollback_manager.execute_rollback()
            raise
    
    return wrapper

# ì‚¬ìš© ì˜ˆì‹œ
@safe_script_execution
def risky_model_update():
    """ìœ„í—˜í•œ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‘ì—…"""
    # ëª¨ë¸ ì—…ë°ì´íŠ¸ ë¡œì§
    pass
```

### ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ìƒíƒœ ëª¨ë‹ˆí„°ë§
```python
# common/utils/script_monitor.py
import time
import psutil
import threading
from typing import Dict, Any
from datetime import datetime

class ScriptMonitor:
    """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, script_name: str):
        self.script_name = script_name
        self.start_time = datetime.now()
        self.monitoring = False
        self.metrics = {
            'cpu_usage': [],
            'memory_usage': [],
            'execution_time': 0,
            'status': 'running'
        }
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring = False
        self.metrics['execution_time'] = (datetime.now() - self.start_time).total_seconds()
        self.metrics['status'] = 'completed'
    
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring:
            # CPU ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ìˆ˜ì§‘
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_percent = psutil.virtual_memory().percent
            
            self.metrics['cpu_usage'].append(cpu_percent)
            self.metrics['memory_usage'].append(memory_percent)
            
            time.sleep(5)  # 5ì´ˆë§ˆë‹¤ ìˆ˜ì§‘
    
    def get_summary(self) -> Dict[str, Any]:
        """ì‹¤í–‰ ìš”ì•½ ë°˜í™˜"""
        return {
            'script_name': self.script_name,
            'start_time': self.start_time.isoformat(),
            'execution_time': self.metrics['execution_time'],
            'avg_cpu_usage': sum(self.metrics['cpu_usage']) / len(self.metrics['cpu_usage']) if self.metrics['cpu_usage'] else 0,
            'avg_memory_usage': sum(self.metrics['memory_usage']) / len(self.metrics['memory_usage']) if self.metrics['memory_usage'] else 0,
            'status': self.metrics['status']
        }

# ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°
def monitor_execution(func):
    """ì‹¤í–‰ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def wrapper(*args, **kwargs):
        monitor = ScriptMonitor(func.__name__)
        monitor.start_monitoring()
        
        try:
            result = func(*args, **kwargs)
            monitor.stop_monitoring()
            
            # ì‹¤í–‰ ìš”ì•½ ì¶œë ¥
            summary = monitor.get_summary()
            print(f"\n=== ì‹¤í–‰ ìš”ì•½: {summary['script_name']} ===")
            print(f"ì‹¤í–‰ ì‹œê°„: {summary['execution_time']:.2f}ì´ˆ")
            print(f"í‰ê·  CPU ì‚¬ìš©ë¥ : {summary['avg_cpu_usage']:.1f}%")
            print(f"í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {summary['avg_memory_usage']:.1f}%")
            print(f"ìƒíƒœ: {summary['status']}")
            
            return result
            
        except Exception as e:
            monitor.stop_monitoring()
            monitor.metrics['status'] = 'failed'
            raise
    
    return wrapper
```




