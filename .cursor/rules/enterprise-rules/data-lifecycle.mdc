---
description: 
globs: 
alwaysApply: false
---
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒëª…ì£¼ê¸° ê´€ë¦¬ (ENTERPRISE)

## ğŸ“Š ë°ì´í„° ë ˆì´í¬ ì•„í‚¤í…ì²˜

### ê³„ì¸µí˜• ë°ì´í„° ì €ì¥ ì „ëµ
```python
# shared/data/data_lake_manager.py
from enum import Enum
from typing import Dict, List, Optional, Any
import boto3
import time
from datetime import datetime, timedelta

class DataTier(Enum):
    """ë°ì´í„° ê³„ì¸µ ì •ì˜"""
    HOT = "hot"         # ì‹¤ì‹œê°„ ì ‘ê·¼ (SSD)
    WARM = "warm"       # ë¹ˆë²ˆí•œ ì ‘ê·¼ (Standard Storage)
    COLD = "cold"       # ê°€ë” ì ‘ê·¼ (Infrequent Access)
    FROZEN = "frozen"   # ì•„ì¹´ì´ë¸Œ (Glacier)

class DataCategory(Enum):
    """ë°ì´í„° ë¶„ë¥˜"""
    RAW_VIDEO = "raw_video"
    PROCESSED_FRAMES = "processed_frames"
    MODEL_OUTPUTS = "model_outputs"
    METADATA = "metadata"
    LOGS = "logs"
    MODELS = "models"
    BACKUPS = "backups"

class DataLakeManager:
    """ì—”í„°í”„ë¼ì´ì¦ˆ ë°ì´í„° ë ˆì´í¬ ê´€ë¦¬"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.s3_client = boto3.client('s3')
        self.glacier_client = boto3.client('glacier')
        
        # ê³„ì¸µë³„ ì €ì¥ ìœ„ì¹˜ ì •ì˜
        self.storage_tiers = {
            DataTier.HOT: {
                'storage_class': 'STANDARD',
                'bucket': config['hot_bucket'],
                'max_size_gb': 1000,  # 1TB
                'retention_days': 7
            },
            DataTier.WARM: {
                'storage_class': 'STANDARD_IA',
                'bucket': config['warm_bucket'],
                'max_size_gb': 5000,  # 5TB
                'retention_days': 30
            },
            DataTier.COLD: {
                'storage_class': 'GLACIER',
                'bucket': config['cold_bucket'],
                'max_size_gb': 50000,  # 50TB
                'retention_days': 365
            },
            DataTier.FROZEN: {
                'storage_class': 'DEEP_ARCHIVE',
                'bucket': config['frozen_bucket'],
                'max_size_gb': -1,    # ë¬´ì œí•œ
                'retention_days': 2555  # 7ë…„
            }
        }
        
        # ë°ì´í„° ë¶„ë¥˜ë³„ ìƒëª…ì£¼ê¸° ì •ì±…
        self.lifecycle_policies = {
            DataCategory.RAW_VIDEO: {
                'initial_tier': DataTier.HOT,
                'transitions': [
                    {'days': 1, 'tier': DataTier.WARM},
                    {'days': 30, 'tier': DataTier.COLD},
                    {'days': 365, 'tier': DataTier.FROZEN}
                ]
            },
            DataCategory.PROCESSED_FRAMES: {
                'initial_tier': DataTier.WARM,
                'transitions': [
                    {'days': 7, 'tier': DataTier.COLD},
                    {'days': 90, 'tier': DataTier.FROZEN}
                ]
            },
            DataCategory.MODEL_OUTPUTS: {
                'initial_tier': DataTier.HOT,
                'transitions': [
                    {'days': 30, 'tier': DataTier.WARM},
                    {'days': 365, 'tier': DataTier.COLD}
                ]
            },
            DataCategory.METADATA: {
                'initial_tier': DataTier.HOT,
                'transitions': [
                    {'days': 90, 'tier': DataTier.WARM}
                ]
            }
        }
    
    def store_data(self, data_path: str, category: DataCategory, metadata: Dict) -> str:
        """ë°ì´í„° ì €ì¥"""
        policy = self.lifecycle_policies[category]
        initial_tier = policy['initial_tier']
        
        # S3 í‚¤ ìƒì„± (íŒŒí‹°ì…”ë‹ í¬í•¨)
        s3_key = self._generate_s3_key(data_path, category, metadata)
        
        # ì´ˆê¸° ê³„ì¸µì— ì €ì¥
        bucket = self.storage_tiers[initial_tier]['bucket']
        storage_class = self.storage_tiers[initial_tier]['storage_class']
        
        # ë©”íƒ€ë°ì´í„° íƒœê·¸ ì¶”ê°€
        tags = {
            'category': category.value,
            'tier': initial_tier.value,
            'created_date': datetime.now().isoformat(),
            'domain': metadata.get('domain', 'unknown'),
            'retention_policy': 'automatic'
        }
        
        # S3 ì—…ë¡œë“œ
        self.s3_client.upload_file(
            data_path,
            bucket,
            s3_key,
            ExtraArgs={
                'StorageClass': storage_class,
                'Tagging': self._format_tags(tags),
                'Metadata': metadata
            }
        )
        
        # ìƒëª…ì£¼ê¸° ê·œì¹™ ì ìš©
        self._apply_lifecycle_policy(bucket, s3_key, policy)
        
        return f"s3://{bucket}/{s3_key}"
    
    def _generate_s3_key(self, data_path: str, category: DataCategory, metadata: Dict) -> str:
        """S3 í‚¤ ìƒì„± (íŒŒí‹°ì…”ë‹)"""
        now = datetime.now()
        
        # íŒŒí‹°ì…”ë‹ êµ¬ì¡°: category/year/month/day/domain/filename
        partition = (
            f"{category.value}/"
            f"year={now.year}/"
            f"month={now.month:02d}/"
            f"day={now.day:02d}/"
            f"domain={metadata.get('domain', 'unknown')}/"
        )
        
        filename = os.path.basename(data_path)
        return f"{partition}{filename}"
    
    def _apply_lifecycle_policy(self, bucket: str, s3_key: str, policy: Dict):
        """ìƒëª…ì£¼ê¸° ì •ì±… ì ìš©"""
        rules = []
        
        for transition in policy['transitions']:
            rules.append({
                'ID': f"transition_to_{transition['tier'].value}",
                'Status': 'Enabled',
                'Filter': {'Prefix': s3_key},
                'Transitions': [{
                    'Days': transition['days'],
                    'StorageClass': self.storage_tiers[transition['tier']]['storage_class']
                }]
            })
        
        # ìƒëª…ì£¼ê¸° êµ¬ì„± ì—…ë°ì´íŠ¸
        self.s3_client.put_bucket_lifecycle_configuration(
            Bucket=bucket,
            LifecycleConfiguration={'Rules': rules}
        )
```

## ğŸ”„ ìë™ ë°ì´í„° ì •ë¦¬ ì‹œìŠ¤í…œ

### ì§€ëŠ¥í˜• ë°ì´í„° ì •ë¦¬
```python
# shared/data/cleanup_manager.py
class IntelligentDataCleanup:
    """ì§€ëŠ¥í˜• ë°ì´í„° ì •ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.cleanup_policies = self._load_cleanup_policies()
        self.usage_analyzer = DataUsageAnalyzer()
        
    def _load_cleanup_policies(self) -> Dict:
        """ì •ë¦¬ ì •ì±… ë¡œë“œ"""
        return {
            'raw_video': {
                'max_age_days': 7,
                'max_size_gb': 500,
                'keep_if_referenced': True,
                'compression_enabled': True
            },
            'processed_frames': {
                'max_age_days': 30,
                'max_size_gb': 200,
                'keep_if_recent_access': True,
                'deduplication_enabled': True
            },
            'temp_files': {
                'max_age_hours': 24,
                'max_size_gb': 50,
                'aggressive_cleanup': True
            },
            'logs': {
                'max_age_days': 90,
                'max_size_gb': 20,
                'compression_enabled': True,
                'rotate_when_full': True
            },
            'model_outputs': {
                'max_age_days': 180,
                'max_size_gb': 100,
                'keep_important_samples': True,
                'archive_before_delete': True
            }
        }
    
    def run_cleanup(self, dry_run: bool = True) -> Dict:
        """ë°ì´í„° ì •ë¦¬ ì‹¤í–‰"""
        cleanup_report = {
            'start_time': datetime.now().isoformat(),
            'actions': [],
            'space_freed_gb': 0,
            'files_processed': 0,
            'errors': []
        }
        
        for data_type, policy in self.cleanup_policies.items():
            try:
                type_report = self._cleanup_data_type(data_type, policy, dry_run)
                cleanup_report['actions'].append(type_report)
                cleanup_report['space_freed_gb'] += type_report['space_freed_gb']
                cleanup_report['files_processed'] += type_report['files_count']
                
            except Exception as e:
                error_info = {
                    'data_type': data_type,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }
                cleanup_report['errors'].append(error_info)
                logger.error(f"Cleanup failed for {data_type}: {str(e)}")
        
        cleanup_report['end_time'] = datetime.now().isoformat()
        self._save_cleanup_report(cleanup_report)
        
        return cleanup_report
    
    def _cleanup_data_type(self, data_type: str, policy: Dict, dry_run: bool) -> Dict:
        """íŠ¹ì • ë°ì´í„° íƒ€ì… ì •ë¦¬"""
        base_path = f"data/{data_type}"
        
        if not os.path.exists(base_path):
            return {'data_type': data_type, 'files_count': 0, 'space_freed_gb': 0}
        
        candidates = self._find_cleanup_candidates(base_path, policy)
        
        # ì‚¬ìš© íŒ¨í„´ ë¶„ì„
        usage_data = self.usage_analyzer.analyze_usage(candidates)
        
        # ì •ë¦¬ ëŒ€ìƒ í•„í„°ë§
        files_to_cleanup = self._filter_by_importance(candidates, usage_data, policy)
        
        files_count = len(files_to_cleanup)
        space_freed = 0
        
        if not dry_run:
            space_freed = self._execute_cleanup(files_to_cleanup, policy)
        else:
            space_freed = sum(os.path.getsize(f) for f in files_to_cleanup) / (1024**3)
        
        return {
            'data_type': data_type,
            'files_count': files_count,
            'space_freed_gb': space_freed,
            'dry_run': dry_run
        }
    
    def _execute_cleanup(self, files_to_cleanup: List[str], policy: Dict) -> float:
        """ì‹¤ì œ ì •ë¦¬ ì‹¤í–‰"""
        total_size = 0
        
        for file_path in files_to_cleanup:
            try:
                file_size = os.path.getsize(file_path)
                
                # ì•„ì¹´ì´ë¹™ í•„ìš” ì‹œ
                if policy.get('archive_before_delete'):
                    self._archive_file(file_path)
                
                # ì••ì¶• í•„ìš” ì‹œ
                if policy.get('compression_enabled'):
                    compressed_path = self._compress_file(file_path)
                    if compressed_path:
                        os.remove(file_path)
                        continue
                
                # íŒŒì¼ ì‚­ì œ
                os.remove(file_path)
                total_size += file_size
                
            except Exception as e:
                logger.error(f"Failed to cleanup {file_path}: {str(e)}")
        
        return total_size / (1024**3)  # GB ë‹¨ìœ„ë¡œ ë°˜í™˜

class DataUsageAnalyzer:
    """ë°ì´í„° ì‚¬ìš© íŒ¨í„´ ë¶„ì„ê¸°"""
    
    def analyze_usage(self, file_paths: List[str]) -> Dict:
        """ì‚¬ìš© íŒ¨í„´ ë¶„ì„"""
        usage_data = {}
        
        for file_path in file_paths:
            # íŒŒì¼ ì ‘ê·¼ ë¡œê·¸ ë¶„ì„
            access_count = self._get_access_count(file_path)
            last_access = self._get_last_access_time(file_path)
            references = self._find_references(file_path)
            
            usage_data[file_path] = {
                'access_count': access_count,
                'last_access': last_access,
                'referenced': len(references) > 0,
                'recent_access': self._is_recent_access(last_access),
                'importance_score': self._calculate_importance(access_count, last_access, references)
            }
        
        return usage_data
    
    def _calculate_importance(self, access_count: int, last_access: float, references: List) -> float:
        """ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        # ì ‘ê·¼ ë¹ˆë„ (40%)
        access_score = min(access_count / 10, 1.0) * 0.4
        
        # ìµœê·¼ì„± (30%)
        days_since_access = (time.time() - last_access) / (24 * 3600)
        recency_score = max(0, 1 - days_since_access / 30) * 0.3
        
        # ì°¸ì¡° ì—¬ë¶€ (30%)
        reference_score = 1.0 if references else 0.0
        reference_score *= 0.3
        
        return access_score + recency_score + reference_score
```

## ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

### ì‹¤ì‹œê°„ ë°ì´í„° í’ˆì§ˆ ê°ì‹œ
```python
# shared/data/quality_monitor.py
class DataQualityMonitor:
    """ë°ì´í„° í’ˆì§ˆ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.quality_metrics = {
            'completeness': 0.0,    # ì™„ì „ì„±
            'accuracy': 0.0,        # ì •í™•ì„±
            'consistency': 0.0,     # ì¼ê´€ì„±
            'timeliness': 0.0,      # ì ì‹œì„±
            'validity': 0.0         # ìœ íš¨ì„±
        }
        
        self.quality_rules = self._load_quality_rules()
        self.alert_thresholds = {
            'completeness': 0.95,
            'accuracy': 0.90,
            'consistency': 0.85,
            'timeliness': 0.80,
            'validity': 0.95
        }
    
    def _load_quality_rules(self) -> Dict:
        """í’ˆì§ˆ ê²€ì¦ ê·œì¹™ ë¡œë“œ"""
        return {
            'image_data': {
                'required_fields': ['timestamp', 'camera_id', 'resolution', 'format'],
                'format_validation': {
                    'timestamp': r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}',
                    'camera_id': r'cam_\d{3}',
                    'resolution': r'\d+x\d+',
                    'format': ['jpg', 'png', 'bmp']
                },
                'value_ranges': {
                    'file_size_mb': {'min': 0.1, 'max': 50.0},
                    'processing_time_ms': {'min': 1, 'max': 5000}
                }
            },
            'detection_results': {
                'required_fields': ['bbox', 'confidence', 'class_name'],
                'format_validation': {
                    'bbox': r'\[\d+,\s*\d+,\s*\d+,\s*\d+\]',
                    'confidence': r'0?\.\d+',
                    'class_name': ['face', 'person', 'object']
                },
                'value_ranges': {
                    'confidence': {'min': 0.0, 'max': 1.0},
                    'bbox_area': {'min': 100, 'max': 1000000}
                }
            }
        }
    
    def validate_data_batch(self, data_batch: List[Dict], data_type: str) -> Dict:
        """ë°ì´í„° ë°°ì¹˜ í’ˆì§ˆ ê²€ì¦"""
        if data_type not in self.quality_rules:
            return {'error': f'Unknown data type: {data_type}'}
        
        rules = self.quality_rules[data_type]
        validation_results = {
            'total_records': len(data_batch),
            'valid_records': 0,
            'completeness_score': 0.0,
            'accuracy_score': 0.0,
            'consistency_score': 0.0,
            'validation_errors': [],
            'quality_summary': {}
        }
        
        valid_count = 0
        completeness_scores = []
        accuracy_scores = []
        
        for i, record in enumerate(data_batch):
            record_validation = self._validate_single_record(record, rules)
            
            if record_validation['is_valid']:
                valid_count += 1
            
            completeness_scores.append(record_validation['completeness'])
            accuracy_scores.append(record_validation['accuracy'])
            
            if record_validation['errors']:
                validation_results['validation_errors'].extend([
                    f"Record {i}: {error}" for error in record_validation['errors']
                ])
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        validation_results['valid_records'] = valid_count
        validation_results['completeness_score'] = sum(completeness_scores) / len(completeness_scores)
        validation_results['accuracy_score'] = sum(accuracy_scores) / len(accuracy_scores)
        validation_results['consistency_score'] = self._calculate_consistency(data_batch)
        
        # í’ˆì§ˆ ìš”ì•½
        validation_results['quality_summary'] = {
            'validity_rate': valid_count / len(data_batch),
            'completeness': validation_results['completeness_score'],
            'accuracy': validation_results['accuracy_score'],
            'consistency': validation_results['consistency_score']
        }
        
        # ì•Œë¦¼ í™•ì¸
        self._check_quality_alerts(validation_results['quality_summary'])
        
        return validation_results
    
    def _validate_single_record(self, record: Dict, rules: Dict) -> Dict:
        """ë‹¨ì¼ ë ˆì½”ë“œ ê²€ì¦"""
        errors = []
        required_fields = rules.get('required_fields', [])
        format_rules = rules.get('format_validation', {})
        value_ranges = rules.get('value_ranges', {})
        
        # ì™„ì „ì„± ê²€ì‚¬
        missing_fields = [field for field in required_fields if field not in record]
        completeness = 1.0 - (len(missing_fields) / len(required_fields))
        
        if missing_fields:
            errors.append(f"Missing required fields: {missing_fields}")
        
        # í˜•ì‹ ê²€ì¦
        accuracy = 1.0
        for field, pattern in format_rules.items():
            if field in record:
                if isinstance(pattern, str):  # ì •ê·œì‹
                    import re
                    if not re.match(pattern, str(record[field])):
                        errors.append(f"Invalid format for {field}: {record[field]}")
                        accuracy -= 0.1
                elif isinstance(pattern, list):  # í—ˆìš© ê°’ ëª©ë¡
                    if record[field] not in pattern:
                        errors.append(f"Invalid value for {field}: {record[field]}")
                        accuracy -= 0.1
        
        # ê°’ ë²”ìœ„ ê²€ì¦
        for field, range_rule in value_ranges.items():
            if field in record:
                value = record[field]
                if 'min' in range_rule and value < range_rule['min']:
                    errors.append(f"{field} below minimum: {value} < {range_rule['min']}")
                    accuracy -= 0.1
                if 'max' in range_rule and value > range_rule['max']:
                    errors.append(f"{field} above maximum: {value} > {range_rule['max']}")
                    accuracy -= 0.1
        
        return {
            'is_valid': len(errors) == 0,
            'completeness': max(0, completeness),
            'accuracy': max(0, accuracy),
            'errors': errors
        }
    
    def _calculate_consistency(self, data_batch: List[Dict]) -> float:
        """ë°ì´í„° ì¼ê´€ì„± ê³„ì‚°"""
        if len(data_batch) < 2:
            return 1.0
        
        consistency_score = 1.0
        
        # í˜•ì‹ ì¼ê´€ì„± ê²€ì‚¬
        first_record_keys = set(data_batch[0].keys())
        for record in data_batch[1:]:
            if set(record.keys()) != first_record_keys:
                consistency_score -= 0.1
        
        # ê°’ ë²”ìœ„ ì¼ê´€ì„± ê²€ì‚¬ (ì´ìƒì¹˜ íƒì§€)
        numeric_fields = []
        for key in first_record_keys:
            if all(isinstance(record.get(key), (int, float)) for record in data_batch):
                numeric_fields.append(key)
        
        for field in numeric_fields:
            values = [record[field] for record in data_batch if field in record]
            if len(values) > 1:
                outliers = self._detect_outliers(values)
                consistency_score -= len(outliers) * 0.05
        
        return max(0, consistency_score)
    
    def _detect_outliers(self, values: List[float]) -> List[int]:
        """ì´ìƒì¹˜ íƒì§€ (IQR ë°©ë²•)"""
        import numpy as np
        
        q1 = np.percentile(values, 25)
        q3 = np.percentile(values, 75)
        iqr = q3 - q1
        
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        outliers = []
        for i, value in enumerate(values):
            if value < lower_bound or value > upper_bound:
                outliers.append(i)
        
        return outliers

# ë°ì´í„° ì¹´íƒˆë¡œê·¸
class DataCatalog:
    """ë°ì´í„° ì¹´íƒˆë¡œê·¸ ê´€ë¦¬"""
    
    def __init__(self):
        self.catalog_db = {}  # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
        
    def register_dataset(self, dataset_info: Dict) -> str:
        """ë°ì´í„°ì…‹ ë“±ë¡"""
        dataset_id = f"ds_{int(time.time())}"
        
        catalog_entry = {
            'id': dataset_id,
            'name': dataset_info['name'],
            'description': dataset_info.get('description', ''),
            'location': dataset_info['location'],
            'schema': dataset_info.get('schema', {}),
            'size_bytes': dataset_info.get('size_bytes', 0),
            'record_count': dataset_info.get('record_count', 0),
            'created_date': datetime.now().isoformat(),
            'last_updated': datetime.now().isoformat(),
            'access_permissions': dataset_info.get('access_permissions', ['read']),
            'quality_score': dataset_info.get('quality_score', 0.0),
            'tags': dataset_info.get('tags', []),
            'lineage': dataset_info.get('lineage', {}),
            'retention_policy': dataset_info.get('retention_policy', 'default')
        }
        
        self.catalog_db[dataset_id] = catalog_entry
        return dataset_id
    
    def search_datasets(self, query: str, filters: Dict = None) -> List[Dict]:
        """ë°ì´í„°ì…‹ ê²€ìƒ‰"""
        results = []
        
        for dataset_id, dataset in self.catalog_db.items():
            # í…ìŠ¤íŠ¸ ê²€ìƒ‰
            if query.lower() in dataset['name'].lower() or query.lower() in dataset['description'].lower():
                if self._matches_filters(dataset, filters or {}):
                    results.append(dataset)
        
        return results
    
    def _matches_filters(self, dataset: Dict, filters: Dict) -> bool:
        """í•„í„° ì¡°ê±´ í™•ì¸"""
        for key, value in filters.items():
            if key not in dataset:
                return False
            
            if isinstance(value, list):
                if dataset[key] not in value:
                    return False
            else:
                if dataset[key] != value:
                    return False
        
        return True
```

## ğŸ—„ï¸ ë°±ì—… ë° ë³µì› ì‹œìŠ¤í…œ

### ìë™í™”ëœ ë°±ì—… ì „ëµ
```python
# shared/data/backup_manager.py
class EnterpriseBackupManager:
    """ì—”í„°í”„ë¼ì´ì¦ˆ ë°±ì—… ê´€ë¦¬"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.backup_strategies = {
            'critical': {
                'frequency': 'hourly',
                'retention_days': 365,
                'replication_zones': 3,
                'encryption': True,
                'compression': True
            },
            'important': {
                'frequency': 'daily',
                'retention_days': 90,
                'replication_zones': 2,
                'encryption': True,
                'compression': True
            },
            'standard': {
                'frequency': 'weekly',
                'retention_days': 30,
                'replication_zones': 1,
                'encryption': False,
                'compression': True
            }
        }
    
    def schedule_backup(self, data_path: str, backup_level: str = 'standard'):
        """ë°±ì—… ìŠ¤ì¼€ì¤„ë§"""
        if backup_level not in self.backup_strategies:
            raise ValueError(f"Unknown backup level: {backup_level}")
        
        strategy = self.backup_strategies[backup_level]
        
        # Celery ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•œ ì£¼ê¸°ì  ë°±ì—…
        from celery import Celery
        
        @Celery.task
        def execute_backup():
            backup_id = self._create_backup(data_path, strategy)
            return backup_id
        
        # ì£¼ê¸°ì— ë”°ë¥¸ ìŠ¤ì¼€ì¤„ë§
        if strategy['frequency'] == 'hourly':
            execute_backup.apply_async(countdown=3600)  # 1ì‹œê°„
        elif strategy['frequency'] == 'daily':
            execute_backup.apply_async(countdown=86400)  # 24ì‹œê°„
        elif strategy['frequency'] == 'weekly':
            execute_backup.apply_async(countdown=604800)  # 7ì¼
    
    def _create_backup(self, data_path: str, strategy: Dict) -> str:
        """ë°±ì—… ìƒì„±"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_id = f"backup_{timestamp}_{hash(data_path)}"
        
        # ë°±ì—… íŒŒì¼ ìƒì„±
        backup_path = f"backups/{backup_id}"
        
        if strategy['compression']:
            backup_path += ".tar.gz"
            self._create_compressed_backup(data_path, backup_path)
        else:
            self._create_simple_backup(data_path, backup_path)
        
        # ì•”í˜¸í™”
        if strategy['encryption']:
            encrypted_path = self._encrypt_backup(backup_path)
            os.remove(backup_path)
            backup_path = encrypted_path
        
        # ë³µì œë³¸ ìƒì„±
        for zone in range(strategy['replication_zones']):
            replica_path = f"{backup_path}.replica_{zone}"
            shutil.copy2(backup_path, replica_path)
        
        # ë°±ì—… ë©”íƒ€ë°ì´í„° ì €ì¥
        self._save_backup_metadata(backup_id, {
            'original_path': data_path,
            'backup_path': backup_path,
            'strategy': strategy,
            'created_at': datetime.now().isoformat(),
            'size_bytes': os.path.getsize(backup_path)
        })
        
        return backup_id
    
    def restore_backup(self, backup_id: str, restore_path: str) -> bool:
        """ë°±ì—… ë³µì›"""
        try:
            metadata = self._get_backup_metadata(backup_id)
            backup_path = metadata['backup_path']
            
            # ë³µí˜¸í™”
            if backup_path.endswith('.encrypted'):
                decrypted_path = self._decrypt_backup(backup_path)
                backup_path = decrypted_path
            
            # ì••ì¶• í•´ì œ
            if backup_path.endswith('.tar.gz'):
                self._extract_compressed_backup(backup_path, restore_path)
            else:
                shutil.copy2(backup_path, restore_path)
            
            logger.info(f"Backup {backup_id} restored to {restore_path}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to restore backup {backup_id}: {str(e)}")
            return False
    
    def cleanup_old_backups(self):
        """ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬"""
        for backup_id, metadata in self._list_all_backups().items():
            created_at = datetime.fromisoformat(metadata['created_at'])
            retention_days = metadata['strategy']['retention_days']
            
            if datetime.now() - created_at > timedelta(days=retention_days):
                self._delete_backup(backup_id)
                logger.info(f"Cleaned up expired backup: {backup_id}")
```

## ğŸ“‹ ë°ì´í„° ê±°ë²„ë„ŒìŠ¤

### ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ í”„ë ˆì„ì›Œí¬
```python
# shared/data/governance.py
class DataGovernanceFramework:
    """ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ í”„ë ˆì„ì›Œí¬"""
    
    def __init__(self):
        self.policies = self._load_governance_policies()
        self.compliance_rules = self._load_compliance_rules()
        
    def _load_governance_policies(self) -> Dict:
        """ê±°ë²„ë„ŒìŠ¤ ì •ì±… ë¡œë“œ"""
        return {
            'data_classification': {
                'public': {'access_level': 'open', 'encryption': False},
                'internal': {'access_level': 'company', 'encryption': True},
                'confidential': {'access_level': 'authorized', 'encryption': True},
                'restricted': {'access_level': 'minimum', 'encryption': True}
            },
            'retention_policies': {
                'personal_data': {'max_retention_days': 1095},  # 3ë…„
                'business_data': {'max_retention_days': 2555},  # 7ë…„
                'system_logs': {'max_retention_days': 365},     # 1ë…„
                'temp_data': {'max_retention_hours': 24}        # 24ì‹œê°„
            },
            'access_controls': {
                'role_based': True,
                'time_based': True,
                'location_based': False,
                'approval_required': ['confidential', 'restricted']
            }
        }
    
    def _load_compliance_rules(self) -> Dict:
        """ì»´í”Œë¼ì´ì–¸ìŠ¤ ê·œì¹™ ë¡œë“œ"""
        return {
            'gdpr': {
                'personal_data_consent': True,
                'right_to_be_forgotten': True,
                'data_portability': True,
                'breach_notification_hours': 72
            },
            'ccpa': {
                'consumer_rights': True,
                'opt_out_sale': True,
                'data_disclosure': True
            },
            'sox': {
                'financial_data_controls': True,
                'audit_trail': True,
                'segregation_of_duties': True
            }
        }
    
    def validate_data_access(self, user_id: str, data_classification: str, operation: str) -> bool:
        """ë°ì´í„° ì ‘ê·¼ ê²€ì¦"""
        user_permissions = self._get_user_permissions(user_id)
        classification_policy = self.policies['data_classification'][data_classification]
        
        # ì ‘ê·¼ ë ˆë²¨ í™•ì¸
        if not self._check_access_level(user_permissions, classification_policy['access_level']):
            return False
        
        # ì‹œê°„ ê¸°ë°˜ ì œì•½ í™•ì¸
        if self.policies['access_controls']['time_based']:
            if not self._check_time_restrictions(user_id):
                return False
        
        # ìŠ¹ì¸ í•„ìš” í™•ì¸
        if data_classification in self.policies['access_controls']['approval_required']:
            if not self._check_approval_status(user_id, data_classification, operation):
                return False
        
        # ê°ì‚¬ ë¡œê·¸ ê¸°ë¡
        self._log_data_access(user_id, data_classification, operation)
        
        return True
    
    def enforce_retention_policy(self, data_path: str, data_type: str):
        """ë³´ì¡´ ì •ì±… ê°•ì œ ì ìš©"""
        if data_type not in self.policies['retention_policies']:
            logger.warning(f"No retention policy for data type: {data_type}")
            return
        
        policy = self.policies['retention_policies'][data_type]
        
        # íŒŒì¼ ìƒì„± ì‹œê°„ í™•ì¸
        creation_time = os.path.getctime(data_path)
        current_time = time.time()
        
        if 'max_retention_days' in policy:
            max_age = policy['max_retention_days'] * 24 * 3600
            if current_time - creation_time > max_age:
                self._schedule_data_deletion(data_path, 'retention_policy_expired')
        
        elif 'max_retention_hours' in policy:
            max_age = policy['max_retention_hours'] * 3600
            if current_time - creation_time > max_age:
                self._schedule_data_deletion(data_path, 'retention_policy_expired')
    
    def generate_compliance_report(self, compliance_type: str) -> Dict:
        """ì»´í”Œë¼ì´ì–¸ìŠ¤ ë¦¬í¬íŠ¸ ìƒì„±"""
        if compliance_type not in self.compliance_rules:
            raise ValueError(f"Unknown compliance type: {compliance_type}")
        
        rules = self.compliance_rules[compliance_type]
        report = {
            'compliance_type': compliance_type,
            'generated_at': datetime.now().isoformat(),
            'compliance_status': {},
            'violations': [],
            'recommendations': []
        }
        
        # ê·œì¹™ë³„ ì¤€ìˆ˜ ìƒíƒœ í™•ì¸
        for rule_name, required in rules.items():
            if required:
                status = self._check_rule_compliance(rule_name)
                report['compliance_status'][rule_name] = status
                
                if not status['compliant']:
                    report['violations'].append({
                        'rule': rule_name,
                        'description': status['description'],
                        'severity': status['severity']
                    })
        
        # ê°œì„  ê¶Œê³ ì‚¬í•­ ìƒì„±
        report['recommendations'] = self._generate_recommendations(report['violations'])
        
        return report
```


---

**ì ìš© ì‹œì **: ì¼ì¼ ë°ì´í„° 1TB ì´ìƒ ì²˜ë¦¬ ì‹œ ë˜ëŠ” ì¥ê¸° ë°ì´í„° ë³´ê´€ì´ í•„ìš”í•  ë•Œ
**ì˜ì¡´ì„±**: `pip install boto3 celery numpy pandas`
**ì„¤ì •**: AWS S3, Glacier, ë°ì´í„°ë² ì´ìŠ¤, ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • í•„ìš”


